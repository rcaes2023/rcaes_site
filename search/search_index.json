{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Research Computing","text":"<p>Fall 2023 | 16:107:606 Special Topics in Atmospheric Sciences Research Computing in Atmospheric and Environmental Sciences Rutgers University, Department of Environmental Sciences</p> <p></p>"},{"location":"Assignment_0/","title":"Create your Accounts on Amarel and GitHub","text":""},{"location":"Assignment_0/#amarel-account","title":"Amarel Account","text":"<p>https://oarc.rutgers.edu/amarel-cluster-access-request/</p>"},{"location":"Assignment_0/#github-account","title":"GitHub Account","text":"<p>https://github.com  Student Account: https://education.github.com/benefits?type=student</p>"},{"location":"Assignment_3/","title":"Assignment 3 - Numpy and Matplotlib","text":"<p>First import numpy and matplotlib</p> In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv('Millbrook_NY_daily_weather.csv', parse_dates=['LST_DATE'])\ndf = df.set_index('LST_DATE')\n\n#########################################################\n#### BELOW ARE THE VARIABLES YOU SHOULD USE IN THE PLOTS!\n#### (numpy arrays) \n#### NO PANDAS ALLOWED!\n#########################################################\n\nt_daily_min = df.T_DAILY_MIN.values\nt_daily_max = df.T_DAILY_MAX.values\nt_daily_mean = df.T_DAILY_MEAN.values\np_daily_calc = df.P_DAILY_CALC.values\nsoil_moisture_5 = df.SOIL_MOISTURE_5_DAILY.values\nsoil_moisture_10 = df.SOIL_MOISTURE_10_DAILY.values\nsoil_moisture_20 = df.SOIL_MOISTURE_20_DAILY.values\nsoil_moisture_50 = df.SOIL_MOISTURE_50_DAILY.values\nsoil_moisture_100 = df.SOIL_MOISTURE_100_DAILY.values\ndate = df.index.values\n</pre> import pandas as pd  df = pd.read_csv('Millbrook_NY_daily_weather.csv', parse_dates=['LST_DATE']) df = df.set_index('LST_DATE')  ######################################################### #### BELOW ARE THE VARIABLES YOU SHOULD USE IN THE PLOTS! #### (numpy arrays)  #### NO PANDAS ALLOWED! #########################################################  t_daily_min = df.T_DAILY_MIN.values t_daily_max = df.T_DAILY_MAX.values t_daily_mean = df.T_DAILY_MEAN.values p_daily_calc = df.P_DAILY_CALC.values soil_moisture_5 = df.SOIL_MOISTURE_5_DAILY.values soil_moisture_10 = df.SOIL_MOISTURE_10_DAILY.values soil_moisture_20 = df.SOIL_MOISTURE_20_DAILY.values soil_moisture_50 = df.SOIL_MOISTURE_50_DAILY.values soil_moisture_100 = df.SOIL_MOISTURE_100_DAILY.values date = df.index.values In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[7]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Assignment_3/#assignment-3-numpy-and-matplotlib","title":"Assignment 3 - Numpy and Matplotlib\u00b6","text":""},{"location":"Assignment_3/#1-creating-and-manipulating-arrays","title":"1 Creating and Manipulating Arrays\u00b6","text":""},{"location":"Assignment_3/#11-create-two-2d-arrays-xx-and-yy-representing-coordinates-x-y-on-the-cartesian-plan","title":"1.1. Create two 2D arrays (xx and yy) representing coordinates x, y on the cartesian plan\u00b6","text":"<p>Both should cover the range (-2, 2) and have 100 points in each direction</p>"},{"location":"Assignment_3/#12-visualize-each-2d-array-using-pcolormesh","title":"1.2. Visualize each 2D array using <code>pcolormesh</code>\u00b6","text":"<p>Use the correct coordiantes for the x and y axes. Provide axis labels for all of your plots in this assignment.</p>"},{"location":"Assignment_3/#13-from-your-cartesian-coordinates-create-polar-coordinates-r-and-varphi","title":"1.3 From your cartesian coordinates, create polar coordinates $r$ and $\\varphi$\u00b6","text":"<p>Refer to the wikipedia page for the conversion formula. You will need to use numpy's <code>arctan2</code> function. Read its documentation.</p>"},{"location":"Assignment_3/#14-visualize-r-and-varphi-as-functions-of-x-and-y","title":"1.4. Visualize $r$ and $\\varphi$ as functions of $x$ and $y$\u00b6","text":""},{"location":"Assignment_3/#15-define-the-function-f-cos24r-sin24varphi-and-plot-it-as-a-function-of-x-and-y","title":"1.5 Define the function $f = \\cos^2(4r) + \\sin^2(4\\varphi)$ and Plot it as a function of $x$ and $y$\u00b6","text":""},{"location":"Assignment_3/#16-plot-the-mean-of-f-with-respect-to-the-x-axis","title":"1.6 Plot the mean of f with respect to the x axis\u00b6","text":"<p>as a function of y</p>"},{"location":"Assignment_3/#17-plot-the-mean-of-f-with-respect-to-the-y-axis","title":"1.7 Plot the mean of f with respect to the y axis\u00b6","text":"<p>as a function of x</p>"},{"location":"Assignment_3/#part-ii-making-plots-with-matplotlib-for-real-data","title":"Part II: Making plots with Matplotlib for real data\u00b6","text":"<p>In this problem, we will plot some daily weather data from a NOAA station in Millbrook, NY.</p> <p>The cell below uses pandas to load the data and populate a bunch of numpy arrays (<code>t_daily_min</code>, <code>t_daily_max</code>, etc.)</p>"},{"location":"Assignment_3/#21-use-numpy-to-calculate-mean-temperature-precipitation-and-soil-moisture-at-different-layers","title":"2.1 Use numpy to calculate mean temperature, precipitation and soil moisture at different layers.\u00b6","text":"<p>Write a loop to make the code short and efficient.</p>"},{"location":"Assignment_3/#22-use-the-numpy-arrays-to-try-to-re-create-the-plot-you-see-below","title":"2.2 Use the numpy arrays to try to re-create the plot you see below\u00b6","text":"<p>Hint: Try fill_between to plot range values</p>"},{"location":"Assignment_4/","title":"Assignment 4: Pandas","text":"In\u00a0[1]: Copied! <pre># do imports here\n</pre> # do imports here In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Use the following code to download a csv file of the NOAA IBTrACS hurrican dataset.</p> In\u00a0[\u00a0]: Copied! <pre>! wget https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/csv/ibtracs.since1980.list.v04r00.csv\n</pre> ! wget https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/csv/ibtracs.since1980.list.v04r00.csv In\u00a0[\u00a0]: Copied! <pre>df=pd.read_csv('ibtracs.since1980.list.v04r00.csv',usecols=range(12), skiprows  = [1], parse_dates=['ISO_TIME'],na_values=[-999, ' '])\n</pre> df=pd.read_csv('ibtracs.since1980.list.v04r00.csv',usecols=range(12), skiprows  = [1], parse_dates=['ISO_TIME'],na_values=[-999, ' ']) In\u00a0[\u00a0]: Copied! <pre>df.head()\n</pre> df.head() In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>You will notice some names are repeated.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Assignment_4/#assignment-4-pandas","title":"Assignment 4: Pandas\u00b6","text":""},{"location":"Assignment_4/#part-i-basic-pandas","title":"Part I: Basic Pandas\u00b6","text":""},{"location":"Assignment_4/#1-use-pandas-read_csv-function-to-open-the-ground-level-ozone-data-at-rutgers-site-in-2021-epa_aqs_ozone_rutgers_2022csv-as-a-dataframe","title":"1) Use Pandas <code>read_csv</code> function to open the ground-level ozone data at Rutgers site in 2021 ('EPA_AQS_Ozone_Rutgers_2022.csv') as a DataFrame\u00b6","text":"<p>(Don't use any special options). Display the first few rows and the DataFrame info.</p>"},{"location":"Assignment_4/#2-re-read-the-data-in-such-a-way-that-the-date-column-is-identified-as-date-and-date-is-used-as-the-index","title":"2) Re-read the data in such a way that the Date column is identified as date and Date is used as the index\u00b6","text":""},{"location":"Assignment_4/#3-rename-the-column-daily-max-8-hour-ozone-concentration-as-ozone","title":"3) Rename the column 'Daily Max 8-hour Ozone Concentration' as 'ozone'\u00b6","text":""},{"location":"Assignment_4/#4-use-describe-to-get-the-basic-statistics-of-ozone-in-2022","title":"4) Use <code>describe</code> to get the basic statistics of ozone in 2022\u00b6","text":""},{"location":"Assignment_4/#5-use-nlargest-to-get-the-10-days-with-highest-ozone-concentration-in-2022","title":"5) Use <code>nlargest</code> to get the 10 days with highest ozone concentration in 2022\u00b6","text":""},{"location":"Assignment_4/#6-make-a-time-series-plot-of-daily-ozone-concentration-in-2022","title":"6) Make a time series plot of daily ozone concentration in 2022\u00b6","text":""},{"location":"Assignment_4/#7-make-a-time-series-plot-of-monthly-average-ozone-concentration-in-2022","title":"7) Make a time series plot of monthly average ozone concentration in 2022\u00b6","text":""},{"location":"Assignment_4/#8-read-the-ozone-data-in-2021-and-merge-it-with-the-data-in-2022","title":"8) Read the ozone data in 2021, and merge it with the data in 2022.\u00b6","text":"<p>Remember to rename the ozone column in the new data before merging two dataframes.</p>"},{"location":"Assignment_4/#9-make-a-time-series-plot-of-monthly-average-ozone-concentration-from-2021-to-2022","title":"9) Make a time series plot of monthly average ozone concentration from 2021 to 2022\u00b6","text":""},{"location":"Assignment_4/#part-ii-advanced-pandas-with-hurricane-data","title":"Part II: Advanced Pandas with hurricane data\u00b6","text":""},{"location":"Assignment_4/#1-get-the-unique-values-of-the-basin-subbasin-and-nature-columns","title":"1) Get the unique values of the <code>BASIN</code>, <code>SUBBASIN</code>, and <code>NATURE</code> columns\u00b6","text":""},{"location":"Assignment_4/#2-rename-the-wmo_wind-column-to-wind-and-wmo_pres-column-to-pressure","title":"2) Rename the <code>WMO_WIND</code> column to <code>Wind</code>, and <code>WMO_PRES</code> column to <code>Pressure</code>\u00b6","text":""},{"location":"Assignment_4/#3-get-the-10-largest-rows-in-the-dataset-by-wind","title":"3) Get the 10 largest rows in the dataset by <code>Wind</code>\u00b6","text":""},{"location":"Assignment_4/#4-group-the-data-on-sid-and-get-the-10-largest-hurricanes-by-maximum-wind","title":"4) Group the data on <code>SID</code> and get the 10 largest hurricanes by maximum <code>Wind</code>\u00b6","text":""},{"location":"Assignment_4/#5-plot-the-count-of-all-datapoints-by-basin","title":"5) Plot the count of all datapoints by Basin\u00b6","text":"<p>as a bar chart</p>"},{"location":"Assignment_4/#6-plot-the-count-of-unique-hurricanes-by-basin","title":"6) Plot the count of unique hurricanes by Basin\u00b6","text":"<p>as a bar chart. (You will need to call <code>groupby</code> twice.)</p>"},{"location":"Assignment_4/#7-make-a-hexbin-of-the-location-of-datapoints-in-latitude-and-longitude","title":"7) Make a <code>hexbin</code> of the location of datapoints in Latitude and Longitude\u00b6","text":""},{"location":"Assignment_4/#8-find-hurricane-sandy-from-2012-and-plot-its-track-as-a-scatter-plot","title":"8) Find Hurricane Sandy (from 2012) and plot its track as a scatter plot\u00b6","text":"<p>Use wind speed to color the points.</p>"},{"location":"Assignment_4/#9-make-time-the-index-on-your-dataframe","title":"9) Make time the index on your dataframe\u00b6","text":""},{"location":"Assignment_4/#10-plot-the-count-of-all-datapoints-per-year-as-a-timeseries","title":"10) Plot the count of all datapoints per year as a timeseries\u00b6","text":"<p>You should use <code>resample</code></p>"},{"location":"Assignment_4/#11-plot-all-tracks-from-the-west-pacific-basinwp-in-2005-color-the-tracks-by-hurricane-sid","title":"11) Plot all tracks from the West Pacific (BASIN:'WP') in 2005. Color the tracks by hurricane SID.\u00b6","text":"<p>First create a subset dataframe by searching <code>SEASON</code> and <code>BASIN</code>. You will probably have to iterate through a <code>GroupBy</code> object from the subset dataframe.</p>"},{"location":"Assignment_4/#12-create-a-filtered-dataframe-that-contains-only-data-from-the-west-pacific-wp-basin","title":"12) Create a filtered dataframe that contains only data from the West Pacific (\"WP\") Basin\u00b6","text":"<p>Use this for the rest of the assignment</p>"},{"location":"Assignment_4/#13-plot-the-number-of-datapoints-per-day","title":"13) Plot the number of datapoints per day\u00b6","text":"<p>Make sure you figure is big enough to actually see the plot</p>"},{"location":"Assignment_4/#14-calculate-the-climatology-of-datapoint-counts-as-a-function-of-dayofyear","title":"14) Calculate the climatology of datapoint counts as a function of <code>dayofyear</code>\u00b6","text":""},{"location":"Assignment_5/","title":"Assignment 5 : Xarray","text":"<p>In this assignment, we will use Xarray to analyze top-of-atmosphere radiation data from NASA's CERES project.</p> <p> Public domain, by NASA, from Wikimedia Commons</p> <p>Start by importing xarray, numpy, and matplotlib</p> In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <p>The answer is that each \"pixel\" or \"grid point\" of this dataset does not represent an equal area of Earth's surface. So naively taking the mean, i.e. giving equal weight to each point, gives the wrong answer.</p> <p>On a lat / lon grid, the relative area of each grid point is proportional to $\\cos(\\lambda)$. ($\\lambda$ is latitude)</p> In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <p>This time around, you should have found something much closer to zero. Ask a climate scientist what the net energy imbalance of Earth due to global warming is estimate to be. Do you think our calculation is precise enough to detect this?</p> In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied!"},{"location":"Assignment_5/#assignment-5-xarray","title":"Assignment 5 : Xarray\u00b6","text":""},{"location":"Assignment_5/#part-i-xarray-for-earths-energy-budget","title":"Part I: Xarray for earth's energy budget\u00b6","text":""},{"location":"Assignment_5/#11-open-the-ceres-dataset-and-display-its-contents","title":"1.1) Open the CERES dataset and display its contents.\u00b6","text":"<p>The CERES dataset has been predownloaded and processed. You can access the dataset here: /scratch/xj103/rcaes/CERES_EBAF-TOA_Edition4.0_200003-201701.condensed.nc</p>"},{"location":"Assignment_5/#12-print-out-the-long_name-attribute-of-each-variable","title":"1.2) Print out the <code>long_name</code> attribute of each variable\u00b6","text":""},{"location":"Assignment_5/#21-calculate-the-time-mean-of-the-entire-dataset","title":"2.1) Calculate the time-mean of the entire dataset\u00b6","text":""},{"location":"Assignment_5/#22-from-this-make-a-2d-plot-of-the-the-time-mean-toa-longwave-shortwave-and-incoming-solar-radiation-flux","title":"2.2) From this, make a 2D plot of the the time-mean TOA longwave, shortwave, and incoming solar radiation flux\u00b6","text":"<p>(All-Sky conditions)</p> <p>Note the sign conventions on each variable.</p>"},{"location":"Assignment_5/#23-add-up-the-three-variables-above-and-verify-visually-that-they-are-equivalent-to-the-toa-net-flux","title":"2.3) Add up the three variables above and verify (visually) that they are equivalent to the TOA net flux\u00b6","text":"<p>You have to pay attention to and think about the sign conventions for each variable in order to get this to work.</p>"},{"location":"Assignment_5/#31-calculate-the-global-mean-of-toa-net-radiation-directly-from-the-dataset","title":"3.1) Calculate the global mean of TOA net radiation directly from the dataset\u00b6","text":"<p>Since the Earth is approximately in radiative balance, the net TOA radiation should be zero. But taking the naive mean from this dataset, you should find a number far from zero. Why?</p>"},{"location":"Assignment_5/#32-create-a-weight-array-proportional-to-coslambda-with-a-mean-value-of-1","title":"3.2) Create a <code>weight</code> array proportional to $\\cos(\\lambda)$ with a mean value of 1\u00b6","text":"<p>Verify its mean is 1 and plot it. Be careful about radians vs. degrees.</p>"},{"location":"Assignment_5/#33-redo-your-global-mean-toa-net-radiation-calculation-with-this-weight-factor","title":"3.3) Redo your global mean TOA net radiation calculation with this weight factor\u00b6","text":"<p>Remember Xarray's handling of broadcasting. Don't make this harder than it needs to be.</p>"},{"location":"Assignment_5/#34-now-that-you-have-a-weight-factor-verify-that-the-toa-incoming-solar-outgoing-longwave-and-outgoing-shortwave-approximately-match-up-with-the-cartoon-above","title":"3.4) Now that you have a <code>weight</code> factor, verify that the TOA incoming solar, outgoing longwave, and outgoing shortwave approximately match up with the cartoon above\u00b6","text":""},{"location":"Assignment_5/#41-plot-the-time-mean-cloud-area-fraction-day-and-night","title":"4.1) Plot the time-mean cloud area fraction (day and night)\u00b6","text":""},{"location":"Assignment_5/#42-define-boolean-masks-for-low-cloud-area-le-25-and-high-cloud-area-ge-75","title":"4.2) Define boolean masks for low cloud area ($\\le$ 25%) and high cloud area ($\\ge$ 75%)\u00b6","text":"<p>Use the whole dataset, not the time mean.</p>"},{"location":"Assignment_5/#43-calculate-and-plot-composites-of-time-mean-outgoing-shortwave-and-longwave-radiation-for-low-and-high-cloud-area-regions","title":"4.3) Calculate and plot composites of time-mean outgoing shortwave and longwave radiation for low and high cloud area regions\u00b6","text":"<p>Your results should be 2D maps.</p> <p>Xarray's where function will be helpful.</p>"},{"location":"Assignment_5/#51-make-a-figure-of-4-subplots-that-show-the-seasonal-mean-toa-outgoing-shortwave-and-another-figure-for-toa-longwave-radiation","title":"5.1) Make a figure of 4 subplots that show the seasonal mean TOA outgoing shortwave and another figure for TOA longwave radiation\u00b6","text":""},{"location":"Assignment_5/#52-subset-the-dataset-for-nj-region-74-w-756-w-388-n-415-n-and-calculate-the-monthly-climatology-of-cloud-visible-optical-depth-and-cloud-area-fraction","title":"5.2) Subset the dataset for NJ region (74\u02daW - 75.6\u02daW, 38.8\u02daN - 41.5\u02daN) and calculate the monthly climatology of cloud visible optical depth and cloud area fraction\u00b6","text":""},{"location":"Assignment_6/","title":"Assignment 6: Cartopy","text":"In\u00a0[2]: Copied! <pre>import xarray as xr\nfrom matplotlib import pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy\nimport pandas as pd\nimport numpy as np\n</pre> import xarray as xr from matplotlib import pyplot as plt import cartopy.crs as ccrs import cartopy import pandas as pd import numpy as np In\u00a0[\u00a0]: Copied! <pre>ds_apcp = xr.open_dataset('https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/NARR/Dailies/monolevel/apcp.2023.nc')\n</pre> ds_apcp = xr.open_dataset('https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/NARR/Dailies/monolevel/apcp.2023.nc') In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[30]: Copied! <pre>\n</pre> Out[30]: <pre>&lt;cartopy.mpl.gridliner.Gridliner at 0x183235ac0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>! wget https://noaadata.apps.nsidc.org/NOAA/G02202_V4/south/daily/2023/seaice_conc_daily_sh_20230629_f17_v04r00.nc\n</pre> ! wget https://noaadata.apps.nsidc.org/NOAA/G02202_V4/south/daily/2023/seaice_conc_daily_sh_20230629_f17_v04r00.nc  In\u00a0[24]: Copied! <pre>\n</pre> Out[24]: <pre>&lt;cartopy.mpl.geocollection.GeoQuadMesh at 0x17f757d60&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[35]: Copied! <pre>\n</pre> Out[35]: <pre>Text(0.5, 1.0, 'Big Earthquakes (Magnitude 8+)')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Assignment_6/#assignment-6-cartopy","title":"Assignment 6: Cartopy\u00b6","text":"<p>For this assignment, your goal is to make three maps that look like the maps shown below.</p>"},{"location":"Assignment_6/#1-plot-data-from-narr","title":"1) Plot data from NARR\u00b6","text":"<p>NARR is NCEP's North American Regional Reanalysis, a widely used product for studying the weather and climate of the continental US. The data is available from NOAA's Earth System Research Laboratory via OPeNDAP, meaing that xarray can opent the data \"remotely\" without downloading a file.</p> <p>For this problem, you should open this precipitation file:</p> <pre><code>https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/NARR/Dailies/monolevel/apcp.2023.nc</code></pre> <p>Your goal is to make a map that looks like the one below. It shows total precipitation on Sept. 29, 2023 in blue.</p> <p>Hint: examine the dataset variables and attirbutes carefully in order to determine the projection of the data.</p>"},{"location":"Assignment_6/#2-antarctic-sea-ice","title":"2) Antarctic Sea Ice\u00b6","text":"<p>Download this file and then use it to plot the concentration of Antarctic Sea Ice on June 29, 2023. Again, you will need to explore the file contents in order to determine the correct projection.</p> <p>Download the data from NOAA@NSIDC.</p> <p>Hint: Check if the coordinates are correctly read. If not, assign the coordiates using <code>swap_dims</code>.</p>"},{"location":"Assignment_6/#3-global-largest-earthquakes","title":"3) Global Largest Earthquakes\u00b6","text":"<p>Create a map that shows the locations of the biggest earthquakes globally.</p>"},{"location":"Assignment_7/","title":"Assignment 7: Environmental Science Packages","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied!"},{"location":"Assignment_7/#assignment-7-environmental-science-packages","title":"Assignment 7: Environmental Science Packages\u00b6","text":"<p>The goal of this assignment is to learn to use existing Python packages.</p>"},{"location":"Assignment_7/#write-a-tutorial-for-an-open-source-python-package","title":"Write a tutorial for an open-source Python package\u00b6","text":"<p>Pick a python package of your choice. You can choose one from the list of the python packages in Lecture Notes, or a python package that you're planning to use in your final project.</p> <p>Your task is to write a jupyter notebook tutorial on this package. Your tutorial should:</p> <ul> <li>Introduce the package's core features, capabilities, references and how it can be applied to analyze environmental data.</li> <li>Provide instructions for installing the package using Conda or Mamba or Google's Colab.</li> <li>Integrate real-world environmental sciences data into your tutorial. This could include datasets used in class or your final project.</li> <li>Provide 2 to 3 examples that guide users through common tasks in environmental data analysis using the package.</li> <li>Be well-organized and reproducible with clean code and clear comments.</li> <li>Don't introduce all functions. Just pick a few that you think would be most useful. Users should be able to finish the tutorial within 30 minutes.</li> <li>Publish your tutorial on GitHub. Submit the link to your GitHub repository on Canvas.</li> </ul>"},{"location":"ChikomoRegionMask/","title":"Regionmask","text":"In\u00a0[\u00a0]: Copied! <pre># Code to install using conda:\nconda install -c conda-forge regionmask cartopy\n</pre> # Code to install using conda: conda install -c conda-forge regionmask cartopy <p>ii. Alternatively one can use pip to install the regionmask package, as well as the other packages used in this tutorial: cartopy.crs, xarray, numpy and matplotlib</p> In\u00a0[\u00a0]: Copied! <pre>#Code to install all required dependencies:\npip install regionmask\n</pre> #Code to install all required dependencies: pip install regionmask In\u00a0[8]: Copied! <pre># Importing packages that will be necessary for this tutorial\nimport regionmask\nimport cartopy.crs as ccrs \nimport xarray as xr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patheffects as pe\n</pre> # Importing packages that will be necessary for this tutorial import regionmask import cartopy.crs as ccrs  import xarray as xr import numpy as np import matplotlib.pyplot as plt import matplotlib.patheffects as pe In\u00a0[55]: Copied! <pre>#Create an object with various predefined regions\nsrex = regionmask.defined_regions.srex\n\n#Showing what srex looks like\nprint(srex)\n</pre> #Create an object with various predefined regions srex = regionmask.defined_regions.srex  #Showing what srex looks like print(srex) <pre>&lt;regionmask.Regions 'SREX'&gt;\nSource:   Seneviratne et al., 2012 (https://www.ipcc.ch/site/assets/uploads/2...\noverlap:  False\n\nRegions:\n 1 ALA       Alaska/N.W. Canada\n 2 CGI     Canada/Greenl./Icel.\n 3 WNA         W. North America\n 4 CNA         C. North America\n 5 ENA         E. North America\n..  ..                      ...\n22 EAS                  E. Asia\n23 SAS                  S. Asia\n24 SEA                S.E. Asia\n25 NAU             N. Australia\n26 SAU S. Australia/New Zealand\n\n[26 regions]\n</pre> In\u00a0[12]: Copied! <pre>srex.plot_regions()\n</pre> srex.plot_regions() Out[12]: <pre>&lt;AxesSubplot:&gt;</pre> In\u00a0[27]: Copied! <pre>#Import cartopy if it was not imported earlier\nimport cartopy.crs as ccrs\n\n#Creating a figure (map canvas) with projection Robisnon\nfigure, ax = plt.subplots(subplot_kw=dict(projection=ccrs.Robinson()))\n\n#Creating text labels for each region, but since path effects is being used the labels will not be in big gray boxes covering important information\ntext_kws = dict(\n    bbox=dict(color=\"none\"),\n    path_effects=[pe.withStroke(linewidth=2, foreground=\"w\")],\n    color=\"#67000d\",\n    fontsize=8,)\n\n# Plotting the predefined srex regions onto the map canvas above and adding the text labels\nsrex.plot_regions(ax=ax, line_kws=dict(lw=1), text_kws=text_kws)\n\n#Adding coastlines to the map\nax.coastlines()\n# Creating a global map\nax.set_global()\n</pre> #Import cartopy if it was not imported earlier import cartopy.crs as ccrs  #Creating a figure (map canvas) with projection Robisnon figure, ax = plt.subplots(subplot_kw=dict(projection=ccrs.Robinson()))  #Creating text labels for each region, but since path effects is being used the labels will not be in big gray boxes covering important information text_kws = dict(     bbox=dict(color=\"none\"),     path_effects=[pe.withStroke(linewidth=2, foreground=\"w\")],     color=\"#67000d\",     fontsize=8,)  # Plotting the predefined srex regions onto the map canvas above and adding the text labels srex.plot_regions(ax=ax, line_kws=dict(lw=1), text_kws=text_kws)  #Adding coastlines to the map ax.coastlines() # Creating a global map ax.set_global() In\u00a0[15]: Copied! <pre>srex.plot();\n</pre> srex.plot(); In\u00a0[26]: Copied! <pre>#import matplotlib.patheffect as pe if not done already\nimport matplotlib.patheffects as pe\n\n#The same text labels (text_kws) will be used as before \nax = srex.plot(\n    projection=ccrs.Robinson(), label=\"abbrev\", add_ocean=True, text_kws=text_kws)\n#Creating a global map\nax.set_global()\n</pre> #import matplotlib.patheffect as pe if not done already import matplotlib.patheffects as pe  #The same text labels (text_kws) will be used as before  ax = srex.plot(     projection=ccrs.Robinson(), label=\"abbrev\", add_ocean=True, text_kws=text_kws) #Creating a global map ax.set_global() In\u00a0[29]: Copied! <pre># regions can be selected by number, abbreviation or long name\nregions = [11, \"CEU\", \"S. Europe/Mediterranean\"]\n\n# choose a good projection for regional maps\nprojection = ccrs.LambertConformal(central_longitude=15)\n\n#Creating the plot\nax = srex[regions].plot(\n    add_ocean=True,\n    resolution=\"50m\",\n    projection=projection,\n    label=\"abbrev\",\n    text_kws=text_kws,)\n\n# fine tune the extent\nax.set_extent([-15, 45, 28, 76], crs=ccrs.PlateCarree())\n</pre> # regions can be selected by number, abbreviation or long name regions = [11, \"CEU\", \"S. Europe/Mediterranean\"]  # choose a good projection for regional maps projection = ccrs.LambertConformal(central_longitude=15)  #Creating the plot ax = srex[regions].plot(     add_ocean=True,     resolution=\"50m\",     projection=projection,     label=\"abbrev\",     text_kws=text_kws,)  # fine tune the extent ax.set_extent([-15, 45, 28, 76], crs=ccrs.PlateCarree()) <p>Assume you have two custom regions in the US, you can easily use these to create Regions:</p> In\u00a0[53]: Copied! <pre># Create two numpy arrays, one for each region\nUS1 = np.array([[-100.0, 30], [-100, 40], [-120, 35]])\nUS2 = np.array([[-100.0, 30], [-80, 30], [-80, 40], [-100, 40]])\n\n#Use the Regions function to convert the new variables into regions \nregionmask.Regions([US1, US2])\n</pre> # Create two numpy arrays, one for each region US1 = np.array([[-100.0, 30], [-100, 40], [-120, 35]]) US2 = np.array([[-100.0, 30], [-80, 30], [-80, 40], [-100, 40]])  #Use the Regions function to convert the new variables into regions  regionmask.Regions([US1, US2]) Out[53]: <pre>&lt;regionmask.Regions 'unnamed'&gt;\noverlap:  None\n\nRegions:\n0 r0 Region0\n1 r1 Region1\n\n[2 regions]</pre> <p>This creates two unnamed regions, but giving the regions additional information will make them more useful. To set names and abbrevs:</p> In\u00a0[50]: Copied! <pre># Creating a names variable for each region\nnames = [\"US_west\", \"US_east\"]\n# Creating an abbreviation variable for each region\nabbrevs = [\"USw\", \"USe\"]\n\n\n# Setting the name and abbreviation for each region\nUSregions = regionmask.Regions([US1, US2], names=names, abbrevs=abbrevs, name=\"US\")\n\n#Calling the new variable USregions to see if the names and abbreviations have been set\nUSregions\n</pre> # Creating a names variable for each region names = [\"US_west\", \"US_east\"] # Creating an abbreviation variable for each region abbrevs = [\"USw\", \"USe\"]   # Setting the name and abbreviation for each region USregions = regionmask.Regions([US1, US2], names=names, abbrevs=abbrevs, name=\"US\")  #Calling the new variable USregions to see if the names and abbreviations have been set USregions Out[50]: <pre>&lt;regionmask.Regions 'US'&gt;\noverlap:  None\n\nRegions:\n0 USw US_west\n1 USe US_east\n\n[2 regions]</pre> In\u00a0[52]: Copied! <pre>#Plotting the regions variable USregions using the plot() method from matplotlib and assigning abbreviations as the label\nax = USregions.plot(label=\"abbrev\")\n\n#Zooming into the two regions\n# fine tune the extent\nax.set_extent([225, 300, 25, 45], crs=ccrs.PlateCarree())\n</pre> #Plotting the regions variable USregions using the plot() method from matplotlib and assigning abbreviations as the label ax = USregions.plot(label=\"abbrev\")  #Zooming into the two regions # fine tune the extent ax.set_extent([225, 300, 25, 45], crs=ccrs.PlateCarree()) In\u00a0[56]: Copied! <pre># Start by defining a lon/lat grid with 1 degree grid spacing, where the points define the center of the grid. \nlon = np.arange(-179.5, 180)\nlat = np.arange(-89.5, 90)\n\n#Creating the mask that uses the srex predefined regions with 1 degree grid spacing\nmask = regionmask.defined_regions.srex.mask(lon, lat)\n\nmask\n</pre> # Start by defining a lon/lat grid with 1 degree grid spacing, where the points define the center of the grid.  lon = np.arange(-179.5, 180) lat = np.arange(-89.5, 90)  #Creating the mask that uses the srex predefined regions with 1 degree grid spacing mask = regionmask.defined_regions.srex.mask(lon, lat)  mask Out[56]: <pre>&lt;xarray.DataArray 'mask' (lat: 180, lon: 360)&gt;\narray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])\nCoordinates:\n  * lat      (lat) float64 -89.5 -88.5 -87.5 -86.5 -85.5 ... 86.5 87.5 88.5 89.5\n  * lon      (lon) float64 -179.5 -178.5 -177.5 -176.5 ... 177.5 178.5 179.5\nAttributes:\n    standard_name:  region\n    flag_values:    [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19...\n    flag_meanings:  ALA CGI WNA CNA ENA CAM AMZ NEB WSA SSA NEU CEU MED SAH W...</pre>xarray.DataArray'mask'<ul><li>lat: 180</li><li>lon: 360</li></ul><ul><li>nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>Coordinates: (2)<ul><li>lat(lat)float64-89.5 -88.5 -87.5 ... 88.5 89.5<pre>array([-89.5, -88.5, -87.5, -86.5, -85.5, -84.5, -83.5, -82.5, -81.5, -80.5,\n       -79.5, -78.5, -77.5, -76.5, -75.5, -74.5, -73.5, -72.5, -71.5, -70.5,\n       -69.5, -68.5, -67.5, -66.5, -65.5, -64.5, -63.5, -62.5, -61.5, -60.5,\n       -59.5, -58.5, -57.5, -56.5, -55.5, -54.5, -53.5, -52.5, -51.5, -50.5,\n       -49.5, -48.5, -47.5, -46.5, -45.5, -44.5, -43.5, -42.5, -41.5, -40.5,\n       -39.5, -38.5, -37.5, -36.5, -35.5, -34.5, -33.5, -32.5, -31.5, -30.5,\n       -29.5, -28.5, -27.5, -26.5, -25.5, -24.5, -23.5, -22.5, -21.5, -20.5,\n       -19.5, -18.5, -17.5, -16.5, -15.5, -14.5, -13.5, -12.5, -11.5, -10.5,\n        -9.5,  -8.5,  -7.5,  -6.5,  -5.5,  -4.5,  -3.5,  -2.5,  -1.5,  -0.5,\n         0.5,   1.5,   2.5,   3.5,   4.5,   5.5,   6.5,   7.5,   8.5,   9.5,\n        10.5,  11.5,  12.5,  13.5,  14.5,  15.5,  16.5,  17.5,  18.5,  19.5,\n        20.5,  21.5,  22.5,  23.5,  24.5,  25.5,  26.5,  27.5,  28.5,  29.5,\n        30.5,  31.5,  32.5,  33.5,  34.5,  35.5,  36.5,  37.5,  38.5,  39.5,\n        40.5,  41.5,  42.5,  43.5,  44.5,  45.5,  46.5,  47.5,  48.5,  49.5,\n        50.5,  51.5,  52.5,  53.5,  54.5,  55.5,  56.5,  57.5,  58.5,  59.5,\n        60.5,  61.5,  62.5,  63.5,  64.5,  65.5,  66.5,  67.5,  68.5,  69.5,\n        70.5,  71.5,  72.5,  73.5,  74.5,  75.5,  76.5,  77.5,  78.5,  79.5,\n        80.5,  81.5,  82.5,  83.5,  84.5,  85.5,  86.5,  87.5,  88.5,  89.5])</pre></li><li>lon(lon)float64-179.5 -178.5 ... 178.5 179.5<pre>array([-179.5, -178.5, -177.5, ...,  177.5,  178.5,  179.5])</pre></li></ul></li><li>Indexes: (2)<ul><li>latPandasIndex<pre>PandasIndex(Index([-89.5, -88.5, -87.5, -86.5, -85.5, -84.5, -83.5, -82.5, -81.5, -80.5,\n       ...\n        80.5,  81.5,  82.5,  83.5,  84.5,  85.5,  86.5,  87.5,  88.5,  89.5],\n      dtype='float64', name='lat', length=180))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.5, -178.5, -177.5, -176.5, -175.5, -174.5, -173.5, -172.5, -171.5,\n       -170.5,\n       ...\n        170.5,  171.5,  172.5,  173.5,  174.5,  175.5,  176.5,  177.5,  178.5,\n        179.5],\n      dtype='float64', name='lon', length=360))</pre></li></ul></li><li>Attributes: (3)standard_name :regionflag_values :[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  25 26]flag_meanings :ALA CGI WNA CNA ENA CAM AMZ NEB WSA SSA NEU CEU MED SAH WAF EAF SAF NAS WAS CAS TIB EAS SAS SEA NAU SAU</li></ul> In\u00a0[38]: Copied! <pre>#Verifying the shape of mask\nprint('Lat shape is:', lat.shape)\nprint('Lon shape is:', lon.shape)\nprint('Mask shape is:', mask.shape)\n</pre> #Verifying the shape of mask print('Lat shape is:', lat.shape) print('Lon shape is:', lon.shape) print('Mask shape is:', mask.shape) <pre>Lat shape is: (180,)\nLon shape is: (360,)\nMask shape is: (180, 360)\n</pre> In\u00a0[54]: Copied! <pre># Import if you have not done so before\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\n\n#Creating the mapping canvas and setting the projection to PlateCarree\nf, ax = plt.subplots(subplot_kw=dict(projection=ccrs.PlateCarree()))\n#Adding Coastlines\nax.coastlines()\n\n#Adding outlines for each srex region to see if the mask lines us\nregionmask.defined_regions.srex.plot(ax=ax, add_label=False, line_kws=dict(lw=0.5, color=\"0.2\"))\n\n#Plotting the new masks\nmask.plot(ax=ax, transform=ccrs.PlateCarree(), add_colorbar=False)\n</pre> # Import if you have not done so before import cartopy.crs as ccrs import matplotlib.pyplot as plt  #Creating the mapping canvas and setting the projection to PlateCarree f, ax = plt.subplots(subplot_kw=dict(projection=ccrs.PlateCarree())) #Adding Coastlines ax.coastlines()  #Adding outlines for each srex region to see if the mask lines us regionmask.defined_regions.srex.plot(ax=ax, add_label=False, line_kws=dict(lw=0.5, color=\"0.2\"))  #Plotting the new masks mask.plot(ax=ax, transform=ccrs.PlateCarree(), add_colorbar=False) Out[54]: <pre>&lt;cartopy.mpl.geocollection.GeoQuadMesh at 0x7f5c29e809d0&gt;</pre> In\u00a0[80]: Copied! <pre>airtemps = xr.tutorial.load_dataset(\"air_temperature\")\n</pre> airtemps = xr.tutorial.load_dataset(\"air_temperature\") In\u00a0[81]: Copied! <pre>#Displaying the data\nairtemps\n</pre> #Displaying the data airtemps Out[81]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>time: 2920</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> In\u00a0[86]: Copied! <pre># choose a good projection for regional maps\nproj = ccrs.LambertConformal(central_longitude=-100)\n\n#Creating the map canvas\nax = plt.subplot(111, projection=proj)\n\n#Plotting the first time step an dtransforming the data projection from PlateCarree to the map canvas projection\nairtemps.isel(time=1).air.plot.pcolormesh(ax=ax, transform=ccrs.PlateCarree())\n#Adding coastline\nax.coastlines();\n</pre> # choose a good projection for regional maps proj = ccrs.LambertConformal(central_longitude=-100)  #Creating the map canvas ax = plt.subplot(111, projection=proj)  #Plotting the first time step an dtransforming the data projection from PlateCarree to the map canvas projection airtemps.isel(time=1).air.plot.pcolormesh(ax=ax, transform=ccrs.PlateCarree()) #Adding coastline ax.coastlines(); In\u00a0[83]: Copied! <pre>mask = regionmask.defined_regions.srex.mask(airtemps)\n</pre> mask = regionmask.defined_regions.srex.mask(airtemps) <p>Region mask is able to automatically detect whether the longitude needs to be wrapped around, i.e. if the regions extend from -180\u00b0 E to 180\u00b0 W, while the grid goes from 0\u00b0 to 360\u00b0 W as in our example:</p> In\u00a0[87]: Copied! <pre>lon = airtemps.lon.values\nprint(\"Grid extent:    {:3.0f}\u00b0E to {:3.0f}\u00b0E\".format(lon.min(), lon.max()))\n\nbounds = regionmask.defined_regions.srex.bounds_global\nprint(\"Region extent: {:3.0f}\u00b0E to {:3.0f}\u00b0E\".format(bounds[0], bounds[2]))\n</pre> lon = airtemps.lon.values print(\"Grid extent:    {:3.0f}\u00b0E to {:3.0f}\u00b0E\".format(lon.min(), lon.max()))  bounds = regionmask.defined_regions.srex.bounds_global print(\"Region extent: {:3.0f}\u00b0E to {:3.0f}\u00b0E\".format(bounds[0], bounds[2])) <pre>Grid extent:    200\u00b0E to 330\u00b0E\nRegion extent: -168\u00b0E to 180\u00b0E\n</pre> In\u00a0[88]: Copied! <pre># Creating the map canvas\nax = plt.subplot(111, projection = ccrs.AlbersEqualArea(central_lon, central_lat))\n\nlow = mask.min()\nhigh = mask.max()\n\nlevels = np.arange(low - 0.5, high + 1)\n\n#Plotting the mask using pcolormesh\nh = mask.plot.pcolormesh(\n    ax=ax, transform=ccrs.PlateCarree(), levels=levels, add_colorbar=False)\n\n\n# for colorbar: find abbreviations of all regions that were selected\n\n#1. Assigning a unique value for each region\nreg = np.unique(mask.values)\n#2. Ensuring to only collect values for areas within a mask (isnan Tests element-wise for NaN and return result as a boolean array)\nreg = reg[~np.isnan(reg)]\n#3. Assigning the abbreviation of each srex region to the corresponding mask regions \nabbrevs = regionmask.defined_regions.srex[reg].abbrevs\n\n#Assigning the colorbar to the figure\ncbar = plt.colorbar(h, orientation=\"horizontal\", fraction=0.075, pad=0.05)\n\n#Editing the colorbar and adding ticks to go with each region\ncbar.set_ticks(reg)\ncbar.set_ticklabels(abbrevs)\ncbar.set_label(\"Region\")\n\n#Adding coastlines to the map canvas\nax.coastlines()\n\n#Zooming into the area and fine tune the extent\nax.set_extent([200, 330, 10, 75], crs=ccrs.PlateCarree())\n</pre> # Creating the map canvas ax = plt.subplot(111, projection = ccrs.AlbersEqualArea(central_lon, central_lat))  low = mask.min() high = mask.max()  levels = np.arange(low - 0.5, high + 1)  #Plotting the mask using pcolormesh h = mask.plot.pcolormesh(     ax=ax, transform=ccrs.PlateCarree(), levels=levels, add_colorbar=False)   # for colorbar: find abbreviations of all regions that were selected  #1. Assigning a unique value for each region reg = np.unique(mask.values) #2. Ensuring to only collect values for areas within a mask (isnan Tests element-wise for NaN and return result as a boolean array) reg = reg[~np.isnan(reg)] #3. Assigning the abbreviation of each srex region to the corresponding mask regions  abbrevs = regionmask.defined_regions.srex[reg].abbrevs  #Assigning the colorbar to the figure cbar = plt.colorbar(h, orientation=\"horizontal\", fraction=0.075, pad=0.05)  #Editing the colorbar and adding ticks to go with each region cbar.set_ticks(reg) cbar.set_ticklabels(abbrevs) cbar.set_label(\"Region\")  #Adding coastlines to the map canvas ax.coastlines()  #Zooming into the area and fine tune the extent ax.set_extent([200, 330, 10, 75], crs=ccrs.PlateCarree()) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"ChikomoRegionMask/#regionmask","title":"Regionmask\u00b6","text":""},{"location":"ChikomoRegionMask/#by-mazvita-chikomo-11202023","title":"By Mazvita Chikomo                   11/20/2023\u00b6","text":""},{"location":"ChikomoRegionMask/#creates-masks-for-geographic-regions","title":"Creates Masks for geographic regions\u00b6","text":"<p>When working with gridded data such as climate model output or reanalysis data it is often important to create regional averages, e.g., over countries, continents or regions defined in the literature. To do so we need to know for each grid point to which region it belongs to. RegionMask allows us to do just that!</p>"},{"location":"ChikomoRegionMask/#regionmask-is-a-package-that","title":"Regionmask is a package that:\u00b6","text":"<ul> <li>Can be used sed to create masks of geographic regions for arbitrary longitude and latitude grids.</li> <li>These masks indicate which region a gridpoint belongs to.</li> <li>They come in two varian-2D integer Masks-3D Boolean Masks.</li> <li>takes great care to consistently treat gridpoints and overlapping regions,</li> <li>contains a number of defined regions, including:</li> <li>countries, landmasks, regions used in the scientific literature, can plot figures of these regions</li> <li>supports using arbitrary existing or user-defined region definitions:regions defined as shapefiles can be accessed via geopandas, user-defined regions can be created via numpy or shapely</li> </ul> <p>This tutorial will highlight some of the utilities of using the regionmask package with additional information at the end of the tutorial</p> <p>References: https://regionmask.readthedocs.io/en/stable/index.html#s://github.com/hydpy-dev/hydpyub.com/hydpy-dev/hydpy</p>"},{"location":"ChikomoRegionMask/#where-to-start","title":"Where to start:\u00b6","text":""},{"location":"ChikomoRegionMask/#1-installation-instructions","title":"1. Installation Instructions\u00b6","text":"<p>In this tutorial there are two ways to install regionmask:</p> <p>i. One can use conda, which at time can be the easiest way to install, however in some cases you may run into this issue:</p> <p>\"Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: / failed with repodata from current_repodata.json, will retry with next repodata source.\"</p>"},{"location":"ChikomoRegionMask/#2-importing-various-packages-required-for-this-tutorial","title":"2. Importing various packages required for this tutorial:\u00b6","text":"<p>Useful Packages:</p> <p>regionmask: Creates masks for different regions.</p> <p>cartopy: Cartopy is a Python package designed for geospatial data processing in order to produce maps and other geospatial data analyses.</p> <p>xarray: An open source project and Python package that introduces labels in the form of dimensions, coordinates, and attributes on top of raw NumPy-like arrays, which allows for more intuitive, more concise, and less error-prone user experience.</p> <p>numpy: A Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.</p> <p>matplotlib: a comprehensive library for creating plots and statistical visualizations in Python</p>"},{"location":"ChikomoRegionMask/#example-1-creating-a-mask-using-predefined-regions-using-defined_regions-function","title":"Example 1: Creating a mask using predefined regions using defined_regions function\u00b6","text":"<p>Using the function regionmask.defined_regions can be made from the outline of the countries obtained from Natural Earth. They are automatically downloaded, cached1 and opened with geopandas. The following countries and regions are defined in regionmask.</p> <ul> <li>Countries 1:110m</li> <li>Countries 1:50</li> <li>Countries 1:10m</li> <li>US States 1:50m</li> <li>US States 1:10m</li> </ul> <p>For this example SREX regions will be used as the predefined regions(Seneviratne et al., 2012)</p>"},{"location":"ChikomoRegionMask/#every-region-has-two-plotting-functions-that-draws-the-outlines-of-all-regions","title":"Every region has two plotting functions that draws the outlines of all regions:\u00b6","text":"<ul> <li>plot(): draws the region polygons on a cartopy GeoAxes (map)</li> <li>plot_regions(): draws the the region polygons onlynes:</li> </ul>"},{"location":"ChikomoRegionMask/#visualizing-the-different-regions-using-plot_regions","title":"Visualizing the different regions using plot_regions()\u00b6","text":"<p>Calling .plot_regions() on any region without any arguments draws the region polygons only</p>"},{"location":"ChikomoRegionMask/#adding-a-map-to-the-polygons-using-plot_regions-and-matplotlibpatheffects","title":"Adding a map to the polygons using plot_regions and matplotlib.patheffects:\u00b6","text":"<p>To create a map one can use the subplots and plot method from Matplotlib, which has a large number of arguments to adjust the layout of the axes. For example, you can pass a custom projection, the labels can display the abbreviation insead of the region number, one can add coastlines etc.</p> <p>This example also shows how to use matplotlib.patheffects to ensure the labels are easily readable without covering too much of the map (compare to the map above):</p> <ul> <li>import matplotlib.patheffect as pe (if you haven't already)</li> </ul> <p>We will also use Cartopy to produce maps and other geospatial data analyses.</p> <ul> <li>import cartopy.crs as ccrs (if you haven't already)</li> </ul>"},{"location":"ChikomoRegionMask/#visualizing-the-different-regions-using-plot","title":"Visualizing the different regions using plot()\u00b6","text":"<p>Calling .plot() on any region without any arguments draws the default map with a PlateCarree() projection and includes the coastlines:</p>"},{"location":"ChikomoRegionMask/#creating-a-more-detailed-global-map-using-plot-and-matplotlibpatheffects","title":"Creating a more detailed global map using .plot() and matplotlib.patheffects\u00b6","text":""},{"location":"ChikomoRegionMask/#plotting-only-a-subset-of-the-regions","title":"Plotting only a subset of the regions:\u00b6","text":"<p>To plot a selection of regions subset them using indexing:</p>"},{"location":"ChikomoRegionMask/#example-2-creating-your-own-regions-using-the-regions-function","title":"Example 2: Creating your own regions using the Regions Function\u00b6","text":""},{"location":"ChikomoRegionMask/#plotting-the-assigned-regions-onto-a-map","title":"Plotting the assigned regions onto a map:\u00b6","text":""},{"location":"ChikomoRegionMask/#example-3-using-the-mask-function-to-create-a-mask-to-define-points-to-each-regions","title":"Example 3. Using the Mask Function to create a mask to define points to each regions\u00b6","text":"<p>The function mask determines which gridpoints lie within the polygon making up each region:</p>"},{"location":"ChikomoRegionMask/#-mask-is-now-an-xarraydataset-with-shape-lat-x-lon","title":"- Mask is now an xarray.Dataset with shape (lat x lon).\u00b6","text":""},{"location":"ChikomoRegionMask/#-if-a-point-lies-outside-of-a-region-a-value-of-nan-is-given","title":"- If a point lies outside of a region a value of NAN is given\u00b6","text":""},{"location":"ChikomoRegionMask/#visualizing-the-masks-created-using-cartopy-and-matplotlib","title":"Visualizing the masks created using cartopy and matplotlib\u00b6","text":""},{"location":"ChikomoRegionMask/#part-4-applying-regionmask-to-real-data","title":"Part 4: Applying regionmask to real data\u00b6","text":"<p>For this tutorial we will use the NCEP reanalysis air temperature subsets which creates an air temperature field over North America. This data is freely available as an open dataset as a part of the Xarray tutorial library.</p> <p>https://docs.xarray.dev/en/stable/generated/xarray.tutorial.open_dataset.html</p>"},{"location":"ChikomoRegionMask/#visualizing-what-the-first-time-step-in-the-dataset-looks-like-using-cartopy-and-indexing","title":"Visualizing what the first time step in the dataset looks like using cartopy and indexing\u00b6","text":""},{"location":"ChikomoRegionMask/#creating-a-mask-that-matches-the-srex-predefined-regions-to-the-air-temperature-data","title":"Creating a mask that matches the srex predefined regions to the air temperature data\u00b6","text":""},{"location":"ChikomoRegionMask/#now-plotting-the-mask-over-the-data","title":"Now  plotting the mask over the data\u00b6","text":""},{"location":"ChikomoRegionMask/#additional-information","title":"Additional Information\u00b6","text":""},{"location":"ChikomoRegionMask/#useful-api-information-from-the-regionmask-package","title":"Useful API information from the regionmask package:\u00b6","text":""},{"location":"ChikomoRegionMask/#top-level-functions","title":"Top Level Functions\u00b6","text":"<ul> <li>mask_geopandas(geodataframe, lon_or_obj[, ...]): Creates a 2D float mask of a set of regions for the given lat/ lon grid</li> <li>(geodataframe, lon_or_obj): Creates a 3D boolean mask of a set of regions for the given lat/ lon grid</li> <li>from_geopandas(geodataframe, *[, numbers, ...]): Creates regionmask.Regions from a geopandas.GeoDataFrame.</li> <li>flatten_3D_mask(mask_3D):Flattens 3D masks</li> <li>plot_3D_mask(mask_3D, **kwargs): Flattens and plot 3D masks</li> <li>set_options(**kwargs):Sets options for regionmask in a controlled context.</li> <li>get_options():Gets options for regionmask.et options for regionmask.</li> </ul> <p>Source: https://regionmask.readthedocs.io/en/stable/api.html</p>"},{"location":"ChikomoRegionMask/#references","title":"References:\u00b6","text":"<p>https://regionmask.readthedocs.io/en/stable/notebooks/plotting.html</p> <p>https://matplotlib.org/</p> <p>https://docs.xarray.dev/en/stable/</p> <p>https://numpy.org/doc/stable/</p> <p>https://www.ibm.com/docs/en/watson-studio-local/1.2.3?topic=notebooks-markdown-jupyter-cheatsheet</p> <p>https://docs.xarray.dev/en/stable/generated/xarray.tutorial.open_dataset.html https://numpy.org/doc/stable/reference/generated/numpy.isnan.html</p>"},{"location":"Danraj_Rasterio/","title":"RASTERIO","text":"<p>Before Rasterio there was one Python option for accessing the many different kind of raster data files used in the GIS field: the Python bindings distributed with the Geospatial Data Abstraction Library, GDAL. These bindings extend Python, but provide little abstraction for GDAL\u2019s C API. This means that Python programs using them tend to read and run like C programs.</p> <p>For more info: https://rasterio.readthedocs.io/en/latest/intro.html</p> <p>Rasterio works with Python 3.8+, Numpy 1.18+, and GDAL 3.1+.</p> <ul> <li>pip install rasterio</li> </ul> <ul> <li>conda install -c conda-forge rasterio</li> </ul> In\u00a0[1]: Copied! <pre># Import Rasterio\nimport rasterio\nfrom rasterio.plot import show\n</pre> # Import Rasterio import rasterio from rasterio.plot import show In\u00a0[2]: Copied! <pre># Read a DEM file\nsrc=rasterio.open('DEM.tif')\n</pre> # Read a DEM file src=rasterio.open('DEM.tif') In\u00a0[3]: Copied! <pre>show(src)\n</pre> show(src) Out[3]: <pre>&lt;Axes: &gt;</pre> In\u00a0[4]: Copied! <pre>src.meta #Reads the metadata\n</pre> src.meta #Reads the metadata Out[4]: <pre>{'driver': 'GTiff',\n 'dtype': 'float32',\n 'nodata': -9999.0,\n 'width': 10814,\n 'height': 21613,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.0002777777780359812, 0.0, -76.00194669160615,\n        0.0, -0.00027777777803598107, 43.00195317827816)}</pre> In\u00a0[5]: Copied! <pre>src.bounds # Boounding Box\n</pre> src.bounds # Boounding Box Out[5]: <pre>BoundingBox(left=-76.00194669160615, bottom=36.9983420615865, right=-72.99805779992505, top=43.00195317827816)</pre> In\u00a0[6]: Copied! <pre>from rasterio.windows import Window\n#Cropping the DEM\nwindow = Window(500,500 ,3000, 3000) # parameters are (col_off, row_off, width, height)\ncrp = src.read(window=window)\nshow(crp)\n</pre> from rasterio.windows import Window #Cropping the DEM window = Window(500,500 ,3000, 3000) # parameters are (col_off, row_off, width, height) crp = src.read(window=window) show(crp) Out[6]: <pre>&lt;Axes: &gt;</pre> <p>You can crop a raster dataset using geographic coordinates in Rasterio, but it involves a few more steps compared to cropping with pixel coordinates. Since raster datasets are inherently grid-based and use row and column indices, you'll first need to convert the geographic coordinates to pixel coordinates.</p> <p>Resampling involves changing the spatial resolution of the raster data, either increasing (upsampling) or decreasing (downsampling) the number of pixels.</p> In\u00a0[47]: Copied! <pre>from rasterio.enums import Resampling\ndata = src.read(\n     out_shape=(\n        src.count,\n        int(src.height * 0.01),  # decreasing the resolution\n        int(src.width *0.01)\n    ),\n        resampling=Resampling.bilinear\n)\n\n    # Update the metadata to reflect the new shape\ntransform = src.transform * src.transform.scale(\n    (src.width / data.shape[-2]),\n    (src.height / data.shape[-1])\n)\n</pre> from rasterio.enums import Resampling data = src.read(      out_shape=(         src.count,         int(src.height * 0.01),  # decreasing the resolution         int(src.width *0.01)     ),         resampling=Resampling.bilinear )      # Update the metadata to reflect the new shape transform = src.transform * src.transform.scale(     (src.width / data.shape[-2]),     (src.height / data.shape[-1]) ) In\u00a0[48]: Copied! <pre>show(data)\n</pre> show(data) Out[48]: <pre>&lt;Axes: &gt;</pre> In\u00a0[50]: Copied! <pre>resampled_height = data.shape[-1]  # Height of the resampled image\nresampled_width = data.shape[-2]\n</pre> resampled_height = data.shape[-1]  # Height of the resampled image resampled_width = data.shape[-2]   In\u00a0[51]: Copied! <pre>resampled_height\n</pre> resampled_height Out[51]: <pre>108</pre> In\u00a0[52]: Copied! <pre>resampled_width\n</pre> resampled_width Out[52]: <pre>216</pre> In\u00a0[9]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[34]: Copied! <pre>def calculate_slope(dem, pixel_size):\n\"\"\"Calculate slope from a DEM.\"\"\"\n    dx, dy = np.gradient(dem, pixel_size[0], pixel_size[1])\n    slope = np.arctan(np.sqrt(dx**2 + dy**2)) * (180 / np.pi)  # Convert to degrees\n    return slope\naffine=src.transform\ndem=src.read(1)\n</pre> def calculate_slope(dem, pixel_size):     \"\"\"Calculate slope from a DEM.\"\"\"     dx, dy = np.gradient(dem, pixel_size[0], pixel_size[1])     slope = np.arctan(np.sqrt(dx**2 + dy**2)) * (180 / np.pi)  # Convert to degrees     return slope affine=src.transform dem=src.read(1) In\u00a0[35]: Copied! <pre>slope = calculate_slope(dem, affine)\n</pre> slope = calculate_slope(dem, affine) <pre>/home/dl1197/miniconda3/envs/rcaes_env/lib/python3.9/site-packages/numpy/lib/function_base.py:1238: RuntimeWarning: divide by zero encountered in divide\n  out[tuple(slice1)] = (f[tuple(slice4)] - f[tuple(slice2)]) / (2. * ax_dx)\n/home/dl1197/miniconda3/envs/rcaes_env/lib/python3.9/site-packages/numpy/lib/function_base.py:1238: RuntimeWarning: invalid value encountered in divide\n  out[tuple(slice1)] = (f[tuple(slice4)] - f[tuple(slice2)]) / (2. * ax_dx)\n/home/dl1197/miniconda3/envs/rcaes_env/lib/python3.9/site-packages/numpy/lib/function_base.py:1259: RuntimeWarning: divide by zero encountered in divide\n  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_0\n/home/dl1197/miniconda3/envs/rcaes_env/lib/python3.9/site-packages/numpy/lib/function_base.py:1259: RuntimeWarning: invalid value encountered in divide\n  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_0\n/home/dl1197/miniconda3/envs/rcaes_env/lib/python3.9/site-packages/numpy/lib/function_base.py:1266: RuntimeWarning: divide by zero encountered in divide\n  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_n\n/home/dl1197/miniconda3/envs/rcaes_env/lib/python3.9/site-packages/numpy/lib/function_base.py:1266: RuntimeWarning: invalid value encountered in divide\n  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_n\n</pre> In\u00a0[36]: Copied! <pre>print(slope) #Values are in degrees\n</pre> print(slope) #Values are in degrees <pre>[[nan nan nan ... nan nan nan]\n [90. 90. 90. ... nan nan nan]\n [90. 90. 90. ... nan nan nan]\n ...\n [90. 90. nan ... nan nan nan]\n [90. 90. nan ... nan nan nan]\n [90. 90. nan ... nan nan nan]]\n</pre> In\u00a0[13]: Copied! <pre>from rasterio.plot import show_hist\n</pre> from rasterio.plot import show_hist In\u00a0[14]: Copied! <pre>x=show_hist(crp, bins=10,stacked='False',edgecolor='black', alpha=0.7)\n</pre> x=show_hist(crp, bins=10,stacked='False',edgecolor='black', alpha=0.7) <p>Cloud-Optimized GeoTIFF Support Read from Remote Sources: Directly read from and write to cloud-optimized GeoTIFFs, a standard format for cloud-based geospatial data storage.</p> <p>If you are using gdal you can use most of the functions of gdal using rasterio.</p> <p>Creating Rasters from arryas.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Danraj_Rasterio/#rasterio","title":"RASTERIO\u00b6","text":""},{"location":"Danraj_Rasterio/#by-danraj-lamichhane","title":"By: Danraj Lamichhane\u00b6","text":""},{"location":"Danraj_Rasterio/#1introduction-to-rasterio","title":"1.Introduction to Rasterio\u00b6","text":""},{"location":"Danraj_Rasterio/#2-installation-and-setup","title":"2. Installation and Setup\u00b6","text":""},{"location":"Danraj_Rasterio/#3-using-rasterio","title":"3. Using Rasterio\u00b6","text":""},{"location":"Danraj_Rasterio/#31-cropping","title":"3.1 Cropping\u00b6","text":""},{"location":"Danraj_Rasterio/#32-resampling","title":"3.2 Resampling\u00b6","text":""},{"location":"Danraj_Rasterio/#33-raster-calculation-and-aanlysis","title":"3.3 Raster Calculation and Aanlysis\u00b6","text":""},{"location":"Danraj_Rasterio/#331-slope-calculation","title":"3.3.1 Slope Calculation:\u00b6","text":"<p>Slope represents the rate of change in elevation at each pixel. It is a crucial factor in many geospatial analyses.</p>"},{"location":"Danraj_Rasterio/#332-histogram","title":"3.3.2 Histogram\u00b6","text":""},{"location":"Danraj_Rasterio/#other-functions","title":"Other Functions\u00b6","text":""},{"location":"Final_Project/","title":"Final Project","text":""},{"location":"Final_Project/#part-i-individual-project-20","title":"Part I: Individual Project (20%)","text":"<p>The goal of the final project is to assess your ability to combine and apply the skills you have learned in class in the context of a real-world research problem. Our class has mostly focused on tools for data analysis and visualization, so this must be the focus of your final project. Specifically, we seek to assess your ability to do the following tasks: \u2022   Discover and download real datasets in standard formats (e.g. CSV, netCDF) \u2022   Load the data into pandas or xarray, performing any necessary data cleanup (dealing with missing values, proper time encoding, etc.) along the way. \u2022   Perform realistic scientific calculation involving, for example tasks such as grouping, aggregating, and applying mathematical formulas. \u2022   Visualize your results in well-formatted plots.</p>"},{"location":"Final_Project/#part-ii-reproducing-another-students-project-10","title":"Part II: Reproducing Another Student\u2019s Project (10%)","text":"<p>The goal of the second part is to assess the reproducibility of the student\u2019s project, and whether the students can reproduce and collaborate with others on code development. Our class focuses on conducting open-source research that are transparent, accessible, reproducible and inclusive, so your final project should demonstrate your understanding and ability to perform open-source research. We seek to assess your ability to: \u2022   Clearly document your analysis to make it reproducible. \u2022   Reproduce the other student\u2019s final project. \u2022   Bonus points will be given if the students submit pull requests and issues for code development. </p>"},{"location":"HeinleArgoPy/","title":"ArgoPy","text":"<p>Argo is an international research program that deploys floats (as seen below) to collect physical and chemical data in the ocean at a variety of depths.</p> <p></p> <p>Image source: NOAA</p> <p>ArgoPy is an effort to make data collected through the Argo network more accessible to the public as well as researchers. It contains up-to-date data from Argo floats from all around the world, as well as tools to clean and visualize the data.</p> <p>Using the function below, users can see a dashboard of active Argo floats.</p> In\u00a0[1]: Copied! <pre>from argopy import dashboard\n\ndashboard()\n</pre> from argopy import dashboard  dashboard() Out[1]: <p>Doumentation can be found here: https://argopy.readthedocs.io/en/latest/index.html</p> To install: conda install -c conda-forge argopy In\u00a0[2]: Copied! <pre>import argopy\nfrom argopy import DataFetcher\n</pre> import argopy from argopy import DataFetcher <p>One of the strengths of ArgoPy is that the DataFetcher function provides easy access to up-to-date Argo data.</p> In\u00a0[3]: Copied! <pre># three methods of data selection\n# by region (list: [lon_min, lon_max, lat_min, lat_max, depth_min, depth_max, date_start, date_end])\ndata_region = DataFetcher().region([-75, -65, 20, 30, 0, 100, '2015-01', '2015-02'])\n\n# by World Meteorological Organization (WMO) float ID \ndata_floats = DataFetcher().float([6902746, 6902757])\n\n# by profile (WMO float ID, [list of cycles])\ndata_profile = DataFetcher().profile(6902746, [10, 20, 30, 40, 50])\n</pre> # three methods of data selection # by region (list: [lon_min, lon_max, lat_min, lat_max, depth_min, depth_max, date_start, date_end]) data_region = DataFetcher().region([-75, -65, 20, 30, 0, 100, '2015-01', '2015-02'])  # by World Meteorological Organization (WMO) float ID  data_floats = DataFetcher().float([6902746, 6902757])  # by profile (WMO float ID, [list of cycles]) data_profile = DataFetcher().profile(6902746, [10, 20, 30, 40, 50]) <p>There are three kinds of plots built into ArgoPy:</p> <ol> <li>'trajectory', which shows the location of the float (default)</li> <li>'profiler' or 'dac', which show metadata histograms</li> <li>'qc_altimetry', which shows quality control information for each float</li> </ol> In\u00a0[4]: Copied! <pre>data_region.plot()\n</pre> data_region.plot() Out[4]: <pre>(&lt;Figure size 900x540 with 1 Axes&gt;,\n &lt;GeoAxesSubplot:xlabel='longitude', ylabel='latitude'&gt;)</pre> In\u00a0[5]: Copied! <pre>data_floats.plot('dac')\n</pre> data_floats.plot('dac') Out[5]: <pre>(&lt;Figure size 900x540 with 1 Axes&gt;, &lt;AxesSubplot:xlabel='Number of profiles'&gt;)</pre> In\u00a0[6]: Copied! <pre>data_profile.plot('qc_altimetry')\n</pre> data_profile.plot('qc_altimetry') <pre>interactive(children=(Dropdown(description='Float', options=('6902746',), value='6902746'), Output()), _dom_cl\u2026</pre> Out[6]: <pre>&lt;function argopy.plot.plot.open_sat_altim_report.&lt;locals&gt;.f(Float)&gt;</pre> In\u00a0[7]: Copied! <pre>import pandas as pd\nfrom matplotlib import pyplot as plt\nimport xarray as xr\nimport cartopy\nimport cartopy.crs as ccrs\n</pre> import pandas as pd from matplotlib import pyplot as plt import xarray as xr import cartopy import cartopy.crs as ccrs In\u00a0[8]: Copied! <pre># convert argo data to pandas dataframe\nargo_pd = data_profile.to_dataframe()\n</pre> # convert argo data to pandas dataframe argo_pd = data_profile.to_dataframe() In\u00a0[9]: Copied! <pre>argo_pd\n</pre> argo_pd Out[9]: CYCLE_NUMBER DATA_MODE DIRECTION PLATFORM_NUMBER POSITION_QC PRES PRES_ERROR PRES_QC PSAL PSAL_ERROR PSAL_QC TEMP TEMP_ERROR TEMP_QC TIME_QC LATITUDE LONGITUDE TIME N_POINTS 0 10 D A 6902746 1 3.0 2.4 1 35.085999 0.01 1 28.636999 0.002 1 1 19.242 -59.534 2017-08-22 06:55:00 1 10 D A 6902746 1 4.0 2.4 1 35.084999 0.01 1 28.638000 0.002 1 1 19.242 -59.534 2017-08-22 06:55:00 2 10 D A 6902746 1 5.0 2.4 1 35.085999 0.01 1 28.636999 0.002 1 1 19.242 -59.534 2017-08-22 06:55:00 3 10 D A 6902746 1 6.0 2.4 1 35.087002 0.01 1 28.636000 0.002 1 1 19.242 -59.534 2017-08-22 06:55:00 4 10 D A 6902746 1 7.0 2.4 1 35.085999 0.01 1 28.639000 0.002 1 1 19.242 -59.534 2017-08-22 06:55:00 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 534 50 D A 6902746 1 1938.0 2.4 1 34.987999 0.01 1 3.755000 0.002 1 1 19.650 -57.767 2018-03-10 06:58:00 535 50 D A 6902746 1 1963.0 2.4 1 34.987000 0.01 1 3.721000 0.002 1 1 19.650 -57.767 2018-03-10 06:58:00 536 50 D A 6902746 1 1988.0 2.4 1 34.985001 0.01 1 3.672000 0.002 1 1 19.650 -57.767 2018-03-10 06:58:00 537 50 D A 6902746 1 2013.0 2.4 1 34.983002 0.01 1 3.634000 0.002 1 1 19.650 -57.767 2018-03-10 06:58:00 538 50 D A 6902746 1 2030.0 2.4 1 34.983002 0.01 1 3.618000 0.002 1 1 19.650 -57.767 2018-03-10 06:58:00 <p>539 rows \u00d7 18 columns</p> In\u00a0[11]: Copied! <pre># plot argo data with matplotlib\n\nfig, ax = plt.subplots()\nax.scatter(x = argo_pd.TEMP,\n           y = argo_pd.PRES,\n           marker = '.')\nplt.xlabel('Temperature ($^\\circ$C)')\nax.xaxis.set_ticks_position('top')         # label x-axis at top of plot\nax.xaxis.set_label_position('top')\nplt.ylabel('Pressure (dbar)')              # pressure is an analog for depth\nplt.gca().invert_yaxis()                   # invert y-axis to reflect depth\n</pre> # plot argo data with matplotlib  fig, ax = plt.subplots() ax.scatter(x = argo_pd.TEMP,            y = argo_pd.PRES,            marker = '.') plt.xlabel('Temperature ($^\\circ$C)') ax.xaxis.set_ticks_position('top')         # label x-axis at top of plot ax.xaxis.set_label_position('top') plt.ylabel('Pressure (dbar)')              # pressure is an analog for depth plt.gca().invert_yaxis()                   # invert y-axis to reflect depth In\u00a0[12]: Copied! <pre># convert argo data to xarray dataset\nargo_xr = data_region.to_xarray()\n</pre> # convert argo data to xarray dataset argo_xr = data_region.to_xarray() In\u00a0[13]: Copied! <pre># perform calculations\nargo_xr['PSAL'].mean()\n</pre> # perform calculations argo_xr['PSAL'].mean() Out[13]: <pre>&lt;xarray.DataArray 'PSAL' ()&gt;\narray(36.588856, dtype=float32)</pre>xarray.DataArray'PSAL'<ul><li>36.59<pre>array(36.588856, dtype=float32)</pre></li><li>Coordinates: (0)<ul></ul></li><li>Indexes: (0)<ul></ul></li><li>Attributes: (0)</li></ul> In\u00a0[37]: Copied! <pre># convert argo data to pandas dataframe\nargo_floats = data_floats.to_dataframe()\n</pre> # convert argo data to pandas dataframe argo_floats = data_floats.to_dataframe() In\u00a0[39]: Copied! <pre># use dataframe with cartopy to create map\nplt.figure(figsize = (10, 10))\nax = plt.axes(projection = ccrs.PlateCarree())\nextent = [-80, -55, 12, 25]\nax.set_extent(extent)\nax.coastlines()\nax.gridlines()\n\nplt.scatter(x = argo_floats.LONGITUDE,\n            y = argo_floats.LATITUDE,\n            c = argo_floats.PRES,\n            cmap = 'Blues',\n            marker = '.',\n            transform = ccrs.PlateCarree())\nplt.colorbar(label = 'Pressure (dbar)', shrink = 0.3)\n</pre> # use dataframe with cartopy to create map plt.figure(figsize = (10, 10)) ax = plt.axes(projection = ccrs.PlateCarree()) extent = [-80, -55, 12, 25] ax.set_extent(extent) ax.coastlines() ax.gridlines()  plt.scatter(x = argo_floats.LONGITUDE,             y = argo_floats.LATITUDE,             c = argo_floats.PRES,             cmap = 'Blues',             marker = '.',             transform = ccrs.PlateCarree()) plt.colorbar(label = 'Pressure (dbar)', shrink = 0.3) Out[39]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7ff00d08f2e0&gt;</pre>"},{"location":"HeinleArgoPy/#argopy","title":"ArgoPy\u00b6","text":""},{"location":"HeinleArgoPy/#matt-heinle","title":"Matt Heinle\u00b6","text":""},{"location":"HeinleArgoPy/#install-argopy","title":"Install ArgoPy\u00b6","text":""},{"location":"HeinleArgoPy/#data-acquisition","title":"Data acquisition\u00b6","text":""},{"location":"HeinleArgoPy/#data-visualization","title":"Data visualization\u00b6","text":""},{"location":"HeinleArgoPy/#compatability-with-other-packages","title":"Compatability with other packages\u00b6","text":""},{"location":"Lecture_10_Cartopy/","title":"Lecture 10: Maps in Scientific Python","text":"In\u00a0[1]: Copied! <pre>import cartopy.crs as ccrs\nimport cartopy\n</pre> import cartopy.crs as ccrs import cartopy <p>Cartopy's projection list tells us that the Plate Carree projection is available with the <code>ccrs.PlateCarree</code> class:</p> <p>https://scitools.org.uk/cartopy/docs/v0.15/crs/projections.html</p> <p>Note: we need to instantiate the class in order to do anything projection-y with it!</p> In\u00a0[2]: Copied! <pre>import cartopy.crs as ccrs\nimport cartopy\nccrs.PlateCarree()\n</pre> import cartopy.crs as ccrs import cartopy ccrs.PlateCarree() Out[2]: 2023-11-12T15:00:03.813952 image/svg+xml Matplotlib v3.7.2, https://matplotlib.org/ <pre>&lt;cartopy.crs.PlateCarree object at 0x10da98910&gt;</pre> In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.axes(projection=ccrs.PlateCarree())\n</pre> import matplotlib.pyplot as plt  plt.axes(projection=ccrs.PlateCarree()) Out[3]: <pre>&lt;GeoAxes: &gt;</pre> <p>That was a little underwhelming, but we can see that the Axes created is indeed one of those GeoAxes[Subplot] instances.</p> <p>One of the most useful methods that this class adds on top of the standard matplotlib Axes class is the <code>coastlines</code> method. With no arguments, it will add the Natural Earth <code>1:110,000,000</code> scale coastline data to the map.</p> In\u00a0[4]: Copied! <pre>plt.figure()\nax = plt.axes(projection=ccrs.PlateCarree())\nax.coastlines()\n</pre> plt.figure() ax = plt.axes(projection=ccrs.PlateCarree()) ax.coastlines() Out[4]: <pre>&lt;cartopy.mpl.feature_artist.FeatureArtist at 0x10f10d640&gt;</pre> <p>We could just as equally created a matplotlib subplot with one of the many approaches that exist. For example, the <code>plt.subplots</code> function could be used:</p> In\u00a0[5]: Copied! <pre>fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()})\nax.coastlines()\n</pre> fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()}) ax.coastlines() Out[5]: <pre>&lt;cartopy.mpl.feature_artist.FeatureArtist at 0x10f11de20&gt;</pre> <p>Projection classes have options we can use to customize the map</p> In\u00a0[6]: Copied! <pre>ccrs.PlateCarree?\n</pre> ccrs.PlateCarree? <pre>Init signature: ccrs.PlateCarree(central_longitude=0.0, globe=None)\nDocstring:     \nThe abstract class which denotes cylindrical projections where we\nwant to allow x values to wrap around.\nInit docstring:\nParameters\n----------\nproj4_params: iterable of key-value pairs\n    The proj4 parameters required to define the\n    desired CRS.  The parameters should not describe\n    the desired elliptic model, instead create an\n    appropriate Globe instance. The ``proj4_params``\n    parameters will override any parameters that the\n    Globe defines.\nglobe: :class:`~cartopy.crs.Globe` instance, optional\n    If omitted, the default Globe instance will be created.\n    See :class:`~cartopy.crs.Globe` for details.\nFile:           ~/opt/anaconda3/envs/research_computing_scipy/lib/python3.9/site-packages/cartopy/crs.py\nType:           ABCMeta\nSubclasses:     </pre> In\u00a0[7]: Copied! <pre>ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=180))\nax.coastlines()\n</pre> ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=180)) ax.coastlines() Out[7]: <pre>&lt;cartopy.mpl.feature_artist.FeatureArtist at 0x10f09e160&gt;</pre> In\u00a0[9]: Copied! <pre>projections = [ccrs.PlateCarree(),\n               ccrs.Mercator(),\n               ccrs.AzimuthalEquidistant()\n              ]\n\nfor proj in projections:\n    plt.figure()\n    ax = plt.axes(projection=proj)\n    ax.stock_img()\n    ax.coastlines()\n    ax.set_title(f'{type(proj)}')\n</pre> projections = [ccrs.PlateCarree(),                ccrs.Mercator(),                ccrs.AzimuthalEquidistant()               ]  for proj in projections:     plt.figure()     ax = plt.axes(projection=proj)     ax.stock_img()     ax.coastlines()     ax.set_title(f'{type(proj)}') In\u00a0[10]: Copied! <pre>ax.set_extent?\n</pre> ax.set_extent? <pre>Signature: ax.set_extent(extents, crs=None)\nDocstring:\nSet the extent (x0, x1, y0, y1) of the map in the given\ncoordinate system.\n\nIf no crs is given, the extents' coordinate system will be assumed\nto be the Geodetic version of this axes' projection.\n\nParameters\n----------\nextents\n    Tuple of floats representing the required extent (x0, x1, y0, y1).\nFile:      ~/opt/anaconda3/envs/research_computing_scipy/lib/python3.9/site-packages/cartopy/mpl/geoaxes.py\nType:      method</pre> In\u00a0[11]: Copied! <pre>central_lon, central_lat = -10, 45\nextent = [-40, 20, 30, 60]\nax = plt.axes(projection=ccrs.Orthographic(central_lon, central_lat))\nax.set_extent(extent)\nax.gridlines()\nax.coastlines(resolution='50m')\n</pre> central_lon, central_lat = -10, 45 extent = [-40, 20, 30, 60] ax = plt.axes(projection=ccrs.Orthographic(central_lon, central_lat)) ax.set_extent(extent) ax.gridlines() ax.coastlines(resolution='50m')  Out[11]: <pre>&lt;cartopy.mpl.feature_artist.FeatureArtist at 0x17e1ce6a0&gt;</pre> In\u00a0[12]: Copied! <pre>import cartopy.feature as cfeature\nimport numpy as np\n\ncentral_lat = 37.5\ncentral_lon = -96\nextent = [-120, -70, 24, 50.5]\ncentral_lon = np.mean(extent[:2])\ncentral_lat = np.mean(extent[2:])\n\nplt.figure(figsize=(12, 6))\nax = plt.axes(projection=ccrs.AlbersEqualArea(central_lon, central_lat))\nax.set_extent(extent)\n\nax.add_feature(cartopy.feature.OCEAN)\nax.add_feature(cartopy.feature.LAND, edgecolor='black')\nax.add_feature(cartopy.feature.LAKES, edgecolor='black')\nax.add_feature(cartopy.feature.RIVERS)\nax.gridlines()\n</pre> import cartopy.feature as cfeature import numpy as np  central_lat = 37.5 central_lon = -96 extent = [-120, -70, 24, 50.5] central_lon = np.mean(extent[:2]) central_lat = np.mean(extent[2:])  plt.figure(figsize=(12, 6)) ax = plt.axes(projection=ccrs.AlbersEqualArea(central_lon, central_lat)) ax.set_extent(extent)  ax.add_feature(cartopy.feature.OCEAN) ax.add_feature(cartopy.feature.LAND, edgecolor='black') ax.add_feature(cartopy.feature.LAKES, edgecolor='black') ax.add_feature(cartopy.feature.RIVERS) ax.gridlines() Out[12]: <pre>&lt;cartopy.mpl.gridliner.Gridliner at 0x17e6e76d0&gt;</pre> In\u00a0[14]: Copied! <pre># create some test data\nnew_york = dict(lon=-74.0060, lat=40.7128)\nhonolulu = dict(lon=-157.8583, lat=21.3069)\nlons = [new_york['lon'], honolulu['lon']]\nlats = [new_york['lat'], honolulu['lat']]\n</pre> # create some test data new_york = dict(lon=-74.0060, lat=40.7128) honolulu = dict(lon=-157.8583, lat=21.3069) lons = [new_york['lon'], honolulu['lon']] lats = [new_york['lat'], honolulu['lat']] <p>Key point: the data also have to be transformed to the projection space. This is done via the <code>transform=</code> keyword in the plotting method. The argument is another <code>cartopy.crs</code> object. If you don't specify a transform, Cartopy assume that the data are using the same projection as the underlying GeoAxis.</p> <p>From the Cartopy Documentation</p> <p>The core concept is that the projection of your axes is independent of the coordinate system your data is defined in. The <code>projection</code> argument is used when creating plots and determines the projection of the resulting plot (i.e. what the plot looks like). The <code>transform</code> argument to plotting functions tells Cartopy what coordinate system your data are defined in.</p> In\u00a0[15]: Copied! <pre>ax = plt.axes(projection=ccrs.PlateCarree())\nax.plot(lons, lats, label='Equirectangular straight line')\nax.plot(lons, lats, label='Great Circle', transform=ccrs.Geodetic())\nax.coastlines()\nax.legend()\nax.set_global()\n</pre> ax = plt.axes(projection=ccrs.PlateCarree()) ax.plot(lons, lats, label='Equirectangular straight line') ax.plot(lons, lats, label='Great Circle', transform=ccrs.Geodetic()) ax.coastlines() ax.legend() ax.set_global() In\u00a0[16]: Copied! <pre>import numpy as np\nlon = np.linspace(-80, 80, 25)\nlat = np.linspace(30, 70, 25)\nlon2d, lat2d = np.meshgrid(lon, lat)\ndata = np.cos(np.deg2rad(lat2d) * 4) + np.sin(np.deg2rad(lon2d) * 4)\nplt.contourf(lon2d, lat2d, data)\n</pre> import numpy as np lon = np.linspace(-80, 80, 25) lat = np.linspace(30, 70, 25) lon2d, lat2d = np.meshgrid(lon, lat) data = np.cos(np.deg2rad(lat2d) * 4) + np.sin(np.deg2rad(lon2d) * 4) plt.contourf(lon2d, lat2d, data) Out[16]: <pre>&lt;matplotlib.contour.QuadContourSet at 0x17e7bb460&gt;</pre> <p>Now we create a <code>PlateCarree</code> projection and plot the data on it without any <code>transform</code> keyword. This happens to work because <code>PlateCarree</code> is the simplest projection of lat / lon data.</p> In\u00a0[17]: Copied! <pre>ax = plt.axes(projection=ccrs.PlateCarree())\nax.set_global()\nax.coastlines()\nax.contourf(lon, lat, data)\n</pre> ax = plt.axes(projection=ccrs.PlateCarree()) ax.set_global() ax.coastlines() ax.contourf(lon, lat, data) Out[17]: <pre>&lt;cartopy.mpl.contour.GeoContourSet at 0x17e54e880&gt;</pre> <p>However, if we try the same thing with a different projection, we get the wrong result.</p> In\u00a0[18]: Copied! <pre>projection = ccrs.RotatedPole(pole_longitude=-177.5, pole_latitude=37.5)\nax = plt.axes(projection=projection)\nax.set_global()\nax.coastlines()\nax.contourf(lon, lat, data)\n</pre> projection = ccrs.RotatedPole(pole_longitude=-177.5, pole_latitude=37.5) ax = plt.axes(projection=projection) ax.set_global() ax.coastlines() ax.contourf(lon, lat, data) Out[18]: <pre>&lt;cartopy.mpl.contour.GeoContourSet at 0x17e3d8f10&gt;</pre> <p>To fix this, we need to pass the correct transform argument to <code>contourf</code>:</p> In\u00a0[19]: Copied! <pre>projection = ccrs.RotatedPole(pole_longitude=-177.5, pole_latitude=37.5)\nax = plt.axes(projection=projection)\nax.set_global()\nax.coastlines()\nax.contourf(lon, lat, data, transform=ccrs.PlateCarree())\n</pre> projection = ccrs.RotatedPole(pole_longitude=-177.5, pole_latitude=37.5) ax = plt.axes(projection=projection) ax.set_global() ax.coastlines() ax.contourf(lon, lat, data, transform=ccrs.PlateCarree()) Out[19]: <pre>&lt;cartopy.mpl.contour.GeoContourSet at 0x17e612a00&gt;</pre> In\u00a0[20]: Copied! <pre>import xarray as xr\nds = xr.open_dataset('/Users/xiaomengjin/Downloads/CERES_EBAF-TOA_Edition4.0_200003-201701.condensed.nc')\n</pre> import xarray as xr ds = xr.open_dataset('/Users/xiaomengjin/Downloads/CERES_EBAF-TOA_Edition4.0_200003-201701.condensed.nc') In\u00a0[21]: Copied! <pre>ds\n</pre> ds Out[21]: <pre>&lt;xarray.Dataset&gt;\nDimensions:                      (lon: 360, time: 203, lat: 180)\nCoordinates:\n  * lon                          (lon) float32 0.5 1.5 2.5 ... 357.5 358.5 359.5\n  * time                         (time) datetime64[ns] 2000-03-15 ... 2017-01-15\n  * lat                          (lat) float32 -89.5 -88.5 -87.5 ... 88.5 89.5\nData variables: (12/14)\n    toa_sw_all_mon               (time, lat, lon) float32 ...\n    toa_lw_all_mon               (time, lat, lon) float32 ...\n    toa_net_all_mon              (time, lat, lon) float32 ...\n    toa_sw_clr_mon               (time, lat, lon) float32 ...\n    toa_lw_clr_mon               (time, lat, lon) float32 ...\n    toa_net_clr_mon              (time, lat, lon) float32 ...\n    ...                           ...\n    toa_cre_net_mon              (time, lat, lon) float32 ...\n    solar_mon                    (time, lat, lon) float32 ...\n    cldarea_total_daynight_mon   (time, lat, lon) float32 ...\n    cldpress_total_daynight_mon  (time, lat, lon) float32 ...\n    cldtemp_total_daynight_mon   (time, lat, lon) float32 ...\n    cldtau_total_day_mon         (time, lat, lon) float32 ...\nAttributes:\n    title:             CERES EBAF (Energy Balanced and Filled) TOA Fluxes. Mo...\n    institution:       NASA/LaRC (Langley Research Center) Hampton, Va\n    Conventions:       CF-1.4\n    comment:           Data is from East to West and South to North.\n    Version:           Edition 4.0; Release Date March 7, 2017\n    Fill_Value:        Fill Value is -999.0\n    DOI:               10.5067/TERRA+AQUA/CERES/EBAF-TOA_L3B.004.0\n    Production_Files:  List of files used in creating the present Master netC...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lon: 360</li><li>time: 203</li><li>lat: 180</li></ul></li><li>Coordinates: (3)<ul><li>lon(lon)float320.5 1.5 2.5 ... 357.5 358.5 359.5long_name :longitudestandard_name :longitudeunits :degrees_eastvalid_range :[  0. 360.]<pre>array([  0.5,   1.5,   2.5, ..., 357.5, 358.5, 359.5], dtype=float32)</pre></li><li>time(time)datetime64[ns]2000-03-15 ... 2017-01-15long_name :timedelta_t :0000-00-01 00:00:00<pre>array(['2000-03-15T00:00:00.000000000', '2000-04-15T00:00:00.000000000',\n       '2000-05-15T00:00:00.000000000', ..., '2016-11-15T00:00:00.000000000',\n       '2016-12-15T00:00:00.000000000', '2017-01-15T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>lat(lat)float32-89.5 -88.5 -87.5 ... 88.5 89.5long_name :latitudestandard_name :latitudeunits :degrees_northvalid_range :[-90.  90.]<pre>array([-89.5, -88.5, -87.5, -86.5, -85.5, -84.5, -83.5, -82.5, -81.5, -80.5,\n       -79.5, -78.5, -77.5, -76.5, -75.5, -74.5, -73.5, -72.5, -71.5, -70.5,\n       -69.5, -68.5, -67.5, -66.5, -65.5, -64.5, -63.5, -62.5, -61.5, -60.5,\n       -59.5, -58.5, -57.5, -56.5, -55.5, -54.5, -53.5, -52.5, -51.5, -50.5,\n       -49.5, -48.5, -47.5, -46.5, -45.5, -44.5, -43.5, -42.5, -41.5, -40.5,\n       -39.5, -38.5, -37.5, -36.5, -35.5, -34.5, -33.5, -32.5, -31.5, -30.5,\n       -29.5, -28.5, -27.5, -26.5, -25.5, -24.5, -23.5, -22.5, -21.5, -20.5,\n       -19.5, -18.5, -17.5, -16.5, -15.5, -14.5, -13.5, -12.5, -11.5, -10.5,\n        -9.5,  -8.5,  -7.5,  -6.5,  -5.5,  -4.5,  -3.5,  -2.5,  -1.5,  -0.5,\n         0.5,   1.5,   2.5,   3.5,   4.5,   5.5,   6.5,   7.5,   8.5,   9.5,\n        10.5,  11.5,  12.5,  13.5,  14.5,  15.5,  16.5,  17.5,  18.5,  19.5,\n        20.5,  21.5,  22.5,  23.5,  24.5,  25.5,  26.5,  27.5,  28.5,  29.5,\n        30.5,  31.5,  32.5,  33.5,  34.5,  35.5,  36.5,  37.5,  38.5,  39.5,\n        40.5,  41.5,  42.5,  43.5,  44.5,  45.5,  46.5,  47.5,  48.5,  49.5,\n        50.5,  51.5,  52.5,  53.5,  54.5,  55.5,  56.5,  57.5,  58.5,  59.5,\n        60.5,  61.5,  62.5,  63.5,  64.5,  65.5,  66.5,  67.5,  68.5,  69.5,\n        70.5,  71.5,  72.5,  73.5,  74.5,  75.5,  76.5,  77.5,  78.5,  79.5,\n        80.5,  81.5,  82.5,  83.5,  84.5,  85.5,  86.5,  87.5,  88.5,  89.5],\n      dtype=float32)</pre></li></ul></li><li>Data variables: (14)<ul><li>toa_sw_all_mon(time, lat, lon)float32...long_name :Top of The Atmosphere Shortwave Flux, Monthly Means, All-Sky conditionsstandard_name :TOA Shortwave Flux - All-SkyCF_name :toa_outgoing_shortwave_fluxunits :W m-2valid_min :      0.00000valid_max :      600.000<pre>[13154400 values with dtype=float32]</pre></li><li>toa_lw_all_mon(time, lat, lon)float32...long_name :Top of The Atmosphere Longwave Flux, Monthly Means, All-Sky conditionsstandard_name :TOA Longwave Flux - All-SkyCF_name :toa_outgoing_longwave_fluxunits :W m-2valid_min :      0.00000valid_max :      400.000<pre>[13154400 values with dtype=float32]</pre></li><li>toa_net_all_mon(time, lat, lon)float32...long_name :Top of The Atmosphere Net Flux, Monthly Means, All-Sky conditionsstandard_name :TOA Net Flux - All-SkyCF_name :toa_net_downward_fluxunits :W m-2valid_min :     -400.000valid_max :      400.000<pre>[13154400 values with dtype=float32]</pre></li><li>toa_sw_clr_mon(time, lat, lon)float32...long_name :Top of The Atmosphere Shortwave Flux, Monthly Means, Clear-Sky conditionsstandard_name :TOA Shortwave Flux - Clear-SkyCF_name :toa_outgoing_shortwave_flux_assuming_clear_skyunits :W m-2valid_min :      0.00000valid_max :      600.000<pre>[13154400 values with dtype=float32]</pre></li><li>toa_lw_clr_mon(time, lat, lon)float32...long_name :Top of The Atmosphere Longwave Flux, Monthly Means, Clear-Sky conditionsstandard_name :TOA Longwave Flux - Clear-SkyCF_name :toa_outgoing_longwave_flux_assuming_clear_skyunits :W m-2valid_min :      0.00000valid_max :      400.000<pre>[13154400 values with dtype=float32]</pre></li><li>toa_net_clr_mon(time, lat, lon)float32...long_name :Top of The Atmosphere Net Flux, Monthly Means, Clear-Sky conditionsstandard_name :TOA Net Flux - Clear-SkyCF_name :noneunits :W m-2valid_min :     -400.000valid_max :      400.000<pre>[13154400 values with dtype=float32]</pre></li><li>toa_cre_sw_mon(time, lat, lon)float32...long_name :Top of The Atmosphere Cloud Radiative Effects Shortwave Flux, Monthly Meansstandard_name :TOA CRE Shortwave FluxCF_name :toa_shortwave_cloud_radiative_effectunits :W m-2valid_min :     -400.000valid_max :      100.000<pre>[13154400 values with dtype=float32]</pre></li><li>toa_cre_lw_mon(time, lat, lon)float32...long_name :Top of The Atmosphere Cloud Radiative Effects Longwave Flux, Monthly Meansstandard_name :TOA CRE Longwave FluxCF_name :toa_longwave_cloud_radiative_effectunits :W m-2valid_min :     -100.000valid_max :      300.000<pre>[13154400 values with dtype=float32]</pre></li><li>toa_cre_net_mon(time, lat, lon)float32...long_name :Top of The Atmosphere Cloud Radiative Effects Net Flux, Monthly Meansstandard_name :TOA CRE Net FluxCF_name :toa_net_cloud_radiative_effectunits :W m-2valid_min :     -400.000valid_max :      400.000<pre>[13154400 values with dtype=float32]</pre></li><li>solar_mon(time, lat, lon)float32...long_name :Incoming Solar Flux, Monthly Meansstandard_name :Incoming Solar FluxCF_name :toa_incoming_shortwave_fluxunits :W m-2valid_min :      0.00000valid_max :      800.000<pre>[13154400 values with dtype=float32]</pre></li><li>cldarea_total_daynight_mon(time, lat, lon)float32...long_name :Cloud Area Fraction, Monthly Means, Daytime-and-Nighttime conditionsstandard_name :Cloud Area Fraction - Daytime-and-NighttimeCF_name :cloud_area_fractionunits :percentvalid_min :      0.00000valid_max :      100.000<pre>[13154400 values with dtype=float32]</pre></li><li>cldpress_total_daynight_mon(time, lat, lon)float32...long_name :Cloud Effective Pressure, Monthly Means, Daytime-and-Nighttime conditionsstandard_name :Cloud Effective Pressure - Daytime-and-NighttimeCF_name :noneunits :hPavalid_min :      0.00000valid_max :      1050.00<pre>[13154400 values with dtype=float32]</pre></li><li>cldtemp_total_daynight_mon(time, lat, lon)float32...long_name :Cloud Effective Temperature, Monthly Means, Daytime-and-Nighttime conditionsstandard_name :Cloud Effective Temperature - Daytime-and-NighttimeCF_name :noneunits :Kvalid_min :      150.000valid_max :      350.000<pre>[13154400 values with dtype=float32]</pre></li><li>cldtau_total_day_mon(time, lat, lon)float32...long_name :Cloud Visible Optical Depth, Monthly Means, Daytime conditionsstandard_name :Cloud Visible Optical Depth - DaytimeCF_name :noneunits :dimensionlessvalid_min :      0.00000valid_max :      250.000<pre>[13154400 values with dtype=float32]</pre></li></ul></li><li>Indexes: (3)<ul><li>lonPandasIndex<pre>PandasIndex(Index([  0.5,   1.5,   2.5,   3.5,   4.5,   5.5,   6.5,   7.5,   8.5,   9.5,\n       ...\n       350.5, 351.5, 352.5, 353.5, 354.5, 355.5, 356.5, 357.5, 358.5, 359.5],\n      dtype='float32', name='lon', length=360))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2000-03-15', '2000-04-15', '2000-05-15', '2000-06-15',\n               '2000-07-15', '2000-08-15', '2000-09-15', '2000-10-15',\n               '2000-11-15', '2000-12-15',\n               ...\n               '2016-04-15', '2016-05-15', '2016-06-15', '2016-07-15',\n               '2016-08-15', '2016-09-15', '2016-10-15', '2016-11-15',\n               '2016-12-15', '2017-01-15'],\n              dtype='datetime64[ns]', name='time', length=203, freq=None))</pre></li><li>latPandasIndex<pre>PandasIndex(Index([-89.5, -88.5, -87.5, -86.5, -85.5, -84.5, -83.5, -82.5, -81.5, -80.5,\n       ...\n        80.5,  81.5,  82.5,  83.5,  84.5,  85.5,  86.5,  87.5,  88.5,  89.5],\n      dtype='float32', name='lat', length=180))</pre></li></ul></li><li>Attributes: (8)title :CERES EBAF (Energy Balanced and Filled) TOA Fluxes. Monthly Averages and 07/2005 to 06/2015 Climatology.institution :NASA/LaRC (Langley Research Center) Hampton, VaConventions :CF-1.4comment :Data is from East to West and South to North.Version :Edition 4.0; Release Date March 7, 2017Fill_Value :Fill Value is -999.0DOI :10.5067/TERRA+AQUA/CERES/EBAF-TOA_L3B.004.0Production_Files :List of files used in creating the present Master netCDF file: /homedir/nloeb/ebaf/monthly_means/adj_fluxes/deliverable/sw*.gz /homedir/nloeb/ebaf/monthly_means/adj_fluxes/deliverable/lw*.gz /homedir/nloeb/ebaf/monthly_means/adj_fluxes/deliverable/net*.gz /homedir/nloeb/ebaf/monthly_means/adj_fluxes/deliverable/solflx*.gz /homedir/nloeb/ebaf/monthly_means/out_glob.dat</li></ul> In\u00a0[22]: Copied! <pre>toa_sw_mean = ds['toa_sw_all_mon'].mean(dim = 'time')\nfig = plt.figure(figsize=(9,6))\nax = plt.axes(projection=ccrs.Robinson())\nax.coastlines()\nax.gridlines()\ntoa_sw_mean.plot(ax=ax, transform=ccrs.PlateCarree(),\n        cmap = 'Reds', cbar_kwargs={'shrink': 0.4})\n</pre> toa_sw_mean = ds['toa_sw_all_mon'].mean(dim = 'time') fig = plt.figure(figsize=(9,6)) ax = plt.axes(projection=ccrs.Robinson()) ax.coastlines() ax.gridlines() toa_sw_mean.plot(ax=ax, transform=ccrs.PlateCarree(),         cmap = 'Reds', cbar_kwargs={'shrink': 0.4}) Out[22]: <pre>&lt;cartopy.mpl.geocollection.GeoQuadMesh at 0x18525da00&gt;</pre> In\u00a0[24]: Copied! <pre>central_lat = 37.5\ncentral_lon = -96\nextent = [-120, -70, 24, 50.5]\ncentral_lon = np.mean(extent[:2])\ncentral_lat = np.mean(extent[2:])\n\nplt.figure(figsize=(12, 6))\nax = plt.axes(projection=ccrs.AlbersEqualArea(central_lon, central_lat))\nax.set_extent(extent)\ntoa_sw_mean.plot(ax=ax, transform=ccrs.PlateCarree(),\n        cmap = 'jet', vmin = 60, vmax = 125, cbar_kwargs={'shrink': 0.4})\nax.add_feature(cartopy.feature.STATES, edgecolor='black')\nax.coastlines()\n\nax.gridlines()\n</pre>  central_lat = 37.5 central_lon = -96 extent = [-120, -70, 24, 50.5] central_lon = np.mean(extent[:2]) central_lat = np.mean(extent[2:])  plt.figure(figsize=(12, 6)) ax = plt.axes(projection=ccrs.AlbersEqualArea(central_lon, central_lat)) ax.set_extent(extent) toa_sw_mean.plot(ax=ax, transform=ccrs.PlateCarree(),         cmap = 'jet', vmin = 60, vmax = 125, cbar_kwargs={'shrink': 0.4}) ax.add_feature(cartopy.feature.STATES, edgecolor='black') ax.coastlines()  ax.gridlines() Out[24]: <pre>&lt;cartopy.mpl.gridliner.Gridliner at 0x185c54640&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Lecture_10_Cartopy/#lecture-10-maps-in-scientific-python","title":"Lecture 10: Maps in Scientific Python\u00b6","text":"<p>Making maps is a fundamental part of geoscience research. Maps differ from regular figures in the following principle ways:</p> <ul> <li>Maps require a projection of geographic coordinates on the 3D Earth to the 2D space of your figure.</li> <li>Maps often include extra decorations besides just our data (e.g. continents, country borders, etc.)</li> </ul> <p>Mapping is a notoriously hard and complicated problem, mostly due to the complexities of projection.</p> <p>In this lecture, we will learn about Cartopy, one of the most common packages for making maps within python.</p>"},{"location":"Lecture_10_Cartopy/#introducing-cartopy","title":"Introducing Cartopy\u00b6","text":"<p>Cartopy makes use of the powerful PROJ.4, numpy and shapely libraries and includes a programatic interface built on top of Matplotlib for the creation of publication quality maps.</p> <p>Key features of cartopy are its object oriented projection definitions, and its ability to transform points, lines, vectors, polygons and images between those projections.</p>"},{"location":"Lecture_10_Cartopy/#cartopy-projections-and-other-reference-systems","title":"Cartopy Projections and other reference systems\u00b6","text":"<p>In Cartopy, each projection is a class. Most classes of projection can be configured in projection-specific ways, although Cartopy takes an opinionated stance on sensible defaults.</p> <p>Let's create a Plate Carree projection instance.</p> <p>To do so, we need cartopy's crs module. This is typically imported as <code>ccrs</code> (Cartopy Coordinate Reference Systems).</p>"},{"location":"Lecture_10_Cartopy/#drawing-a-map","title":"Drawing a map\u00b6","text":"<p>Cartopy optionally depends upon matplotlib, and each projection knows how to create a matplotlib Axes (or AxesSubplot) that can represent itself.</p> <p>The Axes that the projection creates is a cartopy.mpl.geoaxes.GeoAxes. This Axes subclass overrides some of matplotlib's existing methods, and adds a number of extremely useful ones for drawing maps.</p> <p>We'll go back and look at those methods shortly, but first, let's actually see the cartopy+matplotlib dance in action:</p>"},{"location":"Lecture_10_Cartopy/#useful-methods-of-a-geoaxes","title":"Useful methods of a GeoAxes\u00b6","text":"<p>The cartopy.mpl.geoaxes.GeoAxes class adds a number of useful methods.</p> <p>Let's take a look at:</p> <ul> <li><p>set_global - zoom the map out as much as possible</p> </li> <li><p>set_extent - zoom the map to the given bounding box</p> </li> <li><p>gridlines - add a graticule (and optionally labels) to the axes</p> </li> <li><p>coastlines - add Natural Earth coastlines to the axes</p> </li> <li><p>stock_img - add a low-resolution Natural Earth background image to the axes</p> </li> <li><p>imshow - add an image (numpy array) to the axes</p> </li> <li><p>add_geometries - add a collection of geometries (Shapely) to the axes</p> </li> </ul>"},{"location":"Lecture_10_Cartopy/#some-more-examples-of-different-global-projections","title":"Some More Examples of Different Global Projections\u00b6","text":""},{"location":"Lecture_10_Cartopy/#regional-maps","title":"Regional Maps\u00b6","text":"<p>To create a regional map, we use the <code>set_extent</code> method of GeoAxis to limit the size of the region.</p>"},{"location":"Lecture_10_Cartopy/#adding-features-to-the-map","title":"Adding Features to the Map\u00b6","text":"<p>To give our map more styles and details, we add <code>cartopy.feature</code> objects. Many useful features are built in. These \"default features\" are at coarse (110m) resolution.</p> Name Description <code>cartopy.feature.BORDERS</code> Country boundaries <code>cartopy.feature.COASTLINE</code> Coastline, including major islands <code>cartopy.feature.LAKES</code> Natural and artificial lakes <code>cartopy.feature.LAND</code> Land polygons, including major islands <code>cartopy.feature.OCEAN</code> Ocean polygons <code>cartopy.feature.RIVERS</code> Single-line drainages, including lake centerlines <code>cartopy.feature.STATES</code> (limited to the United States at this scale) <p>Below we illustrate these features in a customized map of North America.</p>"},{"location":"Lecture_10_Cartopy/#adding-data-to-the-map","title":"Adding Data to the Map\u00b6","text":"<p>Now that we know how to create a map, let's add our data to it! That's the whole point.</p> <p>Because our map is a matplotlib axis, we can use all the familiar maptplotlib commands to make plots. By default, the map extent will be adjusted to match the data. We can override this with the <code>.set_global</code> or <code>.set_extent</code> commands.</p>"},{"location":"Lecture_10_Cartopy/#plotting-2d-raster-data","title":"Plotting 2D (Raster) Data\u00b6","text":"<p>The same principles apply to 2D data. Below we create some example data defined in regular lat / lon coordinates.</p>"},{"location":"Lecture_10_Cartopy/#xarray-integration","title":"Xarray Integration\u00b6","text":"<p>Cartopy transforms can be passed to xarray! This creates a very quick path for creating professional looking maps from netCDF data.</p>"},{"location":"Lecture_11_Geopandas/","title":"Lecture 11: Environmental Sciences Packages","text":"In\u00a0[1]: Copied! <pre>import geopandas as gpd\n\ngdf = gpd.read_file('/Users/xiaomengjin/Dropbox/0_Rutgers/3_Teaching/Research_Computing/NCA5_Atlas_Global_Warming_Level_1/NCA_Atlas_Counties.shp')\n\ngdf\n</pre> import geopandas as gpd  gdf = gpd.read_file('/Users/xiaomengjin/Dropbox/0_Rutgers/3_Teaching/Research_Computing/NCA5_Atlas_Global_Warming_Level_1/NCA_Atlas_Counties.shp')  gdf Out[1]: OBJECTID NAME STATE_NAME STATE_ABBR FIPS pr_above_n prmax1day_ prmax5yr_G tavg_GWL1 tmax1day_G ... tmean_jja_ tmin_days_ tmin_day_1 tmin_day_2 tmin_jja_G pr_annual_ pr_days_ab SHAPE_Leng SHAPE_Area geometry 0 1 Autauga County Alabama AL 01001 8.866368 1.204632 0.560895 1.732316 1.543474 ... 1.604421 22.678737 -0.039947 -8.215921 1.774947 2.231684 11.870172 2.062534 0.150258 POLYGON ((-86.41312 32.70739, -86.41219 32.526... 1 2 Baldwin County Alabama AL 01003 10.447559 3.454147 1.104216 1.633755 1.526716 ... 1.508863 19.807863 -0.008627 -5.261471 1.667588 3.200307 11.634376 9.150287 0.398401 MULTIPOLYGON (((-87.96018 30.66235, -87.96046 ... 2 3 Barbour County Alabama AL 01005 10.930474 0.800702 0.499965 1.649667 1.629789 ... 1.571404 23.813614 -0.017614 -6.758772 1.625246 2.796249 16.953608 2.681671 0.223264 POLYGON ((-85.25784 32.14794, -85.25924 32.145... 3 4 Bibb County Alabama AL 01007 10.956795 2.631909 1.330318 1.789705 1.623523 ... 1.684205 22.352250 -0.083273 -9.176136 1.856455 1.619284 14.511721 1.887436 0.156487 POLYGON ((-87.06574 33.24691, -87.02685 33.246... 4 5 Blount County Alabama AL 01009 18.316114 6.689886 6.570682 1.757045 1.530205 ... 1.639023 18.284318 -0.132205 -9.861114 1.868750 3.069511 23.057712 2.413198 0.164411 POLYGON ((-86.45302 34.25932, -86.44414 34.259... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3106 3107 Washakie County Wyoming WY 56043 17.864115 6.655069 8.193466 2.103356 2.002747 ... 2.147466 0.158741 -5.406983 -11.806500 2.097448 5.085914 16.041836 4.247988 0.650772 POLYGON ((-108.54727 44.16848, -108.51002 44.1... 3107 3108 Weston County Wyoming WY 56045 20.280563 5.863631 8.519034 2.067239 2.078767 ... 2.164869 0.597778 -4.788625 -10.939795 2.158937 4.700654 18.228163 3.408275 0.695663 POLYGON ((-104.05450 44.18039, -104.05470 43.9... 3108 3109 NaN Puerto Rico PR 72 4.800000 0.700000 5.800000 0.950000 NaN ... NaN 28.700000 NaN NaN NaN NaN NaN 10.736738 0.764291 MULTIPOLYGON (((-66.53130 17.88292, -66.53401 ... 3109 3110 NaN Alaska AK 02 22.300000 6.500000 5.900000 2.100000 NaN ... NaN NaN -7.449000 -12.600000 NaN NaN NaN 557.646894 281.718582 MULTIPOLYGON (((-179.10933 51.30098, -179.1060... 3110 3111 NaN Hawaii HI 15 9.200000 5.300000 8.100000 1.070000 NaN ... NaN 29.600000 NaN -1.100000 NaN NaN NaN 18.946659 1.440200 MULTIPOLYGON (((-155.06851 19.72998, -155.0680... <p>3111 rows \u00d7 23 columns</p> In\u00a0[2]: Copied! <pre>### Pandas indexing and selection methods also work with geopandas\n\ngdf_nj = gdf.loc[gdf.STATE_NAME == 'New Jersey']\n</pre> ### Pandas indexing and selection methods also work with geopandas  gdf_nj = gdf.loc[gdf.STATE_NAME == 'New Jersey'] In\u00a0[3]: Copied! <pre>gdf_nj = gdf_nj.set_index('NAME')\n</pre> gdf_nj = gdf_nj.set_index('NAME') In\u00a0[4]: Copied! <pre>gdf_nj\n</pre> gdf_nj Out[4]: OBJECTID STATE_NAME STATE_ABBR FIPS pr_above_n prmax1day_ prmax5yr_G tavg_GWL1 tmax1day_G tmax_days_ ... tmean_jja_ tmin_days_ tmin_day_1 tmin_day_2 tmin_jja_G pr_annual_ pr_days_ab SHAPE_Leng SHAPE_Area geometry NAME Atlantic County 1741 New Jersey NJ 34001 12.709769 5.173795 9.893641 1.964590 1.760179 0.464282 ... 1.791410 9.996231 -0.214846 -14.452641 1.862923 2.726068 15.623388 12.040877 0.152625 MULTIPOLYGON (((-74.41362 39.54437, -74.41400 ... Bergen County 1742 New Jersey NJ 34003 16.508688 5.163812 8.624438 2.108313 1.919000 0.723812 ... 1.930625 9.349438 -0.502000 -14.903813 1.896625 4.355973 13.356645 2.186622 0.065902 MULTIPOLYGON (((-74.12046 40.85576, -74.12117 ... Burlington County 1743 New Jersey NJ 34005 17.037017 7.749103 12.298672 2.017741 1.757276 0.894690 ... 1.831379 8.895655 -0.335707 -14.852569 1.884414 4.160636 20.504922 3.030257 0.222051 MULTIPOLYGON (((-74.44213 39.55354, -74.44109 ... Camden County 1744 New Jersey NJ 34007 17.107375 7.184875 12.570375 2.048250 1.841625 0.987312 ... 1.908937 10.710687 -0.242750 -14.977312 1.952188 4.326228 19.452632 1.772704 0.061134 MULTIPOLYGON (((-75.08716 39.97180, -75.08736 ... Cape May County 1745 New Jersey NJ 34009 13.507833 5.575222 10.194389 1.884833 1.601944 0.269222 ... 1.729889 10.808944 -0.200167 -14.010056 1.793889 2.755018 18.012068 9.960371 0.069066 MULTIPOLYGON (((-74.58412 39.30441, -74.58470 ... Cumberland County 1746 New Jersey NJ 34011 16.687061 7.617212 11.978455 1.981485 1.765152 0.580061 ... 1.856576 11.125091 -0.220667 -14.537727 1.934333 3.991424 21.338093 5.241841 0.135073 MULTIPOLYGON (((-75.41831 39.41545, -75.41792 ... Essex County 1747 New Jersey NJ 34013 14.555750 5.560125 10.055875 2.089000 1.897375 0.731250 ... 1.935875 8.622875 -0.493250 -14.683375 1.923375 3.768749 10.525440 1.138257 0.035286 POLYGON ((-74.32281 40.90884, -74.32247 40.908... Gloucester County 1748 New Jersey NJ 34015 16.226667 8.225500 12.450750 2.078042 1.969333 1.027333 ... 2.018917 13.232708 -0.162542 -15.102375 2.071208 4.281921 20.030672 2.053469 0.089123 MULTIPOLYGON (((-75.40662 39.78106, -75.40773 ... Hudson County 1749 New Jersey NJ 34017 15.946667 8.062000 11.879333 2.177333 2.002333 0.906333 ... 2.037333 14.763666 -0.102000 -15.584333 2.029667 3.864291 13.227471 1.524890 0.012900 MULTIPOLYGON (((-74.16091 40.64526, -74.16016 ... Hunterdon County 1750 New Jersey NJ 34019 19.074233 7.458533 11.135267 2.087933 1.913300 0.621533 ... 1.938633 4.455567 -1.075267 -14.064233 1.929167 4.933982 16.882525 1.775757 0.120479 POLYGON ((-74.83809 40.75226, -74.82749 40.744... Mercer County 1751 New Jersey NJ 34021 17.423600 7.210800 9.612867 2.075733 1.844733 0.839733 ... 1.949600 8.515200 -0.354933 -15.028600 1.959933 4.484226 15.734961 1.534344 0.062769 POLYGON ((-74.72324 40.37739, -74.72206 40.375... Middlesex County 1752 New Jersey NJ 34023 16.515458 7.180292 11.649917 2.084208 1.860917 0.883833 ... 1.975458 9.029208 -0.369500 -14.953375 1.981625 4.738598 13.874158 3.026746 0.085986 MULTIPOLYGON (((-74.34076 40.48271, -74.34389 ... Monmouth County 1753 New Jersey NJ 34025 17.070813 7.863531 13.860406 2.005531 1.817750 0.617469 ... 1.890375 9.329031 -0.280687 -14.637719 1.907125 3.912146 15.619226 4.256690 0.130309 MULTIPOLYGON (((-73.98408 40.41741, -73.99008 ... Morris County 1754 New Jersey NJ 34027 17.161424 5.485061 9.291970 2.105303 1.951606 0.370364 ... 1.962485 3.653424 -1.411697 -13.954576 1.922727 4.719114 14.513126 2.380815 0.133213 POLYGON ((-74.50049 41.08601, -74.49997 41.085... Ocean County 1755 New Jersey NJ 34029 15.653977 7.068045 12.782796 1.972886 1.729659 0.614295 ... 1.797545 8.254818 -0.323955 -14.488704 1.836182 3.356637 17.491542 15.648201 0.173052 MULTIPOLYGON (((-74.09657 40.12414, -74.09687 ... Passaic County 1756 New Jersey NJ 34031 15.799063 4.963187 9.094875 2.083125 1.851500 0.403250 ... 1.885688 4.598750 -1.223063 -13.993937 1.824375 4.650235 13.346869 1.556083 0.054999 POLYGON ((-74.23429 41.14302, -74.21777 41.136... Salem County 1757 New Jersey NJ 34033 17.727826 9.147435 12.954217 2.074696 1.935739 0.937087 ... 2.003348 13.687652 -0.174261 -15.161043 2.073043 4.786771 22.100363 4.394849 0.092340 MULTIPOLYGON (((-75.48224 39.65725, -75.48268 ... Somerset County 1758 New Jersey NJ 34035 18.454240 6.473880 9.478840 2.098480 1.962760 0.856760 ... 2.016280 6.486240 -0.787800 -14.321200 2.005080 4.908482 18.333972 1.699680 0.083972 POLYGON ((-74.55469 40.75678, -74.55335 40.756... Sussex County 1759 New Jersey NJ 34037 16.832972 4.646583 7.895972 2.144833 1.996778 0.282000 ... 2.002611 2.364194 -2.251417 -13.478472 1.904444 4.451370 14.487146 1.685282 0.148757 POLYGON ((-74.67081 41.34637, -74.63760 41.331... Union County 1760 New Jersey NJ 34039 14.994167 6.106333 11.412667 2.124167 1.841167 0.860667 ... 1.944667 8.997667 -0.469667 -15.386333 1.940500 4.059828 12.067153 1.369704 0.028546 POLYGON ((-74.36903 40.73927, -74.36647 40.737... Warren County 1761 New Jersey NJ 34041 19.093179 6.100571 7.801143 2.126071 2.035357 0.390464 ... 2.010179 2.703071 -1.738714 -13.543821 1.961036 4.861140 15.735885 1.741006 0.100309 POLYGON ((-74.93889 41.06878, -74.90189 41.034... <p>21 rows \u00d7 22 columns</p> In\u00a0[5]: Copied! <pre>gdf_nj.geometry\n</pre> gdf_nj.geometry Out[5]: <pre>NAME\nAtlantic County      MULTIPOLYGON (((-74.41362 39.54437, -74.41400 ...\nBergen County        MULTIPOLYGON (((-74.12046 40.85576, -74.12117 ...\nBurlington County    MULTIPOLYGON (((-74.44213 39.55354, -74.44109 ...\nCamden County        MULTIPOLYGON (((-75.08716 39.97180, -75.08736 ...\nCape May County      MULTIPOLYGON (((-74.58412 39.30441, -74.58470 ...\nCumberland County    MULTIPOLYGON (((-75.41831 39.41545, -75.41792 ...\nEssex County         POLYGON ((-74.32281 40.90884, -74.32247 40.908...\nGloucester County    MULTIPOLYGON (((-75.40662 39.78106, -75.40773 ...\nHudson County        MULTIPOLYGON (((-74.16091 40.64526, -74.16016 ...\nHunterdon County     POLYGON ((-74.83809 40.75226, -74.82749 40.744...\nMercer County        POLYGON ((-74.72324 40.37739, -74.72206 40.375...\nMiddlesex County     MULTIPOLYGON (((-74.34076 40.48271, -74.34389 ...\nMonmouth County      MULTIPOLYGON (((-73.98408 40.41741, -73.99008 ...\nMorris County        POLYGON ((-74.50049 41.08601, -74.49997 41.085...\nOcean County         MULTIPOLYGON (((-74.09657 40.12414, -74.09687 ...\nPassaic County       POLYGON ((-74.23429 41.14302, -74.21777 41.136...\nSalem County         MULTIPOLYGON (((-75.48224 39.65725, -75.48268 ...\nSomerset County      POLYGON ((-74.55469 40.75678, -74.55335 40.756...\nSussex County        POLYGON ((-74.67081 41.34637, -74.63760 41.331...\nUnion County         POLYGON ((-74.36903 40.73927, -74.36647 40.737...\nWarren County        POLYGON ((-74.93889 41.06878, -74.90189 41.034...\nName: geometry, dtype: geometry</pre> In\u00a0[6]: Copied! <pre>gdf_nj.loc['Middlesex County'].geometry\n</pre> gdf_nj.loc['Middlesex County'].geometry Out[6]: In\u00a0[7]: Copied! <pre>gdf_nj[\"boundary\"] = gdf_nj.boundary\ngdf_nj[\"boundary\"]\n</pre> gdf_nj[\"boundary\"] = gdf_nj.boundary gdf_nj[\"boundary\"] Out[7]: <pre>NAME\nAtlantic County      MULTILINESTRING ((-74.41362 39.54437, -74.4140...\nBergen County        MULTILINESTRING ((-74.12046 40.85576, -74.1211...\nBurlington County    MULTILINESTRING ((-74.44213 39.55354, -74.4410...\nCamden County        MULTILINESTRING ((-75.08716 39.97180, -75.0873...\nCape May County      MULTILINESTRING ((-74.58412 39.30441, -74.5847...\nCumberland County    MULTILINESTRING ((-75.41831 39.41545, -75.4179...\nEssex County         LINESTRING (-74.32281 40.90884, -74.32247 40.9...\nGloucester County    MULTILINESTRING ((-75.40662 39.78106, -75.4077...\nHudson County        MULTILINESTRING ((-74.16091 40.64526, -74.1601...\nHunterdon County     LINESTRING (-74.83809 40.75226, -74.82749 40.7...\nMercer County        LINESTRING (-74.72324 40.37739, -74.72206 40.3...\nMiddlesex County     MULTILINESTRING ((-74.34076 40.48271, -74.3438...\nMonmouth County      MULTILINESTRING ((-73.98408 40.41741, -73.9900...\nMorris County        LINESTRING (-74.50049 41.08601, -74.49997 41.0...\nOcean County         MULTILINESTRING ((-74.09657 40.12414, -74.0968...\nPassaic County       LINESTRING (-74.23429 41.14302, -74.21777 41.1...\nSalem County         MULTILINESTRING ((-75.48224 39.65725, -75.4826...\nSomerset County      LINESTRING (-74.55469 40.75678, -74.55335 40.7...\nSussex County        LINESTRING (-74.67081 41.34637, -74.63760 41.3...\nUnion County         LINESTRING (-74.36903 40.73927, -74.36647 40.7...\nWarren County        LINESTRING (-74.93889 41.06878, -74.90189 41.0...\nName: boundary, dtype: geometry</pre> In\u00a0[8]: Copied! <pre>gdf_nj.loc['Middlesex County'].boundary\n</pre> gdf_nj.loc['Middlesex County'].boundary Out[8]: <p>Since we have saved boundary as a new column, we now have two geometry columns in the same <code>GeoDataFrame</code>.</p> <p>We can also create new geometries, which could be, for example, the centroid:</p> In\u00a0[9]: Copied! <pre>gdf_nj[\"centroid\"] = gdf_nj.centroid\ngdf_nj[\"centroid\"]\n</pre>  gdf_nj[\"centroid\"] = gdf_nj.centroid gdf_nj[\"centroid\"] <pre>/var/folders/7b/6t7qqfj57bb0_ml_y_5bw86r0000gn/T/ipykernel_21619/3860640333.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf_nj[\"centroid\"] = gdf_nj.centroid\n</pre> Out[9]: <pre>NAME\nAtlantic County      POINT (-74.67735 39.48279)\nBergen County        POINT (-74.07824 40.96239)\nBurlington County    POINT (-74.66766 39.87828)\nCamden County        POINT (-74.95778 39.80156)\nCape May County      POINT (-74.80712 39.15175)\nCumberland County    POINT (-75.11028 39.37396)\nEssex County         POINT (-74.24861 40.78830)\nGloucester County    POINT (-75.13712 39.71375)\nHudson County        POINT (-74.07725 40.74073)\nHunterdon County     POINT (-74.91224 40.56730)\nMercer County        POINT (-74.70174 40.28344)\nMiddlesex County     POINT (-74.41303 40.43831)\nMonmouth County      POINT (-74.22610 40.25884)\nMorris County        POINT (-74.54452 40.86201)\nOcean County         POINT (-74.29942 39.91042)\nPassaic County       POINT (-74.30083 41.03439)\nSalem County         POINT (-75.34574 39.58701)\nSomerset County      POINT (-74.61633 40.56351)\nSussex County        POINT (-74.69084 41.13931)\nUnion County         POINT (-74.31056 40.66027)\nWarren County        POINT (-74.99735 40.85715)\nName: centroid, dtype: geometry</pre> In\u00a0[10]: Copied! <pre>gdf_nj.plot()\n</pre> gdf_nj.plot() Out[10]: <pre>&lt;Axes: &gt;</pre> In\u00a0[11]: Copied! <pre>gdf_nj.plot('tavg_GWL1', legend = True)\n</pre> gdf_nj.plot('tavg_GWL1', legend = True) Out[11]: <pre>&lt;Axes: &gt;</pre> In\u00a0[12]: Copied! <pre>gdf_nj.columns\n</pre> gdf_nj.columns Out[12]: <pre>Index(['OBJECTID', 'STATE_NAME', 'STATE_ABBR', 'FIPS', 'pr_above_n',\n       'prmax1day_', 'prmax5yr_G', 'tavg_GWL1', 'tmax1day_G', 'tmax_days_',\n       'tmax_day_1', 'tmax_day_2', 'tmean_jja_', 'tmin_days_', 'tmin_day_1',\n       'tmin_day_2', 'tmin_jja_G', 'pr_annual_', 'pr_days_ab', 'SHAPE_Leng',\n       'SHAPE_Area', 'geometry', 'boundary', 'centroid'],\n      dtype='object')</pre> <p>You can also explore your data interactively using <code>GeoDataFrame.explore()</code>, which behaves in the same way <code>plot()</code> does but returns an interactive map instead.</p> <p>Switching the active geometry (<code>GeoDataFrame.set_geometry</code>) to centroids, we can plot the same data using point geometry.</p> In\u00a0[13]: Copied! <pre>gdf_nj = gdf_nj.set_geometry('boundary')\n</pre> gdf_nj = gdf_nj.set_geometry('boundary') In\u00a0[14]: Copied! <pre>gdf_nj.plot('tavg_GWL1', legend = True)\n</pre> gdf_nj.plot('tavg_GWL1', legend = True) Out[14]: <pre>&lt;Axes: &gt;</pre> <p>And we can also layer both <code>GeoSeries</code> on top of each other. We just need to use one plot as an axis for the other.</p> In\u00a0[15]: Copied! <pre>ax = gdf_nj[\"geometry\"].plot()\ngdf_nj[\"centroid\"].plot(ax=ax, color=\"black\")\n</pre> ax = gdf_nj[\"geometry\"].plot() gdf_nj[\"centroid\"].plot(ax=ax, color=\"black\") Out[15]: <pre>&lt;Axes: &gt;</pre> <p>Now we set the active geometry back to the original <code>GeoSeries</code>.</p> In\u00a0[16]: Copied! <pre>gdf_nj = gdf_nj.set_geometry(\"geometry\")\n</pre> gdf_nj = gdf_nj.set_geometry(\"geometry\") In\u00a0[17]: Copied! <pre># buffering the active geometry by 0.1 degrees\ngdf_nj[\"buffered\"] = gdf_nj.buffer(0.1)\n\ngdf_nj[\"buffered_centroid\"] = gdf_nj[\"centroid\"].buffer(0.05)\n</pre> # buffering the active geometry by 0.1 degrees gdf_nj[\"buffered\"] = gdf_nj.buffer(0.1)  gdf_nj[\"buffered_centroid\"] = gdf_nj[\"centroid\"].buffer(0.05) <pre>/var/folders/7b/6t7qqfj57bb0_ml_y_5bw86r0000gn/T/ipykernel_21619/3355730635.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf_nj[\"buffered\"] = gdf_nj.buffer(0.1)\n/var/folders/7b/6t7qqfj57bb0_ml_y_5bw86r0000gn/T/ipykernel_21619/3355730635.py:4: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf_nj[\"buffered_centroid\"] = gdf_nj[\"centroid\"].buffer(0.05)\n</pre> In\u00a0[18]: Copied! <pre># saving the first plot as an axis and setting alpha (transparency) to 0.5\nax = gdf_nj[\"buffered\"].plot(alpha=0.5)\n# passing the first plot as an axis to the second\ngdf_nj[\"buffered_centroid\"].plot(ax=ax, color=\"red\", alpha=0.5)\n# # passing the first plot and setting linewidth to 0.5\ngdf_nj[\"boundary\"].plot(ax=ax, color=\"white\", linewidth=0.5)\n</pre> # saving the first plot as an axis and setting alpha (transparency) to 0.5 ax = gdf_nj[\"buffered\"].plot(alpha=0.5) # passing the first plot as an axis to the second gdf_nj[\"buffered_centroid\"].plot(ax=ax, color=\"red\", alpha=0.5) # # passing the first plot and setting linewidth to 0.5 gdf_nj[\"boundary\"].plot(ax=ax, color=\"white\", linewidth=0.5) Out[18]: <pre>&lt;Axes: &gt;</pre> In\u00a0[19]: Copied! <pre>middlesex = gdf_nj.loc[\"Middlesex County\", \"geometry\"]\nmiddlesex\n</pre> middlesex = gdf_nj.loc[\"Middlesex County\", \"geometry\"] middlesex Out[19]: <p>The polygon is a shapely geometry object, as any other geometry used in GeoPandas.</p> In\u00a0[20]: Copied! <pre>type(middlesex)\n</pre> type(middlesex) Out[20]: <pre>shapely.geometry.multipolygon.MultiPolygon</pre> <p>Then we can check which of the geometries in <code>gdf[\"buffered\"]</code> intersects it.</p> In\u00a0[21]: Copied! <pre>gdf_nj[\"buffered\"].intersects(middlesex)\n</pre> gdf_nj[\"buffered\"].intersects(middlesex) Out[21]: <pre>NAME\nAtlantic County      False\nBergen County        False\nBurlington County    False\nCamden County        False\nCape May County      False\nCumberland County    False\nEssex County          True\nGloucester County    False\nHudson County         True\nHunterdon County     False\nMercer County         True\nMiddlesex County      True\nMonmouth County       True\nMorris County         True\nOcean County          True\nPassaic County       False\nSalem County         False\nSomerset County       True\nSussex County        False\nUnion County          True\nWarren County        False\ndtype: bool</pre> In\u00a0[22]: Copied! <pre>import numpy as np\nfrom matplotlib import pyplot as plt\nfrom cartopy import crs as ccrs\nimport cartopy\n</pre> import numpy as np from matplotlib import pyplot as plt from cartopy import crs as ccrs import cartopy  In\u00a0[24]: Copied! <pre>central_lat = 37.5\ncentral_lon = -96\nextent = [-120, -70, 20, 50.5]\ncentral_lon = np.mean(extent[:2])\ncentral_lat = np.mean(extent[2:])\ncrs = ccrs.AlbersEqualArea(central_lon, central_lat)\n\nplt.figure(figsize=(12, 6))\nax = plt.axes(projection=crs)\nax.set_extent(extent)\n\ncrs_proj4 = crs.proj4_init\ngdf.to_crs(crs_proj4).plot('tavg_GWL1', vmin = 1, vmax = 2.5, ax = ax)\n\nax.add_feature(cartopy.feature.STATES, edgecolor='black')\nax.coastlines()\n</pre>  central_lat = 37.5 central_lon = -96 extent = [-120, -70, 20, 50.5] central_lon = np.mean(extent[:2]) central_lat = np.mean(extent[2:]) crs = ccrs.AlbersEqualArea(central_lon, central_lat)  plt.figure(figsize=(12, 6)) ax = plt.axes(projection=crs) ax.set_extent(extent)  crs_proj4 = crs.proj4_init gdf.to_crs(crs_proj4).plot('tavg_GWL1', vmin = 1, vmax = 2.5, ax = ax)  ax.add_feature(cartopy.feature.STATES, edgecolor='black') ax.coastlines()  Out[24]: <pre>&lt;cartopy.mpl.feature_artist.FeatureArtist at 0x18d3d5880&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Lecture_11_Geopandas/#lecture-11-environmental-sciences-packages","title":"Lecture 11: Environmental Sciences Packages\u00b6","text":""},{"location":"Lecture_11_Geopandas/#a-collection-of-python-packages-for-atmospheric-and-environmental-sciences","title":"A collection of Python packages for atmospheric and environmental sciences\u00b6","text":""},{"location":"Lecture_11_Geopandas/#statistics","title":"Statistics\u00b6","text":"<ul> <li>SciPy: SciPy provides algorithms for optimization, integration, interpolation, eigenvalue problems, algebraic equations, differential equations, statistics and many other classes of problems.</li> <li>statsmodels: A python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.</li> </ul>"},{"location":"Lecture_11_Geopandas/#visualizations","title":"Visualizations\u00b6","text":"<ul> <li>Seaborn: A Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.</li> <li>Bokeh An interactive visualization library for modern web browsers.</li> <li>plotly: An interactive, open-source, and browser-based graphing library for Python.</li> </ul>"},{"location":"Lecture_11_Geopandas/#machine-learning","title":"Machine Learning:\u00b6","text":"<ul> <li>Scikit-learn: A machine learning library that provides almost all the machine learning algorithms you might need.</li> <li>PyCaret: PyCaret is an open-source, low-code machine learning library in Python that automates machine learning workflows</li> <li>TensorFlow: An end-to-end open source platform for machine learning.TensorFlow is  a framework for defining and running computations that involve tensors, which are partially defined computational objects that eventually produce a value.</li> </ul>"},{"location":"Lecture_11_Geopandas/#geospatial-analysis-and-mapping","title":"Geospatial Analysis and Mapping\u00b6","text":"<ul> <li>Geopandas: GeoPandas is an open source project to make working with geospatial data in python easier. GeoPandas extends the datatypes used by pandas to allow spatial operations on geometric types.</li> <li>xESMF: Universal regridder for geospatial data.</li> <li>pyResample: Resampling geospatial image data.</li> <li>Rasterio: Rasterio is a GDAL and Numpy-based Python library designed to make your work with geospatial raster data more productive, more fun \u2014 more Zen. It is a highly useful module for raster processing which you can use for reading and writing several different raster formats in Python. Python automatically registers all known GDAL drivers for reading supported formats when importing the module.</li> <li>Regionmask: Plotting and creation of masks of spatial regions</li> <li>GeoViews: Library that makes it easy to explore and visualize geographical, meteorological, and oceanographic datasets, such as those used in weather, climate, and remote sensing research</li> </ul>"},{"location":"Lecture_11_Geopandas/#hydrology","title":"Hydrology:\u00b6","text":"<ul> <li>Pytesmo: Python Toolbox for the Evaluation of Soil Moisture Observations.</li> <li>pyDEM: PyDEM is a package for topographic (terrain) analysis. It takes in digital elevation model (DEM) rasters, and it outputs quantities like slope, aspect, upstream area, and topographic wetness index.</li> <li>HydrPy: A framework for the development and application of hydrological models based on Python.</li> <li>PyFlo: It is an open-source Python library for performing hydraulic and hydrology stormwater analysis. Features include network hydraulic grade analysis and time/iteration based storage and flood routing simulations.</li> </ul>"},{"location":"Lecture_11_Geopandas/#atmospheric-science","title":"Atmospheric Science:\u00b6","text":"<ul> <li>MetPy: It is a collection of tools in Python for reading, visualizing and performing calculations with weather data.</li> <li>ACT: Toolkit for working with atmospheric time-series datasets of varying dimensions.</li> <li>Satpy: Reading, manipulating, and writing data from remote-sensing earth-observing meteorological satellite instruments.</li> <li>climmetlab: Python package aiming at simplifying access to climate and meteorological datasets</li> <li>xgcm: Extends the xarray data model to understand finite volume grid cells (common in General Circulation Models) and provides interpolation and difference operations for such grids</li> </ul>"},{"location":"Lecture_11_Geopandas/#geology","title":"Geology\u00b6","text":"<ul> <li>GemPy: GemPy is a tool for generating three-dimensional structural geological models in Python. It allows the user to create complex combinations of stratigraphical and structural features such as folds, faults, and unconformities. It was furthermore designed to enable probabilistic modeling to address parameter and model uncertainties.</li> <li>MTpy: MTpy is a Python Toolbox for Magnetotelluric (MT) Data Processing, Analysis, Modelling and Visualization.</li> <li>Pyleoclim: Pyleoclim is a Python package designed for the analysis of paleoclimate data. Pyleoclim leverages various data science libraries (numpy, pandas, scikit-learn) for time series analysis, as well as and Matplotlib and Cartopy for the creation of publication-quality figures.</li> </ul>"},{"location":"Lecture_11_Geopandas/#geopandas","title":"Geopandas\u00b6","text":"<p>GeoPandas, as the name suggests, extends the popular data science library pandas by adding support for geospatial data.</p> <p>The core data structure in GeoPandas is the <code>geopandas.GeoDataFrame</code>, a subclass of <code>pandas.DataFrame</code>, that can store geometry columns and perform spatial operations. The <code>geopandas.GeoSeries</code>, a subclass of <code>pandas.Series</code>, handles the geometries. Therefore, your <code>GeoDataFrame</code> is a combination of <code>pandas.Series</code>, with traditional data (numerical, boolean, text etc.), and <code>geopandas.GeoSeries</code>, with geometries (points, polygons etc.). You can have as many columns with geometries as you wish; there's no limit typical for desktop GIS software.</p> <p></p> <p>Each <code>GeoSeries</code> can contain any geometry type (you can even mix them within a single array) and has a <code>GeoSeries.crs</code> attribute, which stores information about the projection (CRS stands for Coordinate Reference System). Therefore, each <code>GeoSeries</code> in a <code>GeoDataFrame</code> can be in a different projection, allowing you to have, for example, multiple versions (different projections) of the same geometry.</p> <p>Only one <code>GeoSeries</code> in a <code>GeoDataFrame</code> is considered the active geometry, which means that all geometric operations applied to a <code>GeoDataFrame</code> operate on this active column.</p>"},{"location":"Lecture_11_Geopandas/#reading-files","title":"Reading files\u00b6","text":""},{"location":"Lecture_11_Geopandas/#simple-accessors-and-methods","title":"Simple accessors and methods\u00b6","text":"<p>Now we have our <code>GeoDataFrame</code> and can start working with its geometry.</p> <p>Since there was only one geometry column in the NCA dataset, this column automatically becomes the active geometry and spatial methods used on the <code>GeoDataFrame</code> will be applied to the <code>\"geometry\"</code> column.</p>"},{"location":"Lecture_11_Geopandas/#getting-polygon-boundary-and-centroid","title":"Getting polygon boundary and centroid\u00b6","text":"<p>To get the boundary of each polygon (LineString), access the <code>GeoDataFrame.boundary</code>:</p>"},{"location":"Lecture_11_Geopandas/#plotting-data-on-maps","title":"Plotting Data on Maps\u00b6","text":"<p>GeoPandas can also plot maps, so we can check how the geometries appear in space. To plot the active geometry, call <code>GeoDataFrame.plot()</code>. To color code by data, pass in that column as the first argument.</p>"},{"location":"Lecture_11_Geopandas/#buffer","title":"Buffer\u00b6","text":"<p>In other cases, we may need to buffer the geometry using <code>GeoDataFrame.buffer()</code>. Geometry methods are automatically applied to the active geometry, but we can apply them directly to any <code>GeoSeries</code> as well. Let's buffer the counties and their centroids and plot both on top of each other.</p>"},{"location":"Lecture_11_Geopandas/#geometry-relations","title":"Geometry relations\u00b6","text":"<p>We can also ask about the spatial relations of different geometries. Using the geometries above, we can check which of the buffered counties intersect the original geometry of Middlesex, i.e., is within 0.1 degree difference from Middlesex.</p> <p>First, we get a polygon of Middlesex.</p>"},{"location":"Lecture_11_Geopandas/#integration-with-cartopy","title":"Integration with Cartopy\u00b6","text":""},{"location":"Lecture_12_reproducible_research_packaging/","title":"Lecture 12: Organization and Packaging of Python Projects","text":"<p>In Assginment 2, we wrote several functions for unit conversion. Now let's write a module for these functions. Open a file called <code>temperature_unit_convert.py</code> in a text editor. The file should be in the same directory as the notebook you are working in now.) Populate it with the functions you defined. Mine is like this:</p> <pre>\"\"\"\nA python module for unit conversion for temperature.\n\"\"\"\n\ndef k_to_c(temp):\n\"\"\"Convert temperature from kelvin to celsius.\n    PARAMETERS\n    ----------\n    temp : float\n        Temperature in Kelvin. \n    RETURNS\n    -------\n    temp_c : float\n        Temperature in Celsius. \n    \"\"\"\n\n    temp_c = temp - 273.15\n    return temp_c\n\ndef c_to_k(temp):\n\"\"\"Convert temperature from kelvin to celsius.\n    PARAMETERS\n    ----------\n    temp : float\n        Temperature in Celsius. \n    RETURNS\n    -------\n    temp_F : float\n        Temperature in Kelvin. \n    \"\"\"    \n    temp_k = temp+273.15\n    return temp_k\n\ndef temp_to_F(temp, C = True):\n\"\"\"Convert temperature to Fahrenheit.\n    PARAMETERS\n    ----------\n    temp : float\n        Temperature in Celsius or Kelvin. \n    C    : bool, default: True\n        If True, input temperature is in Celsius. If False, input temperature is in Kelvin.\n    RETURNS\n    -------\n    temp_F : float\n        Temperature in Fahrenheit. \n    \"\"\"        \n    if C:\n        temp_F = (temp * 9/5) + 32 \n    else:\n        temp_c = k_to_c(temp)\n        temp_F = (temp_c * 9/5) + 32 \n    return temp_F\n        \n\ndef temp_from_F(temp, C = True):\n\"\"\"Convert temperature from Fahrenheit to Celsius or Kelvin.\n    PARAMETERS\n    ----------\n    temp : float\n        Temperature in Fahrenheit. \n    C    : bool, default: True\n        If True, out temperature is in Celsius. If False, out temperature is in Kelvin.\n    RETURNS\n    -------\n    temp_F : float\n        Temperature in Celsius or Kelvin. \n    \"\"\"   \n    \n    temp_c = (temp - 32) * 5/9\n    if C:\n        return(temp_c) \n    else:\n        temp_k = c_to_k(temp_c)\n    return temp_k\n</pre> <p>The module begins with a docstring explaining what it does. Then it contains some data (just a constant <code>R</code>) and a single function.</p> <p>Now let's import our module</p> In\u00a0[1]: Copied! <pre>import temperature_unit_convert\nhelp(temperature_unit_convert)\n</pre> import temperature_unit_convert help(temperature_unit_convert)  <pre>Help on module temperature_unit_convert:\n\nNAME\n    temperature_unit_convert - A python module for unit conversion for temperature.\n\nFUNCTIONS\n    c_to_k(temp)\n        Convert temperature from kelvin to celsius.\n        \n        PARAMETERS\n        ----------\n        temp : float\n            Temperature in Celsius. \n            \n        RETURNS\n        -------\n        temp_F : float\n            Temperature in Kelvin.\n    \n    k_to_c(temp)\n        Convert temperature from kelvin to celsius.\n        \n        PARAMETERS\n        ----------\n        temp : float\n            Temperature in Kelvin. \n            \n        RETURNS\n        -------\n        temp_c : float\n            Temperature in Celsius.\n    \n    temp_from_F(temp, C=True)\n        Convert temperature from Fahrenheit to Celsius or Kelvin.\n        \n        PARAMETERS\n        ----------\n        temp : float\n            Temperature in Fahrenheit. \n        C    : bool, default: True\n            If True, out temperature is in Celsius. If False, out temperature is in Kelvin.\n        \n        RETURNS\n        -------\n        temp_F : float\n            Temperature in Celsius or Kelvin.\n    \n    temp_to_F(temp, C=True)\n        Convert temperature to Fahrenheit.\n        \n        PARAMETERS\n        ----------\n        temp : float\n            Temperature in Celsius or Kelvin. \n        C    : bool, default: True\n            If True, input temperature is in Celsius. If False, input temperature is in Kelvin.\n        \n        RETURNS\n        -------\n        temp_F : float\n            Temperature in Fahrenheit.\n\nFILE\n    /Users/xiaomengjin/Dropbox/0_Rutgers/3_Teaching/Research_Computing/Lectures/temperature_unit_convert.py\n\n\n</pre> <p>And let's try using it to make a calculation</p> In\u00a0[2]: Copied! <pre>temperature_unit_convert.c_to_k(0)\n</pre> temperature_unit_convert.c_to_k(0) Out[2]: <pre>273.15</pre> In\u00a0[5]: Copied! <pre>temperature_unit_convert.temp_to_F(300, C = False)\n</pre> temperature_unit_convert.temp_to_F(300, C = False) Out[5]: <pre>80.33000000000004</pre> <p>We could just import the function we need</p> In\u00a0[7]: Copied! <pre>from temperature_unit_convert import c_to_k\nc_to_k(0)\n</pre> from temperature_unit_convert import c_to_k c_to_k(0) Out[7]: <pre>273.15</pre> <p>If we change the module, we need to either restart our kernel or else reload the module.</p> In\u00a0[8]: Copied! <pre>from importlib import reload\nreload(temperature_unit_convert)\n</pre> from importlib import reload reload(temperature_unit_convert)  Out[8]: <pre>&lt;module 'temperature_unit_convert' from '/Users/xiaomengjin/Dropbox/0_Rutgers/3_Teaching/Research_Computing/Lectures/temperature_unit_convert.py'&gt;</pre> <p>Modules are a simple way to share code between different scripts or notebooks in the same project. Module files must reside in the same directory as any script which imports them! This is a big limitation; it means you can't share modules between different projects.</p> <p>Once you have a piece of code that is general-purpose enough to share between projects, you need to create a package.</p> <p><code>setup.py</code> is the magic file that makes your package installable and accessible anywhere. Here is an extremely basic <code>setup.py</code></p> <pre>from setuptools import setup\n\nsetup(\n    name = \"temperature_unit_convert\",\n    version = \"0.1.0\",\n    author = \"Xiaomeng Jin\",\n    packages=['temperature_unit_convert'],\n    install_requires=['numpy'],\n)\n</pre> <p>There is a dizzying range of options for <code>setup.py</code>. More fields are required if you want to upload your package to pypi (so it is installable via <code>pip</code>).</p> <p>To run the setup script, we call the following from the command line</p> <pre><code>python setup.py install</code></pre> <p>The package files are copied to our python library directory. If we plan to keep developing the package, we can install it in \"developer mode\" as</p> <pre><code>python setup.py develop</code></pre> <p>In this case, the files are symlinked rather than copied.</p>"},{"location":"Lecture_12_reproducible_research_packaging/#lecture-12-organization-and-packaging-of-python-projects","title":"Lecture 12: Organization and Packaging of Python Projects\u00b6","text":"<p>A complex research project often relies and many different programs and software packages to accomplish the research goals. An important part of scientific computing is deciding how to organize and structure the code you use for research. A well-structured project can make you a more efficient and effective researcher.</p> <p>Just putting all of your code into git repositories won't magically turn a mess of scripts into a beautiful, well-organized project. More deliberate effort is required.</p>"},{"location":"Lecture_12_reproducible_research_packaging/#types-of-projects","title":"Types of Projects\u00b6","text":"<p>Not all projects are created equal. Based on my experience, I categorize three different types of \"research code\" scenarios commonly encountered in geosciences.</p> <ol> <li>Exploratory analyses: When exploring a new idea, a single notebook or script is often all we need.</li> <li>A Single Paper: The \"paper\" is a standard unit of scientific output. The code related to a single paper usually belongs together.</li> <li>Reusable software elements: In the course of our research computing, we often identify specialized routines that we want to package for reuse in other projects, or by other scientists. This is where \"scripts\" become \"software.\"</li> </ol> <p>This lecture outlines some suggested practices for each category.</p>"},{"location":"Lecture_12_reproducible_research_packaging/#exploratory-analysis","title":"Exploratory Analysis\u00b6","text":"<p>When starting something new, we are often motivated to just start coding and get some results quick. This is fine! Jupyter notebooks are an ideal format for open-ended exploratory analysis, since they are totally self-contained: they encapsulate text, code, and figures. If we find someting cool or useful, it is important to preserve these exploratory notebooks.</p> <p>A dedicated github repository can be overkill for a single file. Instead, I recommend github's \"gist\" mechanism for saving and sharing such \"one-off\" notebooks and code snippets. Gists are like mini repos you can easily share and embed. (You can create one right now by going to https://gist.github.com/.)</p> <p>You can upload any file (including an <code>.ipynb</code> notebook file) by dragging and dropping it into the gist website. You have the choice of making you gist public or secret. (There is no private option, but a secret gist can only be seen by others if you give them the URL.)</p> <p>GitHub's rendering of Gists is a bit buggy. For a more consistent rendering experience, you can share your gist via http://nbviewer.ipython.org/.</p>"},{"location":"Lecture_12_reproducible_research_packaging/#a-single-paper","title":"A Single Paper\u00b6","text":""},{"location":"Lecture_12_reproducible_research_packaging/#scientific-reproducibility","title":"Scientific Reproducibility\u00b6","text":"<p>Reproducibility is a cornerstone of the scientific process. However, today one often reads that science is in the midst of a reproducibility crisis. This crisis may be due to increasing complexity and cost of scientific analysis, together with mounting pressure to publish as much and as quickly as possible.</p> <p>Today almost all earth science relies on some form of computation, from simple statistical analysis and curve fitting to advanced numerical simulation. In principle, computational science should be highly reproducible. Keep in mind that the audience for a reproducibile project is not just other scientists...it's you, a year from now, or whenever you need to repeat and / or build on earlier work. Most scientists build on their Ph.D. work for a decade following graduation. Extra time spent on reproducibility now will make you more productive in the long run.</p> <p>We begin with an important observation.</p> <p>An article about computational science \u2026 is not the scholarship itself, it\u2019s merely scholarship advertisement. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures.</p> <p>Donoho, D. et al. (2009), Reproducible research in computational harmonic analysis, Comp. Sci. Eng. 11(1):8\u201318, doi: 10.1109/MCSE.2009.15</p> <p>Sandve et al. (2013) give some specific recommmendations for computational reproducibility.</p> <ol> <li>For every result, keep track of how it was produced</li> <li>Avoid manual data-manipulation steps</li> <li>Archive the exact versions of all external programs used</li> <li>Version-control all custom scripts</li> <li>Record all intermediate results, when possible in standard formats</li> <li>For analyses that include randomness, note underlying random seeds</li> <li>Always store raw data behind plots</li> <li>Generate hierarchical analysis output, allowing layers of increasing detail to be inspected</li> <li>Connect textual statements to underlying results</li> <li>Provide public access to scripts, runs, and results</li> </ol> <p>These recommendations suggest a certain structure for a project.</p>"},{"location":"Lecture_12_reproducible_research_packaging/#project-layout","title":"Project Layout\u00b6","text":"<p>A reproducible single-paper project directory structure might look something like this</p> <pre><code>README.md\nLICENSE\nenvironment.yml\ndata/intermediate_results.csv\nnotebooks/process_raw_data.ipynb\nnotebooks/figure1.ipynb\nnotebooks/figure2.ipynb\nnotebooks/helper.py\nmanuscript/manuscript.tex</code></pre>"},{"location":"Lecture_12_reproducible_research_packaging/#reuseable-software-elements","title":"Reuseable Software Elements\u00b6","text":"<p>Scientific software can perhaps be grouped into two categories: single-use \"scripts\" that are used in a very specific context to do a very specific thing (e.g.~to generate a specific figure for a paper), and reuseable components which encapsulate a more generic workflow. Once you find yourself repeating the same chunks of code in many different scripts or projects, it's time to start composing reusable software elements.</p>"},{"location":"Lecture_12_reproducible_research_packaging/#modules","title":"Modules\u00b6","text":"<p>The basic element of reusability in python is the module. A module is a <code>.py</code> file which contains python objects which can be imported by other scripts or notebooks. Let's illustrate how modules work with a simple example.</p>"},{"location":"Lecture_12_reproducible_research_packaging/#packages","title":"Packages\u00b6","text":"<p>Packages are python's way of encapsulating reusable code elements for sharing with others. Packaging is a huge and complicated topic. We will just scratch the surface.</p> <p>We have already interacted with many packages already. Browse some of their github repositories to explore the structure of a large python package:</p> <ul> <li>NumPy: https://github.com/numpy/numpy</li> <li>Pandas: https://github.com/pandas-dev/pandas</li> <li>Xarray: https://github.com/pydata/xarray</li> </ul> <p>These packages all have a common basic structure. Imagine we wanted to turn our temperature unit conversion module into a package. It would look like this.</p> <pre><code>README.md\nLICENSE\nenvironment.yml\nrequirements.txt\nsetup.py\ntemperature_unit_convert/__init__.py\ntemperature_unit_convert/temperature_unit_convert.py\ntemperature_unit_convert/tests/__init__.py\ntemperature_unit_convert/tests/test_unit_convert.py</code></pre> <p>The actual package is contained in the <code>temperature_unit_convert</code> subdirectory. The other files are auxilliary files which help others understand and install your package. Here is an overview of what they do</p> File Name Purpose <code>README.md</code> Explain what the package is for <code>LICENSE</code> Defines the legal terms under which other can use the package. Open source is encouraged! <code>environment.yml</code> A conda environment which describes the package's dependencies (more info) <code>requirements.txt</code> A file which describes the package's dependences for pip. (more info) <code>setup.py</code> A special python script which installs your package. (more info)"},{"location":"Lecture_12_reproducible_research_packaging/#the-actual-package","title":"The actual package\u00b6","text":"<p>The directory <code>temperature_unit_convert</code> is the actual package. Any directory that contains an <code>__init__.py</code> file is recognized by python as a package. This file can be blank, bu it needs to be present. From the root directory, we can import a module from the package as follows</p> <pre>from temperature_unit_convert import temperature_unit_convert\n</pre> <p>Yes, this is a bit redundant. That's because the <code>temperature_unit_convert.py</code> module has the same name as the <code>temperature_unit_convert</code> package directory.</p> <p>However, this import will only work from the parent directory. It is not globally accessible from your python environment.</p>"},{"location":"Lecture_12_reproducible_research_packaging/#testing","title":"Testing\u00b6","text":"<p>A software package requires tests to ensure that it works properly.</p> <p>Tests don't have to be complicated. They are simply a check to verify that your code does what it is supposed to do.</p> <p>To add tests to our project, we create create the file <code>temperature_unit_convert/tests/test_unit_convert.py</code>. (We also need an <code>__init__.py</code> file in the <code>tests</code> directory.) The example below shows an example of a test function for our package.</p> <pre>import pytest\n\nfrom temperature_unit_convert.temperature_unit_convert import k_to_c, c_to_k, temp_from_F, temp_to_F\n\ndef test_unit_convert():\n    # some known results\n    # Verify that the \"round trip\" conversion from and back to C.\n    \n    for orig in [10, 20, 30]:\n        new = k_to_c(c_to_k(orig))\n        assert new == orig\n    # Verify that the \"round trip\" conversion from and back to F.\n        \n    for orig_F in [100, 90, 95]:\n        new = temp_from_F(temp_to_F(orig_F))\n        assert new == orig_F    \n    \n    # now check that we can't pass the wrong number of arguments\n    with pytest.raises(TypeError):\n        k_to_c(1, 2, 3)\n</pre> <p>We will use pytest to run our tests. If you don't have pytest installed in your active python environment, take a minute to run <code>pip install pytest</code> from the command line. Now run</p> <pre>py.test -v\n</pre> <p>from the root directory of your project. You should see a notification that the tests passed. Try playing around with the tests to cause something to fail.</p>"},{"location":"Lecture_12_reproducible_research_packaging/#publishing-python-package-to-github","title":"Publishing python package to github\u00b6","text":"<p>Go to your GitHub, open a new repository named <code>temperature_unit_convert</code>. Make it Public. Under terminal, go back to your package directory. First clean the directory:</p> <pre><code>python setup.py clean --all \n</code></pre> <p>Initiate a git repository:</p> <pre><code>git init\n</code></pre> <p>Add and commit your package:</p> <pre><code>git add *\ngit commit -m 'initial commit'\n</code></pre> <p>Follow the command on GitHub, push this repository to your new GitHub repository</p> <pre><code>git remote add origin https://github.com/xjin49/temperature_unit_convert.git\ngit branch -M main\ngit push -u origin main\n</code></pre> <p>Now you should be able to see your python package in your Github.</p>"},{"location":"Lecture_13_Cloud_computing/","title":"Lecture 13: Cloud computing","text":"<p>Reference: https://github.com/pangeo-gallery/cmip6</p> <p>Install python packages: zarr, fsspec, gcsfs, nc-time-axis</p> In\u00a0[\u00a0]: Copied! <pre>from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport zarr\nimport fsspec\n</pre> from matplotlib import pyplot as plt import numpy as np import pandas as pd import xarray as xr import zarr import fsspec In\u00a0[3]: Copied! <pre>df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\ndf.head()\n</pre> df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv') df.head() Out[3]: activity_id institution_id source_id experiment_id member_id table_id variable_id grid_label zstore dcpp_init_year version 0 HighResMIP CMCC CMCC-CM2-HR4 highresSST-present r1i1p1f1 Amon ps gn gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/... NaN 20170706 1 HighResMIP CMCC CMCC-CM2-HR4 highresSST-present r1i1p1f1 Amon rsds gn gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/... NaN 20170706 2 HighResMIP CMCC CMCC-CM2-HR4 highresSST-present r1i1p1f1 Amon rlus gn gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/... NaN 20170706 3 HighResMIP CMCC CMCC-CM2-HR4 highresSST-present r1i1p1f1 Amon rlds gn gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/... NaN 20170706 4 HighResMIP CMCC CMCC-CM2-HR4 highresSST-present r1i1p1f1 Amon psl gn gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/... NaN 20170706 In\u00a0[4]: Copied! <pre>df['source_id'].unique()\n</pre> df['source_id'].unique() Out[4]: <pre>array(['CMCC-CM2-HR4', 'EC-Earth3P-HR', 'HadGEM3-GC31-MM',\n       'HadGEM3-GC31-HM', 'HadGEM3-GC31-LM', 'EC-Earth3P', 'ECMWF-IFS-HR',\n       'ECMWF-IFS-LR', 'HadGEM3-GC31-LL', 'CMCC-CM2-VHR4', 'GFDL-CM4',\n       'GFDL-AM4', 'IPSL-CM6A-LR', 'E3SM-1-0', 'CNRM-CM6-1', 'GFDL-ESM4',\n       'GFDL-ESM2M', 'GFDL-CM4C192', 'GFDL-OM4p5B', 'GISS-E2-1-G',\n       'GISS-E2-1-H', 'CNRM-ESM2-1', 'BCC-CSM2-MR', 'BCC-ESM1', 'MIROC6',\n       'AWI-CM-1-1-MR', 'EC-Earth3-LR', 'IPSL-CM6A-ATM-HR', 'CESM2',\n       'CESM2-WACCM', 'CNRM-CM6-1-HR', 'MRI-ESM2-0', 'CanESM5',\n       'SAM0-UNICON', 'GISS-E2-1-G-CC', 'UKESM1-0-LL', 'EC-Earth3',\n       'EC-Earth3-Veg', 'FGOALS-f3-L', 'CanESM5-CanOE', 'INM-CM4-8',\n       'INM-CM5-0', 'NESM3', 'MPI-ESM-1-2-HAM', 'CAMS-CSM1-0',\n       'MPI-ESM1-2-LR', 'MPI-ESM1-2-HR', 'MRI-AGCM3-2-H', 'MRI-AGCM3-2-S',\n       'MCM-UA-1-0', 'INM-CM5-H', 'KACE-1-0-G', 'NorESM2-LM',\n       'FGOALS-f3-H', 'FGOALS-g3', 'MIROC-ES2L', 'FIO-ESM-2-0', 'NorCPM1',\n       'NorESM1-F', 'MPI-ESM1-2-XR', 'CESM1-1-CAM5-CMIP5', 'E3SM-1-1',\n       'KIOST-ESM', 'ACCESS-CM2', 'NorESM2-MM', 'ACCESS-ESM1-5',\n       'IITM-ESM', 'GISS-E2-2-G', 'CESM2-FV2', 'GISS-E2-2-H',\n       'CESM2-WACCM-FV2', 'CIESM', 'E3SM-1-1-ECA', 'TaiESM1',\n       'AWI-ESM-1-1-LR', 'EC-Earth3-Veg-LR', 'CMCC-ESM2', 'CAS-ESM2-0',\n       'CMCC-CM2-SR5', 'EC-Earth3-AerChem', 'IPSL-CM6A-LR-INCA',\n       'IPSL-CM5A2-INCA', 'BCC-CSM2-HR', 'EC-Earth3P-VHR',\n       'CESM1-WACCM-SC', 'EC-Earth3-CC', 'MIROC-ES2H', 'ICON-ESM-LR'],\n      dtype=object)</pre> In\u00a0[5]: Copied! <pre>df['experiment_id'].unique()\n</pre> df['experiment_id'].unique() Out[5]: <pre>array(['highresSST-present', 'piControl', 'control-1950', 'hist-1950',\n       'historical', 'amip', 'abrupt-4xCO2', 'abrupt-2xCO2',\n       'abrupt-0p5xCO2', '1pctCO2', 'ssp585', 'esm-piControl', 'esm-hist',\n       'hist-piAer', 'histSST-1950HC', 'ssp245', 'hist-1950HC', 'histSST',\n       'piClim-2xVOC', 'piClim-2xNOx', 'piClim-2xdust', 'piClim-2xss',\n       'piClim-histall', 'hist-piNTCF', 'histSST-piNTCF',\n       'aqua-control-lwoff', 'piClim-lu', 'histSST-piO3', 'piClim-CH4',\n       'piClim-NTCF', 'piClim-NOx', 'piClim-O3', 'piClim-HC',\n       'faf-heat-NA0pct', 'ssp370SST-lowCH4', 'piClim-VOC',\n       'ssp370-lowNTCF', 'piClim-control', 'piClim-aer', 'hist-aer',\n       'faf-heat', 'faf-heat-NA50pct', 'ssp370SST-lowNTCF',\n       'ssp370SST-ssp126Lu', 'ssp370SST', 'ssp370pdSST', 'histSST-piAer',\n       'piClim-ghg', 'piClim-anthro', 'faf-all', 'hist-nat', 'hist-GHG',\n       'ssp119', 'piClim-histnat', 'piClim-4xCO2', 'ssp370',\n       'piClim-histghg', 'highresSST-future', 'esm-ssp585-ssp126Lu',\n       'ssp126-ssp370Lu', 'ssp370-ssp126Lu', 'land-noLu', 'histSST-piCH4',\n       'ssp126', 'esm-pi-CO2pulse', 'amip-hist', 'piClim-histaer',\n       'amip-4xCO2', 'faf-water', 'faf-passiveheat', '1pctCO2-rad',\n       'faf-stress', '1pctCO2-bgc', 'aqua-control', 'amip-future4K',\n       'amip-p4K', 'aqua-p4K', 'amip-lwoff', 'amip-m4K', 'aqua-4xCO2',\n       'amip-p4K-lwoff', 'hist-noLu', '1pctCO2-cdr',\n       'land-hist-altStartYear', 'land-hist', 'omip1', 'esm-pi-cdr-pulse',\n       'esm-ssp585', 'abrupt-solp4p', 'piControl-spinup', 'hist-stratO3',\n       'abrupt-solm4p', 'midHolocene', 'lig127k', 'aqua-p4K-lwoff',\n       'esm-piControl-spinup', 'ssp245-GHG', 'ssp245-nat',\n       'dcppC-amv-neg', 'dcppC-amv-ExTrop-neg', 'dcppC-atl-control',\n       'dcppC-amv-pos', 'dcppC-ipv-NexTrop-neg', 'dcppC-ipv-NexTrop-pos',\n       'dcppC-atl-pacemaker', 'dcppC-amv-ExTrop-pos',\n       'dcppC-amv-Trop-neg', 'dcppC-pac-control', 'dcppC-ipv-pos',\n       'dcppC-pac-pacemaker', 'dcppC-ipv-neg', 'dcppC-amv-Trop-pos',\n       'piClim-BC', 'piClim-2xfire', 'piClim-SO2', 'piClim-OC',\n       'piClim-N2O', 'piClim-2xDMS', 'ssp460', 'ssp434', 'ssp534-over',\n       'deforest-globe', 'historical-cmip5', 'hist-bgc',\n       'piControl-cmip5', 'rcp26-cmip5', 'rcp45-cmip5', 'rcp85-cmip5',\n       'pdSST-piArcSIC', 'pdSST-piAntSIC', 'piSST-piSIC', 'piSST-pdSIC',\n       'ssp245-stratO3', 'hist-sol', 'hist-CO2', 'hist-volc',\n       'hist-totalO3', 'hist-nat-cmip5', 'hist-aer-cmip5',\n       'hist-GHG-cmip5', 'pdSST-futAntSIC', 'futSST-pdSIC', 'pdSST-pdSIC',\n       'ssp245-aer', 'pdSST-futArcSIC', 'dcppA-hindcast', 'dcppA-assim',\n       'dcppC-hindcast-noPinatubo', 'dcppC-hindcast-noElChichon',\n       'dcppC-hindcast-noAgung', 'hist-resIPO', 'ssp245-cov-modgreen',\n       'ssp245-cov-fossil', 'ssp245-cov-strgreen', 'ssp245-covid', 'lgm',\n       'ssp585-bgc', '1pctCO2to4x-withism', '1pctCO2-4xext', 'past1000',\n       'pa-futArcSIC', 'pa-pdSIC', 'historical-ext', 'pdSST-futArcSICSIT',\n       'pdSST-futOkhotskSIC', 'pdSST-futBKSeasSIC', 'pa-piArcSIC',\n       'pa-piAntSIC', 'pa-futAntSIC', 'pdSST-pdSICSIT'], dtype=object)</pre> In\u00a0[6]: Copied! <pre>df['institution_id'].unique()\n</pre> df['institution_id'].unique() Out[6]: <pre>array(['CMCC', 'EC-Earth-Consortium', 'MOHC', 'ECMWF', 'NOAA-GFDL',\n       'IPSL', 'E3SM-Project', 'CNRM-CERFACS', 'NASA-GISS', 'BCC',\n       'MIROC', 'AWI', 'NCAR', 'MRI', 'CCCma', 'SNU', 'CAS', 'INM',\n       'NUIST', 'HAMMOZ-Consortium', 'CAMS', 'MPI-M', 'DKRZ', 'DWD', 'UA',\n       'NIMS-KMA', 'NCC', 'FIO-QLNM', 'KIOST', 'CSIRO-ARCCSS', 'CSIRO',\n       'CCCR-IITM', 'THU', 'AS-RCEC', 'NERC', 'RUBISCO'], dtype=object)</pre> <p>The columns of the dataframe correspond to the CMI6 controlled vocabulary. A beginners' guide to these terms is available in this document.</p> <p>Here we filter the data to find monthly surface air temperature for historical experiments.</p> In\u00a0[7]: Copied! <pre>df_ta = df.query(\"activity_id=='CMIP' &amp; table_id == 'Amon' &amp; variable_id == 'tas' &amp; experiment_id == 'historical'\")\ndf_ta\n</pre> df_ta = df.query(\"activity_id=='CMIP' &amp; table_id == 'Amon' &amp; variable_id == 'tas' &amp; experiment_id == 'historical'\") df_ta Out[7]: activity_id institution_id source_id experiment_id member_id table_id variable_id grid_label zstore dcpp_init_year version 973 CMIP NOAA-GFDL GFDL-ESM4 historical r3i1p1f1 Amon tas gr1 gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/hist... NaN 20180701 1766 CMIP NOAA-GFDL GFDL-ESM4 historical r2i1p1f1 Amon tas gr1 gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/hist... NaN 20180701 8074 CMIP NOAA-GFDL GFDL-CM4 historical r1i1p1f1 Amon tas gr1 gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-CM4/histo... NaN 20180701 22185 CMIP IPSL IPSL-CM6A-LR historical r8i1p1f1 Amon tas gr gs://cmip6/CMIP6/CMIP/IPSL/IPSL-CM6A-LR/histor... NaN 20180803 22298 CMIP IPSL IPSL-CM6A-LR historical r2i1p1f1 Amon tas gr gs://cmip6/CMIP6/CMIP/IPSL/IPSL-CM6A-LR/histor... NaN 20180803 ... ... ... ... ... ... ... ... ... ... ... ... 522952 CMIP MRI MRI-ESM2-0 historical r7i1p1f1 Amon tas gn gs://cmip6/CMIP6/CMIP/MRI/MRI-ESM2-0/historica... NaN 20210813 523274 CMIP MRI MRI-ESM2-0 historical r6i1p1f1 Amon tas gn gs://cmip6/CMIP6/CMIP/MRI/MRI-ESM2-0/historica... NaN 20210907 523712 CMIP CMCC CMCC-CM2-SR5 historical r3i1p2f1 Amon tas gn gs://cmip6/CMIP6/CMIP/CMCC/CMCC-CM2-SR5/histor... NaN 20211108 523721 CMIP CMCC CMCC-CM2-SR5 historical r2i1p2f1 Amon tas gn gs://cmip6/CMIP6/CMIP/CMCC/CMCC-CM2-SR5/histor... NaN 20211109 523769 CMIP EC-Earth-Consortium EC-Earth3-Veg historical r1i1p1f1 Amon tas gr gs://cmip6/CMIP6/CMIP/EC-Earth-Consortium/EC-E... NaN 20211207 <p>635 rows \u00d7 11 columns</p> <p>Now we do further filtering to find just the models from NOAA-GFDL.</p> In\u00a0[17]: Copied! <pre>df_ta_gfdl = df_ta.query('institution_id == \"NOAA-GFDL\"')\ndf_ta_gfdl\n</pre> df_ta_gfdl = df_ta.query('institution_id == \"NOAA-GFDL\"') df_ta_gfdl Out[17]: activity_id institution_id source_id experiment_id member_id table_id variable_id grid_label zstore dcpp_init_year version 973 CMIP NOAA-GFDL GFDL-ESM4 historical r3i1p1f1 Amon tas gr1 gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/hist... NaN 20180701 1766 CMIP NOAA-GFDL GFDL-ESM4 historical r2i1p1f1 Amon tas gr1 gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/hist... NaN 20180701 8074 CMIP NOAA-GFDL GFDL-CM4 historical r1i1p1f1 Amon tas gr1 gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-CM4/histo... NaN 20180701 244695 CMIP NOAA-GFDL GFDL-ESM4 historical r1i1p1f1 Amon tas gr1 gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/hist... NaN 20190726 In\u00a0[18]: Copied! <pre>df_ta_gfdl\n</pre> df_ta_gfdl Out[18]: activity_id institution_id source_id experiment_id member_id table_id variable_id grid_label zstore dcpp_init_year version 973 CMIP NOAA-GFDL GFDL-ESM4 historical r3i1p1f1 Amon tas gr1 gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/hist... NaN 20180701 1766 CMIP NOAA-GFDL GFDL-ESM4 historical r2i1p1f1 Amon tas gr1 gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/hist... NaN 20180701 8074 CMIP NOAA-GFDL GFDL-CM4 historical r1i1p1f1 Amon tas gr1 gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-CM4/histo... NaN 20180701 244695 CMIP NOAA-GFDL GFDL-ESM4 historical r1i1p1f1 Amon tas gr1 gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/hist... NaN 20190726 In\u00a0[19]: Copied! <pre># get the path to a specific zarr store (the first one from the dataframe above)\nzstore = df_ta_gfdl.zstore.values[-1]\nprint(zstore)\n\n# create an interface to the store\nmapper = fsspec.get_mapper(zstore)\n\n# open it using xarray and zarr\nds = xr.open_zarr(mapper, consolidated=True)\nds\n</pre> # get the path to a specific zarr store (the first one from the dataframe above) zstore = df_ta_gfdl.zstore.values[-1] print(zstore)  # create an interface to the store mapper = fsspec.get_mapper(zstore)  # open it using xarray and zarr ds = xr.open_zarr(mapper, consolidated=True) ds <pre>gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/historical/r1i1p1f1/Amon/tas/gr1/v20190726/\n</pre> Out[19]: <pre>&lt;xarray.Dataset&gt;\nDimensions:    (bnds: 2, lat: 180, lon: 288, time: 1980)\nCoordinates:\n  * bnds       (bnds) float64 1.0 2.0\n    height     float64 ...\n  * lat        (lat) float64 -89.5 -88.5 -87.5 -86.5 ... 86.5 87.5 88.5 89.5\n    lat_bnds   (lat, bnds) float64 dask.array&lt;chunksize=(180, 2), meta=np.ndarray&gt;\n  * lon        (lon) float64 0.625 1.875 3.125 4.375 ... 355.6 356.9 358.1 359.4\n    lon_bnds   (lon, bnds) float64 dask.array&lt;chunksize=(288, 2), meta=np.ndarray&gt;\n  * time       (time) object 1850-01-16 12:00:00 ... 2014-12-16 12:00:00\n    time_bnds  (time, bnds) object dask.array&lt;chunksize=(1980, 2), meta=np.ndarray&gt;\nData variables:\n    tas        (time, lat, lon) float32 dask.array&lt;chunksize=(600, 180, 288), meta=np.ndarray&gt;\nAttributes: (12/49)\n    Conventions:            CF-1.7 CMIP-6.0 UGRID-1.0\n    activity_id:            CMIP\n    branch_method:          standard\n    branch_time_in_child:   0.0\n    branch_time_in_parent:  36500.0\n    comment:                &lt;null ref&gt;\n    ...                     ...\n    variable_id:            tas\n    variant_info:           N/A\n    variant_label:          r1i1p1f1\n    status:                 2019-09-10;created;by nhn2@columbia.edu\n    netcdf_tracking_ids:    hdl:21.14100/75e5c5a7-d7c4-4860-beb1-db454f25f13a...\n    version_id:             v20190726</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>bnds: 2</li><li>lat: 180</li><li>lon: 288</li><li>time: 1980</li></ul></li><li>Coordinates: (8)<ul><li>bnds(bnds)float641.0 2.0long_name :vertex number<pre>array([1., 2.])</pre></li><li>height()float64...axis :Zcell_methods :time: pointdescription :~2 m standard surface air temperature and surface humidity  heightlong_name :heightpositive :upstandard_name :heightunits :m<pre>[1 values with dtype=float64]</pre></li><li>lat(lat)float64-89.5 -88.5 -87.5 ... 88.5 89.5axis :Ybounds :lat_bndscell_methods :time: pointlong_name :latitudestandard_name :latitudeunits :degrees_north<pre>array([-89.5, -88.5, -87.5, -86.5, -85.5, -84.5, -83.5, -82.5, -81.5, -80.5,\n       -79.5, -78.5, -77.5, -76.5, -75.5, -74.5, -73.5, -72.5, -71.5, -70.5,\n       -69.5, -68.5, -67.5, -66.5, -65.5, -64.5, -63.5, -62.5, -61.5, -60.5,\n       -59.5, -58.5, -57.5, -56.5, -55.5, -54.5, -53.5, -52.5, -51.5, -50.5,\n       -49.5, -48.5, -47.5, -46.5, -45.5, -44.5, -43.5, -42.5, -41.5, -40.5,\n       -39.5, -38.5, -37.5, -36.5, -35.5, -34.5, -33.5, -32.5, -31.5, -30.5,\n       -29.5, -28.5, -27.5, -26.5, -25.5, -24.5, -23.5, -22.5, -21.5, -20.5,\n       -19.5, -18.5, -17.5, -16.5, -15.5, -14.5, -13.5, -12.5, -11.5, -10.5,\n        -9.5,  -8.5,  -7.5,  -6.5,  -5.5,  -4.5,  -3.5,  -2.5,  -1.5,  -0.5,\n         0.5,   1.5,   2.5,   3.5,   4.5,   5.5,   6.5,   7.5,   8.5,   9.5,\n        10.5,  11.5,  12.5,  13.5,  14.5,  15.5,  16.5,  17.5,  18.5,  19.5,\n        20.5,  21.5,  22.5,  23.5,  24.5,  25.5,  26.5,  27.5,  28.5,  29.5,\n        30.5,  31.5,  32.5,  33.5,  34.5,  35.5,  36.5,  37.5,  38.5,  39.5,\n        40.5,  41.5,  42.5,  43.5,  44.5,  45.5,  46.5,  47.5,  48.5,  49.5,\n        50.5,  51.5,  52.5,  53.5,  54.5,  55.5,  56.5,  57.5,  58.5,  59.5,\n        60.5,  61.5,  62.5,  63.5,  64.5,  65.5,  66.5,  67.5,  68.5,  69.5,\n        70.5,  71.5,  72.5,  73.5,  74.5,  75.5,  76.5,  77.5,  78.5,  79.5,\n        80.5,  81.5,  82.5,  83.5,  84.5,  85.5,  86.5,  87.5,  88.5,  89.5])</pre></li><li>lat_bnds(lat, bnds)float64dask.array&lt;chunksize=(180, 2), meta=np.ndarray&gt;axis :Ylong_name :latitude boundsunits :degrees_north  Array   Chunk   Bytes   2.81 kiB   2.81 kiB   Shape   (180, 2)   (180, 2)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 180 </li><li>lon(lon)float640.625 1.875 3.125 ... 358.1 359.4axis :Xbounds :lon_bndscell_methods :time: pointlong_name :longitudestandard_name :longitudeunits :degrees_east<pre>array([  0.625,   1.875,   3.125, ..., 356.875, 358.125, 359.375])</pre></li><li>lon_bnds(lon, bnds)float64dask.array&lt;chunksize=(288, 2), meta=np.ndarray&gt;axis :Xlong_name :longitude boundsunits :degrees_east  Array   Chunk   Bytes   4.50 kiB   4.50 kiB   Shape   (288, 2)   (288, 2)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 288 </li><li>time(time)object1850-01-16 12:00:00 ... 2014-12-...axis :Tbounds :time_bndscalendar_type :noleapdescription :Temporal meanlong_name :timestandard_name :time<pre>array([cftime.DatetimeNoLeap(1850, 1, 16, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(1850, 2, 15, 0, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(1850, 3, 16, 12, 0, 0, 0, has_year_zero=True),\n       ...,\n       cftime.DatetimeNoLeap(2014, 10, 16, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(2014, 11, 16, 0, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(2014, 12, 16, 12, 0, 0, 0, has_year_zero=True)],\n      dtype=object)</pre></li><li>time_bnds(time, bnds)objectdask.array&lt;chunksize=(1980, 2), meta=np.ndarray&gt;long_name :time axis boundaries  Array   Chunk   Bytes   30.94 kiB   30.94 kiB   Shape   (1980, 2)   (1980, 2)   Dask graph   1 chunks in 2 graph layers   Data type   object numpy.ndarray  2 1980 </li></ul></li><li>Data variables: (1)<ul><li>tas(time, lat, lon)float32dask.array&lt;chunksize=(600, 180, 288), meta=np.ndarray&gt;cell_measures :area: areacellacell_methods :area: time: meaninterp_method :conserve_order2long_name :Near-Surface Air Temperatureoriginal_name :tasstandard_name :air_temperatureunits :K  Array   Chunk   Bytes   391.55 MiB   118.65 MiB   Shape   (1980, 180, 288)   (600, 180, 288)   Dask graph   4 chunks in 2 graph layers   Data type   float32 numpy.ndarray  288 180 1980 </li></ul></li><li>Indexes: (4)<ul><li>bndsPandasIndex<pre>PandasIndex(Index([1.0, 2.0], dtype='float64', name='bnds'))</pre></li><li>latPandasIndex<pre>PandasIndex(Index([-89.5, -88.5, -87.5, -86.5, -85.5, -84.5, -83.5, -82.5, -81.5, -80.5,\n       ...\n        80.5,  81.5,  82.5,  83.5,  84.5,  85.5,  86.5,  87.5,  88.5,  89.5],\n      dtype='float64', name='lat', length=180))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([             0.625, 1.8749999999999998,              3.125,\n                    4.375,              5.625,              6.875,\n                    8.125,              9.375,             10.625,\n                   11.875,\n       ...\n                  348.125,            349.375,            350.625,\n                  351.875,            353.125,            354.375,\n                  355.625,            356.875,            358.125,\n                  359.375],\n      dtype='float64', name='lon', length=288))</pre></li><li>timePandasIndex<pre>PandasIndex(CFTimeIndex([1850-01-16 12:00:00, 1850-02-15 00:00:00, 1850-03-16 12:00:00,\n             1850-04-16 00:00:00, 1850-05-16 12:00:00, 1850-06-16 00:00:00,\n             1850-07-16 12:00:00, 1850-08-16 12:00:00, 1850-09-16 00:00:00,\n             1850-10-16 12:00:00,\n             ...\n             2014-03-16 12:00:00, 2014-04-16 00:00:00, 2014-05-16 12:00:00,\n             2014-06-16 00:00:00, 2014-07-16 12:00:00, 2014-08-16 12:00:00,\n             2014-09-16 00:00:00, 2014-10-16 12:00:00, 2014-11-16 00:00:00,\n             2014-12-16 12:00:00],\n            dtype='object', length=1980, calendar='noleap', freq='None'))</pre></li></ul></li><li>Attributes: (49)Conventions :CF-1.7 CMIP-6.0 UGRID-1.0activity_id :CMIPbranch_method :standardbranch_time_in_child :0.0branch_time_in_parent :36500.0comment :&lt;null ref&gt;contact :gfdl.climate.model.info@noaa.govcreation_date :2019-07-26T20:13:55Zdata_specs_version :01.00.27experiment :all-forcing simulation of the recent pastexperiment_id :historicalexternal_variables :areacellaforcing_index :1frequency :monfurther_info_url :https://furtherinfo.es-doc.org/CMIP6.NOAA-GFDL.GFDL-ESM4.historical.none.r1i1p1f1grid :atmos data regridded from Cubed-sphere (c96) to 180,288; interpolation method: conserve_order2grid_label :gr1history :File was processed by fremetar (GFDL analog of CMOR). TripleID: [exper_id_MFLg3OOf97,realiz_id_6UiFuoEKMa,run_id_eWiWnwFCM2]initialization_index :1institution :National Oceanic and Atmospheric Administration, Geophysical Fluid Dynamics Laboratory, Princeton, NJ 08540, USAinstitution_id :NOAA-GFDLlicense :CMIP6 model data produced by NOAA-GFDL is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License (https://creativecommons.org/licenses/). Consult https://pcmdi.llnl.gov/CMIP6/TermsOfUse for terms of use governing CMIP6 output, including citation requirements and proper acknowledgment. Further information about this data, including some limitations, can be found via the further_info_url (recorded as a global attribute in this file). The data producers and data providers make no warranty, either express or implied, including, but not limited to, warranties of merchantability and fitness for a particular purpose. All liabilities arising from the supply of the information (including any liability arising in negligence) are excluded to the fullest extent permitted by law.mip_era :CMIP6nominal_resolution :100 kmparent_activity_id :CMIPparent_experiment_id :piControlparent_mip_era :CMIP6parent_source_id :GFDL-ESM4parent_time_units :days since 0001-1-1parent_variant_label :r1i1p1f1physics_index :1product :model-outputrealization_index :1realm :atmosreferences :see further_info_url attributesource :GFDL-ESM4 (2018): atmos: GFDL-AM4.1 (Cubed-sphere (c96) - 1 degree nominal horizontal resolution; 360 x 180 longitude/latitude; 49 levels; top level 1 Pa) ocean: GFDL-OM4p5 (GFDL-MOM6, tripolar - nominal 0.5 deg; 720 x 576 longitude/latitude; 75 levels; top grid cell 0-2 m) seaIce: GFDL-SIM4p5 (GFDL-SIS2.0, tripolar - nominal 0.5 deg; 720 x 576 longitude/latitude; 5 layers; 5 thickness categories) land: GFDL-LM4.1 aerosol: interactive atmosChem: GFDL-ATMCHEM4.1 (full atmospheric chemistry) ocnBgchem: GFDL-COBALTv2 landIce: GFDL-LM4.1 (GFDL ID: 2019_0334)source_id :GFDL-ESM4source_type :AOGCM AER CHEM BGCsub_experiment :nonesub_experiment_id :nonetable_id :Amontitle :NOAA GFDL GFDL-ESM4 model output prepared for CMIP6 all-forcing simulation of the recent pasttracking_id :hdl:21.14100/75e5c5a7-d7c4-4860-beb1-db454f25f13a hdl:21.14100/a54fe2f4-0a68-4420-8702-36750bde9b05variable_id :tasvariant_info :N/Avariant_label :r1i1p1f1status :2019-09-10;created;by nhn2@columbia.edunetcdf_tracking_ids :hdl:21.14100/75e5c5a7-d7c4-4860-beb1-db454f25f13a hdl:21.14100/a54fe2f4-0a68-4420-8702-36750bde9b05version_id :v20190726</li></ul> <p>Plot a map from a specific date.</p> In\u00a0[20]: Copied! <pre>ds.tas.sel(time='1950-01').plot()\n</pre> ds.tas.sel(time='1950-01').plot() Out[20]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7fb548d59790&gt;</pre> <p>Create a timeseries of global-average surface air temperature. For this we need the area weighting factor for each gridpoint.</p> In\u00a0[22]: Copied! <pre>df_area = df.query(\"variable_id == 'areacella' &amp; source_id == 'GFDL-ESM4'\")\nds_area = xr.open_zarr(fsspec.get_mapper(df_area.zstore.values[0]), consolidated=True)\nds_area\n</pre> df_area = df.query(\"variable_id == 'areacella' &amp; source_id == 'GFDL-ESM4'\") ds_area = xr.open_zarr(fsspec.get_mapper(df_area.zstore.values[0]), consolidated=True) ds_area Out[22]: <pre>&lt;xarray.Dataset&gt;\nDimensions:    (lat: 180, lon: 288, bnds: 2)\nCoordinates:\n  * bnds       (bnds) float64 1.0 2.0\n  * lat        (lat) float64 -89.5 -88.5 -87.5 -86.5 ... 86.5 87.5 88.5 89.5\n    lat_bnds   (lat, bnds) float64 dask.array&lt;chunksize=(180, 2), meta=np.ndarray&gt;\n  * lon        (lon) float64 0.625 1.875 3.125 4.375 ... 355.6 356.9 358.1 359.4\n    lon_bnds   (lon, bnds) float64 dask.array&lt;chunksize=(288, 2), meta=np.ndarray&gt;\nData variables:\n    areacella  (lat, lon) float32 dask.array&lt;chunksize=(180, 288), meta=np.ndarray&gt;\nAttributes: (12/48)\n    Conventions:            CF-1.7 CMIP-6.0 UGRID-1.0\n    activity_id:            CMIP\n    branch_method:          standard\n    branch_time_in_child:   0.0\n    branch_time_in_parent:  36500.0\n    comment:                &lt;null ref&gt;\n    ...                     ...\n    tracking_id:            hdl:21.14100/deeb5d3d-bbcc-464c-bb7d-ade2ac4d7ed0\n    variable_id:            areacella\n    variant_info:           N/A\n    variant_label:          r1i1p1f1\n    netcdf_tracking_ids:    hdl:21.14100/deeb5d3d-bbcc-464c-bb7d-ade2ac4d7ed0\n    version_id:             v20180701</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 180</li><li>lon: 288</li><li>bnds: 2</li></ul></li><li>Coordinates: (5)<ul><li>bnds(bnds)float641.0 2.0long_name :vertex number<pre>array([1., 2.])</pre></li><li>lat(lat)float64-89.5 -88.5 -87.5 ... 88.5 89.5axis :Ybounds :lat_bndscell_methods :time: pointlong_name :latitudestandard_name :latitudeunits :degrees_north<pre>array([-89.5, -88.5, -87.5, -86.5, -85.5, -84.5, -83.5, -82.5, -81.5, -80.5,\n       -79.5, -78.5, -77.5, -76.5, -75.5, -74.5, -73.5, -72.5, -71.5, -70.5,\n       -69.5, -68.5, -67.5, -66.5, -65.5, -64.5, -63.5, -62.5, -61.5, -60.5,\n       -59.5, -58.5, -57.5, -56.5, -55.5, -54.5, -53.5, -52.5, -51.5, -50.5,\n       -49.5, -48.5, -47.5, -46.5, -45.5, -44.5, -43.5, -42.5, -41.5, -40.5,\n       -39.5, -38.5, -37.5, -36.5, -35.5, -34.5, -33.5, -32.5, -31.5, -30.5,\n       -29.5, -28.5, -27.5, -26.5, -25.5, -24.5, -23.5, -22.5, -21.5, -20.5,\n       -19.5, -18.5, -17.5, -16.5, -15.5, -14.5, -13.5, -12.5, -11.5, -10.5,\n        -9.5,  -8.5,  -7.5,  -6.5,  -5.5,  -4.5,  -3.5,  -2.5,  -1.5,  -0.5,\n         0.5,   1.5,   2.5,   3.5,   4.5,   5.5,   6.5,   7.5,   8.5,   9.5,\n        10.5,  11.5,  12.5,  13.5,  14.5,  15.5,  16.5,  17.5,  18.5,  19.5,\n        20.5,  21.5,  22.5,  23.5,  24.5,  25.5,  26.5,  27.5,  28.5,  29.5,\n        30.5,  31.5,  32.5,  33.5,  34.5,  35.5,  36.5,  37.5,  38.5,  39.5,\n        40.5,  41.5,  42.5,  43.5,  44.5,  45.5,  46.5,  47.5,  48.5,  49.5,\n        50.5,  51.5,  52.5,  53.5,  54.5,  55.5,  56.5,  57.5,  58.5,  59.5,\n        60.5,  61.5,  62.5,  63.5,  64.5,  65.5,  66.5,  67.5,  68.5,  69.5,\n        70.5,  71.5,  72.5,  73.5,  74.5,  75.5,  76.5,  77.5,  78.5,  79.5,\n        80.5,  81.5,  82.5,  83.5,  84.5,  85.5,  86.5,  87.5,  88.5,  89.5])</pre></li><li>lat_bnds(lat, bnds)float64dask.array&lt;chunksize=(180, 2), meta=np.ndarray&gt;axis :Ylong_name :latitude boundsunits :degrees_north  Array   Chunk   Bytes   2.81 kiB   2.81 kiB   Shape   (180, 2)   (180, 2)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 180 </li><li>lon(lon)float640.625 1.875 3.125 ... 358.1 359.4axis :Xbounds :lon_bndscell_methods :time: pointlong_name :longitudestandard_name :longitudeunits :degrees_east<pre>array([  0.625,   1.875,   3.125, ..., 356.875, 358.125, 359.375])</pre></li><li>lon_bnds(lon, bnds)float64dask.array&lt;chunksize=(288, 2), meta=np.ndarray&gt;axis :Xlong_name :longitude boundsunits :degrees_east  Array   Chunk   Bytes   4.50 kiB   4.50 kiB   Shape   (288, 2)   (288, 2)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 288 </li></ul></li><li>Data variables: (1)<ul><li>areacella(lat, lon)float32dask.array&lt;chunksize=(180, 288), meta=np.ndarray&gt;cell_methods :area: suminterp_method :conserve_order1long_name :Grid-Cell Area for Atmospheric Variablesoriginal_name :areacellastandard_name :cell_areaunits :m2  Array   Chunk   Bytes   202.50 kiB   202.50 kiB   Shape   (180, 288)   (180, 288)   Dask graph   1 chunks in 2 graph layers   Data type   float32 numpy.ndarray  288 180 </li></ul></li><li>Indexes: (3)<ul><li>bndsPandasIndex<pre>PandasIndex(Index([1.0, 2.0], dtype='float64', name='bnds'))</pre></li><li>latPandasIndex<pre>PandasIndex(Index([-89.5, -88.5, -87.5, -86.5, -85.5, -84.5, -83.5, -82.5, -81.5, -80.5,\n       ...\n        80.5,  81.5,  82.5,  83.5,  84.5,  85.5,  86.5,  87.5,  88.5,  89.5],\n      dtype='float64', name='lat', length=180))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([  0.625,   1.875,   3.125,   4.375,   5.625,   6.875,   8.125,   9.375,\n        10.625,  11.875,\n       ...\n       348.125, 349.375, 350.625, 351.875, 353.125, 354.375, 355.625, 356.875,\n       358.125, 359.375],\n      dtype='float64', name='lon', length=288))</pre></li></ul></li><li>Attributes: (48)Conventions :CF-1.7 CMIP-6.0 UGRID-1.0activity_id :CMIPbranch_method :standardbranch_time_in_child :0.0branch_time_in_parent :36500.0comment :&lt;null ref&gt;contact :gfdl.climate.model.info@noaa.govcreation_date :2019-05-21T04:16:30Zdata_specs_version :01.00.27experiment :1 percent per year increase in CO2experiment_id :1pctCO2forcing_index :1frequency :fxfurther_info_url :https://furtherinfo.es-doc.org/CMIP6.NOAA-GFDL.GFDL-ESM4.1pctCO2.none.r1i1p1f1grid :atmos data regridded from Cubed-sphere (c96) to 180,288; interpolation method: conserve_order1grid_label :gr1history :File was processed by fremetar (GFDL analog of CMOR). TripleID: [exper_id_B87UgqxI9B,realiz_id_fN7PwOvCuS,run_id_w2CeZpk80Y]initialization_index :1institution :National Oceanic and Atmospheric Administration, Geophysical Fluid Dynamics Laboratory, Princeton, NJ 08540, USAinstitution_id :NOAA-GFDLlicense :CMIP6 model data produced by NOAA-GFDL is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License (https://creativecommons.org/licenses/). Consult https://pcmdi.llnl.gov/CMIP6/TermsOfUse for terms of use governing CMIP6 output, including citation requirements and proper acknowledgment. Further information about this data, including some limitations, can be found via the further_info_url (recorded as a global attribute in this file). The data producers and data providers make no warranty, either express or implied, including, but not limited to, warranties of merchantability and fitness for a particular purpose. All liabilities arising from the supply of the information (including any liability arising in negligence) are excluded to the fullest extent permitted by law.mip_era :CMIP6nominal_resolution :100 kmparent_activity_id :CMIPparent_experiment_id :piControlparent_mip_era :CMIP6parent_source_id :GFDL-ESM4parent_time_units :days since 0001-1-1parent_variant_label :r1i1p1f1physics_index :1product :model-outputrealization_index :1realm :atmos landreferences :see further_info_url attributesource :GFDL-ESM4 (2018): atmos: GFDL-AM4.1 (Cubed-sphere (c96) - 1 degree nominal horizontal resolution; 360 x 180 longitude/latitude; 49 levels; top level 1 Pa) ocean: GFDL-OM4p5 (GFDL-MOM6, tripolar - nominal 0.5 deg; 720 x 576 longitude/latitude; 75 levels; top grid cell 0-2 m) seaIce: GFDL-SIM4p5 (GFDL-SIS2.0, tripolar - nominal 0.5 deg; 720 x 576 longitude/latitude; 5 layers; 5 thickness categories) land: GFDL-LM4.1 aerosol: interactive atmosChem: GFDL-ATMCHEM4.1 (full atmospheric chemistry) ocnBgchem: GFDL-COBALTv2 landIce: GFDL-LM4.1 (GFDL ID: 2019_0277)source_id :GFDL-ESM4source_type :AOGCM AER CHEM BGCstatus :2020-09-09;created; by gcs.cmip6.ldeo@gmail.comsub_experiment :nonesub_experiment_id :nonetable_id :fxtitle :NOAA GFDL GFDL-ESM4 model output prepared for CMIP6 1 percent per year increase in CO2tracking_id :hdl:21.14100/deeb5d3d-bbcc-464c-bb7d-ade2ac4d7ed0variable_id :areacellavariant_info :N/Avariant_label :r1i1p1f1netcdf_tracking_ids :hdl:21.14100/deeb5d3d-bbcc-464c-bb7d-ade2ac4d7ed0version_id :v20180701</li></ul> In\u00a0[23]: Copied! <pre>total_area = ds_area.areacella.sum(dim=['lon', 'lat'])\nta_timeseries = (ds.tas * ds_area.areacella).sum(dim=['lon', 'lat']) / total_area\nta_timeseries\n</pre> total_area = ds_area.areacella.sum(dim=['lon', 'lat']) ta_timeseries = (ds.tas * ds_area.areacella).sum(dim=['lon', 'lat']) / total_area ta_timeseries Out[23]: <pre>&lt;xarray.DataArray (time: 1980)&gt;\ndask.array&lt;truediv, shape=(1980,), dtype=float32, chunksize=(600,), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * time     (time) object 1850-01-16 12:00:00 ... 2014-12-16 12:00:00\n    height   float64 ...</pre>xarray.DataArray<ul><li>time: 1980</li></ul><ul><li>dask.array&lt;chunksize=(600,), meta=np.ndarray&gt;  Array   Chunk   Bytes   7.73 kiB   2.34 kiB   Shape   (1980,)   (600,)   Dask graph   4 chunks in 22 graph layers   Data type   float32 numpy.ndarray  1980 1 </li><li>Coordinates: (2)<ul><li>time(time)object1850-01-16 12:00:00 ... 2014-12-...axis :Tbounds :time_bndscalendar_type :noleapdescription :Temporal meanlong_name :timestandard_name :time<pre>array([cftime.DatetimeNoLeap(1850, 1, 16, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(1850, 2, 15, 0, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(1850, 3, 16, 12, 0, 0, 0, has_year_zero=True),\n       ...,\n       cftime.DatetimeNoLeap(2014, 10, 16, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(2014, 11, 16, 0, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(2014, 12, 16, 12, 0, 0, 0, has_year_zero=True)],\n      dtype=object)</pre></li><li>height()float64...axis :Zcell_methods :time: pointdescription :~2 m standard surface air temperature and surface humidity  heightlong_name :heightpositive :upstandard_name :heightunits :m<pre>[1 values with dtype=float64]</pre></li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(CFTimeIndex([1850-01-16 12:00:00, 1850-02-15 00:00:00, 1850-03-16 12:00:00,\n             1850-04-16 00:00:00, 1850-05-16 12:00:00, 1850-06-16 00:00:00,\n             1850-07-16 12:00:00, 1850-08-16 12:00:00, 1850-09-16 00:00:00,\n             1850-10-16 12:00:00,\n             ...\n             2014-03-16 12:00:00, 2014-04-16 00:00:00, 2014-05-16 12:00:00,\n             2014-06-16 00:00:00, 2014-07-16 12:00:00, 2014-08-16 12:00:00,\n             2014-09-16 00:00:00, 2014-10-16 12:00:00, 2014-11-16 00:00:00,\n             2014-12-16 12:00:00],\n            dtype='object', length=1980, calendar='noleap', freq='None'))</pre></li></ul></li><li>Attributes: (0)</li></ul> <p>By default the data are loaded lazily, as Dask arrays. Here we trigger computation explicitly.</p> In\u00a0[24]: Copied! <pre>%time ta_timeseries.load()\n</pre> %time ta_timeseries.load() <pre>CPU times: user 6.3 s, sys: 1.56 s, total: 7.87 s\nWall time: 40.4 s\n</pre> Out[24]: <pre>&lt;xarray.DataArray (time: 1980)&gt;\narray([224.00783, 224.21144, 224.76314, ..., 226.22311, 225.26149,\n       224.80829], dtype=float32)\nCoordinates:\n  * time     (time) object 1850-01-16 12:00:00 ... 2014-12-16 12:00:00\n    height   float64 2.0</pre>xarray.DataArray<ul><li>time: 1980</li></ul><ul><li>224.0 224.2 224.8 225.6 226.3 226.9 ... 227.7 227.1 226.2 225.3 224.8<pre>array([224.00783, 224.21144, 224.76314, ..., 226.22311, 225.26149,\n       224.80829], dtype=float32)</pre></li><li>Coordinates: (2)<ul><li>time(time)object1850-01-16 12:00:00 ... 2014-12-...axis :Tbounds :time_bndscalendar_type :noleapdescription :Temporal meanlong_name :timestandard_name :time<pre>array([cftime.DatetimeNoLeap(1850, 1, 16, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(1850, 2, 15, 0, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(1850, 3, 16, 12, 0, 0, 0, has_year_zero=True),\n       ...,\n       cftime.DatetimeNoLeap(2014, 10, 16, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(2014, 11, 16, 0, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(2014, 12, 16, 12, 0, 0, 0, has_year_zero=True)],\n      dtype=object)</pre></li><li>height()float642.0axis :Zcell_methods :time: pointdescription :~2 m standard surface air temperature and surface humidity  heightlong_name :heightpositive :upstandard_name :heightunits :m<pre>array(2.)</pre></li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(CFTimeIndex([1850-01-16 12:00:00, 1850-02-15 00:00:00, 1850-03-16 12:00:00,\n             1850-04-16 00:00:00, 1850-05-16 12:00:00, 1850-06-16 00:00:00,\n             1850-07-16 12:00:00, 1850-08-16 12:00:00, 1850-09-16 00:00:00,\n             1850-10-16 12:00:00,\n             ...\n             2014-03-16 12:00:00, 2014-04-16 00:00:00, 2014-05-16 12:00:00,\n             2014-06-16 00:00:00, 2014-07-16 12:00:00, 2014-08-16 12:00:00,\n             2014-09-16 00:00:00, 2014-10-16 12:00:00, 2014-11-16 00:00:00,\n             2014-12-16 12:00:00],\n            dtype='object', length=1980, calendar='noleap', freq='None'))</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[25]: Copied! <pre>ta_timeseries.plot(label='monthly')\nta_timeseries.rolling(time=12).mean().plot(label='12 month rolling mean')\nplt.legend()\nplt.title('Global Mean Surface Air Temperature')\n</pre> ta_timeseries.plot(label='monthly') ta_timeseries.rolling(time=12).mean().plot(label='12 month rolling mean') plt.legend() plt.title('Global Mean Surface Air Temperature') Out[25]: <pre>Text(0.5, 1.0, 'Global Mean Surface Air Temperature')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Lecture_13_Cloud_computing/#lecture-13-cloud-computing","title":"Lecture 13: Cloud computing\u00b6","text":""},{"location":"Lecture_13_Cloud_computing/#browse-catalog","title":"Browse Catalog\u00b6","text":"<p>The data catatalog is stored as a CSV file. Here we read it with Pandas.</p>"},{"location":"Lecture_13_Cloud_computing/#load-data","title":"Load Data\u00b6","text":"<p>Now we will load a single store using fsspec, zarr, and xarray.</p>"},{"location":"Lecture_2_Core_Python/","title":"Lecture 2 Core Python Language","text":"In\u00a0[1]: Copied! <pre># comments are anything that comes after the \"#\" symbol\na = 1       # assign 1 to variable a\nb = \"hello\" # assign \"hello\" to variable b\n</pre> # comments are anything that comes after the \"#\" symbol a = 1       # assign 1 to variable a b = \"hello\" # assign \"hello\" to variable b <p>The following identifiers are used as reserved words, or keywords of the language, and cannot be used as ordinary identifiers. They must be spelled exactly as written here:</p> <pre><code>False      class      finally    is         return\nNone       continue   for        lambda     try\nTrue       def        from       nonlocal   while\nand        del        global     not        with\nas         elif       if         or         yield\nassert     else       import     pass\nbreak      except     in         raise</code></pre> <p>Additionally, the following a built in functions which are always available in your namespace once you open a python interpreter</p> <pre><code>abs() dict() help() min() setattr() all() dir() hex() next() slice() any()\ndivmod() id() object() sorted() ascii() enumerate() input() oct() staticmethod()\nbin() eval() int() open() str() bool() exec() isinstance() ord() sum() bytearray()\nfilter() issubclass() pow() super() bytes() float() iter() print() tuple()\ncallable() format() len() property() type() chr() frozenset() list() range()\nvars() classmethod() getattr() locals() repr() zip() compile() globals() map()\nreversed() __import__() complex() hasattr() max() round() delattr() hash()\nmemoryview() set()</code></pre> In\u00a0[2]: Copied! <pre># how to we see our variables?\nprint(a)\nprint(b)\nprint(a,b)\n</pre> # how to we see our variables? print(a) print(b) print(a,b) <pre>1\nhello\n1 hello\n</pre> <p>All variables are objects. Every object has a type (class). To find out what type your variables are</p> In\u00a0[5]: Copied! <pre># as a shortcut, iPython notebooks will automatically print whatever is on the last line\ntype(b)\n</pre> # as a shortcut, iPython notebooks will automatically print whatever is on the last line type(b) Out[5]: <pre>str</pre> In\u00a0[6]: Copied! <pre>type(a) is int\n</pre> type(a) is int Out[6]: <pre>True</pre> <p>Different objects attributes and methods, which can be accessed via the syntax <code>variable.method</code></p> <p>IPython will autocomplete if you press <code>&lt;tab&gt;</code> to show you the methods available.</p> In\u00a0[7]: Copied! <pre># this returns the method itself\nb.capitalize\n</pre> # this returns the method itself b.capitalize Out[7]: <pre>&lt;function str.capitalize()&gt;</pre> In\u00a0[8]: Copied! <pre># this calls the method\nb.capitalize()\n# there are lots of other methods\n</pre> # this calls the method b.capitalize() # there are lots of other methods Out[8]: <pre>'Hello'</pre> In\u00a0[9]: Copied! <pre># binary operations act differently on different types of objects\nc = 'World'\nprint(b + c)\nprint(a + 2)\nprint(a + b)\n</pre> # binary operations act differently on different types of objects c = 'World' print(b + c) print(a + 2) print(a + b) <pre>helloWorld\n3\n</pre> <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[9], line 5\n      3 print(b + c)\n      4 print(a + 2)\n----&gt; 5 print(a + b)\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'</pre> In\u00a0[10]: Copied! <pre># addition / subtraction\n1+1-5\n</pre> # addition / subtraction 1+1-5 Out[10]: <pre>-3</pre> In\u00a0[11]: Copied! <pre># multiplication\n5 * 10\n</pre> # multiplication 5 * 10 Out[11]: <pre>50</pre> In\u00a0[12]: Copied! <pre># division\n1/2\n</pre> # division 1/2 Out[12]: <pre>0.5</pre> In\u00a0[13]: Copied! <pre># that was automatically converted to a float\ntype(1/2)\n</pre> # that was automatically converted to a float type(1/2) Out[13]: <pre>float</pre> In\u00a0[13]: Copied! <pre># exponentiation\n2**4\n</pre> # exponentiation 2**4 Out[13]: <pre>16</pre> In\u00a0[14]: Copied! <pre># rounding\nround(9/10)\n</pre> # rounding round(9/10) Out[14]: <pre>1</pre> In\u00a0[15]: Copied! <pre># built in complex number support\n(1+2j) / (3-4j)\n</pre> # built in complex number support (1+2j) / (3-4j) Out[15]: <pre>(-0.2+0.4j)</pre> In\u00a0[15]: Copied! <pre># logic\nTrue and True\n</pre> # logic True and True Out[15]: <pre>True</pre> In\u00a0[16]: Copied! <pre>True and False\n</pre> True and False Out[16]: <pre>False</pre> In\u00a0[17]: Copied! <pre>True or True\n</pre> True or True Out[17]: <pre>True</pre> In\u00a0[18]: Copied! <pre>(not True) or (not False)\n</pre> (not True) or (not False) Out[18]: <pre>True</pre> In\u00a0[19]: Copied! <pre>x = 100\nif x &gt; 0:\n    print('Positive Number')\nelif x &lt; 0:\n    print('Negative Number')\nelse:\n    print ('Zero!')\n</pre> x = 100 if x &gt; 0:     print('Positive Number') elif x &lt; 0:     print('Negative Number') else:     print ('Zero!') <pre>Positive Number\n</pre> In\u00a0[20]: Copied! <pre># indentation is MANDATORY\n# blocks are closed by indentation level\nif x &gt; 0:\n    print('Positive Number')\n    if x &gt;= 100:\n        print('Huge number!')\n</pre> # indentation is MANDATORY # blocks are closed by indentation level if x &gt; 0:     print('Positive Number')     if x &gt;= 100:         print('Huge number!') <pre>Positive Number\nHuge number!\n</pre> In\u00a0[21]: Copied! <pre># make a loop \ncount = 0\nwhile count &lt; 10:\n    # bad way\n    # count = count + 1\n    # better way\n    count += 1\nprint(count)\n</pre> # make a loop  count = 0 while count &lt; 10:     # bad way     # count = count + 1     # better way     count += 1 print(count) <pre>10\n</pre> In\u00a0[22]: Copied! <pre># use range\nfor i in range(5):\n    print(i)\n</pre> # use range for i in range(5):     print(i) <pre>0\n1\n2\n3\n4\n</pre> <p>Important point: in python, we always count from 0!</p> In\u00a0[23]: Copied! <pre># what is range?\ntype(range)\n</pre> # what is range? type(range) Out[23]: <pre>type</pre> In\u00a0[24]: Copied! <pre>range?\n</pre> range? <pre>Init signature: range(self, /, *args, **kwargs)\nDocstring:     \nrange(stop) -&gt; range object\nrange(start, stop[, step]) -&gt; range object\n\nReturn an object that produces a sequence of integers from start (inclusive)\nto stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\nstart defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\nThese are exactly the valid indices for a list of 4 elements.\nWhen step is given, it specifies the increment (or decrement).\nType:           type\nSubclasses:     </pre> In\u00a0[25]: Copied! <pre># iterate over a list we make up\nfor pet in ['dog', 'cat', 'fish']:\n    print(pet, len(pet))\n</pre> # iterate over a list we make up for pet in ['dog', 'cat', 'fish']:     print(pet, len(pet)) <pre>dog 3\ncat 3\nfish 4\n</pre> <p>What is the thing in brackets? A list! Lists are one of the core python data structures.</p> In\u00a0[26]: Copied! <pre>l = ['dog', 'cat', 'fish']\ntype(l)\n</pre> l = ['dog', 'cat', 'fish'] type(l) Out[26]: <pre>list</pre> In\u00a0[27]: Copied! <pre># list have lots of methods\nl.sort()\nl\n</pre> # list have lots of methods l.sort() l Out[27]: <pre>['cat', 'dog', 'fish']</pre> In\u00a0[28]: Copied! <pre># we can convert a range to a list\nr = list(range(5))\nr\n</pre> # we can convert a range to a list r = list(range(5)) r Out[28]: <pre>[0, 1, 2, 3, 4]</pre> In\u00a0[29]: Copied! <pre>while r:\n    p = r.pop()\n    print('p:', p)\n    print('r:', r)\n</pre> while r:     p = r.pop()     print('p:', p)     print('r:', r) <pre>p: 4\nr: [0, 1, 2, 3]\np: 3\nr: [0, 1, 2]\np: 2\nr: [0, 1]\np: 1\nr: [0]\np: 0\nr: []\n</pre> <p>There are many different ways to interact with lists. Exploring them is part of the fun of python.</p> <p>list.append(x) Add an item to the end of the list. Equivalent to a[len(a):] = [x].</p> <p>list.extend(L) Extend the list by appending all the items in the given list. Equivalent to a[len(a):] = L.</p> <p>list.insert(i, x) Insert an item at a given position. The first argument is the index of the element before which to insert, so a.insert(0, x) inserts at the front of the list, and a.insert(len(a), x) is equivalent to a.append(x).</p> <p>list.remove(x) Remove the first item from the list whose value is x. It is an error if there is no such item.</p> <p>list.pop([i]) Remove the item at the given position in the list, and return it. If no index is specified, a.pop() removes and returns the last item in the list. (The square brackets around the i in the method signature denote that the parameter is optional, not that you should type square brackets at that position. You will see this notation frequently in the Python Library Reference.)</p> <p>list.clear() Remove all items from the list. Equivalent to del a[:].</p> <p>list.index(x) Return the index in the list of the first item whose value is x. It is an error if there is no such item.</p> <p>list.count(x) Return the number of times x appears in the list.</p> <p>list.sort() Sort the items of the list in place.</p> <p>list.reverse() Reverse the elements of the list in place.</p> <p>list.copy() Return a shallow copy of the list. Equivalent to a[:].</p> <p>Don't assume you know how list operations work!</p> In\u00a0[30]: Copied! <pre># \"add\" two lists\nx = list(range(5))\ny = list(range(10,15))\nz = x + y\nz\n</pre> # \"add\" two lists x = list(range(5)) y = list(range(10,15)) z = x + y z Out[30]: <pre>[0, 1, 2, 3, 4, 10, 11, 12, 13, 14]</pre> In\u00a0[31]: Copied! <pre># access items from a list\nprint('first', z[0])\nprint('last', z[-1])\nprint('first 3', z[:3])\nprint('last 3', z[-3:])\nprint('middle, skipping every other item', z[5:10:2])\n</pre> # access items from a list print('first', z[0]) print('last', z[-1]) print('first 3', z[:3]) print('last 3', z[-3:]) print('middle, skipping every other item', z[5:10:2]) <pre>first 0\nlast 14\nfirst 3 [0, 1, 2]\nlast 3 [12, 13, 14]\nmiddle, skipping every other item [10, 12, 14]\n</pre> <p>MEMORIZE THIS SYNTAX! It is central to so much of python and often proves confusing for users coming from other languages.</p> <p>In terms of set notation, python indexing is left inclusive, right exclusive. If you remember this, you will never go wrong.</p> In\u00a0[32]: Copied! <pre># that means we get an error from the following\nN = len(z)\nz[N]\n</pre> # that means we get an error from the following N = len(z) z[N] <pre>\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[32], line 3\n      1 # that means we get an error from the following\n      2 N = len(z)\n----&gt; 3 z[N]\n\nIndexError: list index out of range</pre> In\u00a0[33]: Copied! <pre># this index notation also applies to strings\nname = 'Xiaomeng Jin'\nprint(name[:4])\n</pre> # this index notation also applies to strings name = 'Xiaomeng Jin' print(name[:4]) <pre>Xiao\n</pre> In\u00a0[34]: Copied! <pre>print(name[:-4])\n</pre> print(name[:-4]) <pre>Xiaomeng\n</pre> In\u00a0[36]: Copied! <pre>print(name[-3:])\n</pre> print(name[-3:]) <pre>Jin\n</pre> In\u00a0[37]: Copied! <pre># you can also test for the presence of items in a list\n5 in z\n</pre> # you can also test for the presence of items in a list 5 in z Out[37]: <pre>False</pre> <p>Lists are not meant for math! They don't have a datatype.</p> In\u00a0[38]: Copied! <pre>z[4] = 'fish'\nz\n</pre> z[4] = 'fish' z Out[38]: <pre>[0, 1, 2, 3, 'fish', 10, 11, 12, 13, 14]</pre> <p>Python is full of tricks for iterating and working with lists</p> In\u00a0[42]: Copied! <pre># a cool python trick: list comprehension\nsquares = [n**2 for n in range(5)]\nsquares\n</pre> # a cool python trick: list comprehension squares = [n**2 for n in range(5)] squares Out[42]: <pre>[0, 1, 4, 9, 16]</pre> In\u00a0[39]: Copied! <pre># iterate over two lists together uzing zip\nfor item1, item2 in zip(x,y):\n    print('first:', item1, 'second:', item2)\n</pre> # iterate over two lists together uzing zip for item1, item2 in zip(x,y):     print('first:', item1, 'second:', item2) <pre>first: 0 second: 10\nfirst: 1 second: 11\nfirst: 2 second: 12\nfirst: 3 second: 13\nfirst: 4 second: 14\n</pre> In\u00a0[43]: Copied! <pre># tuples are created with parentheses, or just commas\na = ('Jin', 32, True)\nb = 'Wang', 25, False\ntype(b)\n</pre> # tuples are created with parentheses, or just commas a = ('Jin', 32, True) b = 'Wang', 25, False type(b) Out[43]: <pre>tuple</pre> In\u00a0[44]: Copied! <pre>b\n</pre> b Out[44]: <pre>('Wang', 25, False)</pre> In\u00a0[45]: Copied! <pre># can be indexed like arrays\nprint(a[1]) # not the first element!\n</pre> # can be indexed like arrays print(a[1]) # not the first element! <pre>32\n</pre> In\u00a0[46]: Copied! <pre># and they can be unpacked\nname, age, status = a\n</pre> # and they can be unpacked name, age, status = a In\u00a0[47]: Copied! <pre># different ways to create dictionaries : Curly brackets or dict\nd = {'name': 'Jin', 'age': 32}\ne = dict(name='Wang', age=25)\ne\n</pre> # different ways to create dictionaries : Curly brackets or dict d = {'name': 'Jin', 'age': 32} e = dict(name='Wang', age=25) e Out[47]: <pre>{'name': 'Wang', 'age': 25}</pre> In\u00a0[48]: Copied! <pre># access a value\nd['name']\n</pre> # access a value d['name'] Out[48]: <pre>'Jin'</pre> <p>Square brackets <code>[...]</code> are python for \"get item\" in many different contexts.</p> In\u00a0[49]: Copied! <pre># test for the presence of a key\nprint('age' in d)\nprint('height' in e)\n</pre> # test for the presence of a key print('age' in d) print('height' in e) <pre>True\nFalse\n</pre> In\u00a0[50]: Copied! <pre># try to access a non-existant key\nd['height']\n</pre> # try to access a non-existant key d['height'] <pre>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[50], line 2\n      1 # try to access a non-existant key\n----&gt; 2 d['height']\n\nKeyError: 'height'</pre> In\u00a0[51]: Copied! <pre># add a new key\nd['height'] = (5,3) # a tuple\nd\n</pre> # add a new key d['height'] = (5,3) # a tuple d Out[51]: <pre>{'name': 'Jin', 'age': 32, 'height': (5, 3)}</pre> In\u00a0[52]: Copied! <pre># keys don't have to be strings\nd[99] = 'nighty nine'\nd\n</pre> # keys don't have to be strings d[99] = 'nighty nine' d Out[52]: <pre>{'name': 'Jin', 'age': 32, 'height': (5, 3), 99: 'nighty nine'}</pre> In\u00a0[53]: Copied! <pre># iterate over keys\nfor k in d:\n    print(k, d[k])\n</pre> # iterate over keys for k in d:     print(k, d[k]) <pre>name Jin\nage 32\nheight (5, 3)\n99 nighty nine\n</pre> In\u00a0[54]: Copied! <pre># better way\n### python 2\n### for key, val in d.iteritems()\nfor key, val in d.items():\n    print(key, val)\n</pre> # better way ### python 2 ### for key, val in d.iteritems() for key, val in d.items():     print(key, val) <pre>name Jin\nage 32\nheight (5, 3)\n99 nighty nine\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Lecture_2_Core_Python/#lecture-2-core-python-language","title":"Lecture 2 Core Python Language\u00b6","text":"<p>Mostly copied from the official python tutorial</p>"},{"location":"Lecture_2_Core_Python/#invoking-python","title":"Invoking Python\u00b6","text":"<p>There are three main ways to use python.</p> <ol> <li>By running a python file, e.g. <code>python myscript.py</code></li> <li>Through an interactive console (python interpreter or ipython shell)</li> <li>In an interactive iPython notebook</li> </ol> <p>We will be using the iPython notebook.</p>"},{"location":"Lecture_2_Core_Python/#basic-variables-numbers-and-string","title":"Basic Variables: Numbers and String\u00b6","text":""},{"location":"Lecture_2_Core_Python/#math","title":"Math\u00b6","text":"<p>Basic arithmetic and boolean logic is part of the core python library.</p>"},{"location":"Lecture_2_Core_Python/#conditionals","title":"Conditionals\u00b6","text":"<p>The first step to programming. Plus an intro to python syntax.</p>"},{"location":"Lecture_2_Core_Python/#more-flow-control","title":"More Flow Control\u00b6","text":""},{"location":"Lecture_2_Core_Python/#lists","title":"Lists\u00b6","text":""},{"location":"Lecture_2_Core_Python/#other-data-structures","title":"Other Data Structures\u00b6","text":"<p>We are almost there. We have the building blocks we need to do basic programming. But python has some other data structures we need to learn about.</p>"},{"location":"Lecture_2_Core_Python/#tuples","title":"Tuples\u00b6","text":"<p>Tuples are similar to lists, but they are immutable\u2014they can't be extended or modified. What is the point of this? Generally speaking: to pack together inhomogeneous data. Tuples can then be unpacked and distributed by other parts of your code.</p> <p>Tuples may seem confusing at first, but with time you will come to appreciate them. Tuples are great to use if you want the data in your collection to be read-only, never to change, and always remain the same and constant.</p>"},{"location":"Lecture_2_Core_Python/#dictionaries","title":"Dictionaries\u00b6","text":"<p>This is an extremely useful data structure. It maps keys to values.</p> <p>Dictionaries are unordered!</p> <p>Tuples can be used as dictionary keys (specifically, tuples that contain immutable values like strings, numbers, and other tuples). Lists can never be used as dictionary keys, because lists are mutable.</p>"},{"location":"Lecture_2_install_python_amarel/","title":"Install Conda and Python","text":"<ul> <li>Connect to Amarel Open OnDemand </li> <li>Click Clusters </li> <li>Choose Amarel Cluster Shell Access </li> <li>Enter your password </li> <li>In the terminal, do the following commands (one line each time). If you're using Windows, type 'Ctrl+c' to copy and 'Ctrl+Shift+v' to paste command. </li> </ul> <pre><code>$ module use /projects/community/modulefiles\n$ module load anaconda/2020.07-gc563\n$ cd\n$ source .bashrc\n$ mkdir -p .conda/pkgs/cache .conda/envs \n</code></pre> <ul> <li>Test if conda is successfully installed: </li> </ul> <pre><code>$ which conda\n</code></pre> <ul> <li>Install a conda environment called 'rcaes_env': </li> </ul> <pre><code>$ conda create -n rcaes_env\n</code></pre> <ul> <li>Enter Y to proceed. Wait until you see the following. It may take a while. </li> </ul> <pre><code>#                                                                                                                                                               \n# To activate this environment, use                                                                                                                             \n#                                                                                                                                                               \n#     $ conda activate rcaes_env                                                                                                                              \n#                                                                                                                                                               \n# To deactivate an active environment, use                                                                                                                      \n#                                                                                                                                                               \n#     $ conda deactivate    \n</code></pre> <ul> <li>Now let's activate the environment:</li> </ul> <pre><code>$ conda activate rcaes_env \n</code></pre> <ul> <li>First, let's install essential packages</li> </ul> <pre><code>conda install -c conda-forge python=3.9 jupyter jupyterlab notebook numpy scipy ipython\n</code></pre> <ul> <li>Next, let's install github CLI, a command-line interface to GitHub for use in your terminal or your scripts</li> </ul> <pre><code>conda install -c conda-forge gh\n</code></pre> <ul> <li> <p>Next, go back to Amarel Open OnDemand. This time, we will launch a personal jupyter. Click on 'Interactive Apps', choose 'Personal Jupyter'. </p> </li> <li> <p>Settings for Personal Jupyter: </p> </li> </ul> <pre><code>    Number of hours: 10 \n    Number of cores: 1 \n    Gigabytes of memory: 10 \n    Partition: main\n    Leave Reservation and slurm feature blank \n    conda path: /projects/community/anaconda/2020.07/gc563 \n    conda environment: rcaes_env\n</code></pre>"},{"location":"Lecture_2_install_python_amarel/#conda-alternative-mamba","title":"Conda Alternative: Mamba","text":"<ul> <li> <p>If you find Conda super slow at solving the environment, I'd recommend you try using an alternative package manager Mamba. </p> </li> <li> <p>Install Miniforge using: </p> </li> </ul> <pre><code>wget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre> <ul> <li> <p>Close and reopen the terminal window. </p> </li> <li> <p>Create a new environment <code>rcaes_env_new</code>: </p> </li> </ul> <pre><code>mamba create -n rcaes_env_new python=3.9 jupyter jupyterlab notebook numpy scipy ipython pandas matplotlib cartopy geopandas xarray dask netCDF4 bottleneck gh\n</code></pre> <ul> <li> <p>Next, go back to Amarel Open OnDemand. This time, we will launch a personal jupyter. Click on 'Interactive Apps', choose 'Personal Jupyter'. </p> </li> <li> <p>Settings for Personal Jupyter: </p> </li> </ul> <pre><code>    Number of hours: 10 \n    Number of cores: 1 \n    Gigabytes of memory: 10 \n    Partition: main\n    Leave Reservation and slurm feature blank \n    conda path: /home/YOURNETID/miniforge3\n    conda environment: rcaes_env_new\n</code></pre>"},{"location":"Lecture_2_intro_to_unix/","title":"Intro to Unix","text":"<p>The notes below are modified from the excellent Unix Shell tutorial  that is freely available on the Software Carpentry website. I highly recommend checking out the full version for further reading. The material is being used here under the terms of the Creative Commons Attribution license.</p>"},{"location":"Lecture_2_intro_to_unix/#what-is-unix-shell","title":"What is Unix shell?","text":"<p>The Unix shell is both a command-line interface and a scripting language. With the shell, it is possible to invoke complicated programs like climate modeling or a simple command that create an empty directory.  The most popular Unix Shell is Bash. </p>"},{"location":"Lecture_2_intro_to_unix/#navigating-files-and-directories","title":"Navigating Files and Directories","text":"<p>Several commands are frequently used to create, inspect, rename, and delete files and directories.</p> <p>To get started, open a terminal using the OpenOnDemand Clusters: Amarel Cluster Shell Access. After entering your password, you will see the welcome message. </p> <pre><code>(base) [xj103@amarel2 ~]$ \n</code></pre> <p>The dollar sign is a prompt, which shows us that the shell is waiting for input.</p> <p><code>xj103</code> is our username and <code>amarel2</code> is the hostname. The username will be your NetID. The hostname indicates the node you're at. Here we are using login nodes (amarel1, amarel2, etc.). Cluster login nodes provide a shared environment where users can transfer data, build software, and prepare their calculations.  Running applications on a shared login node or doing things that consume significant compute, memory, or network resources can unfairly impact other users.  Please do not do that. Do not run your research applications on the login node.  If you log into the terminal via a compute nodes, you will see the hostname as hal0001, hal0002 etc., which are nodes assigned to you for computation. </p> <p>From now on, we will just use a <code>$</code> to indicate the prompt.</p> <p>To find out your username in general, you can use the command</p> <pre><code>$ whoami\nxj103\n</code></pre> <p>and to find out your hostname</p> <pre><code>$ hostname\namarel2.amarel.rutgers.edu\n</code></pre> <p>Next, let's find out where we are by running a command called <code>pwd</code> (which stands for \"print working directory\"). At any moment, our current working directory is our current default directory, i.e., the directory that the computer assumes we want to run commands in unless we explicitly specify something else. Here, the computer's response is <code>/home/xj103</code>, which is the home directory of the user named <code>xj103</code>.</p> <pre><code>$ pwd\n</code></pre> <pre><code>/home/xj103\n</code></pre> <p>To understand what a \"home directory\" is, let's have a look at how the file system as a whole is organized.  For the sake of this example, we'll be illustrating the filesystem on Amarel.  </p> <pre><code>$ cd /\n</code></pre> <p>Now let's learn the command that will let us see the contents of our own filesystem.  We can see what's in our home directory by running <code>ls</code>, which stands for \"listing\":</p> <pre><code>$ ls\n</code></pre> <p>On a Unix computer, at the top is the root directory that holds everything else. We refer to it using a slash character <code>/</code> on its own; this is the leading slash in <code>/home/xj103</code>.</p> <p>Inside that directory are several other directories: <code>bin</code> (which is where some built-in programs are stored), <code>lib</code> (for the software \"libraries\" used by different programs), <code>home</code> (where users' personal directories are located), <code>projects</code> (where project data are stored), <code>etc</code> (system-wide configuration files), and so on.  </p> <p>Now let's go back to our home directory with ~ (tilde)</p> <pre><code>$ cd ~\n</code></pre> <p><code>ls</code> prints the names of the files and directories in the current directory in alphabetical order, arranged neatly into columns. We can make its output more comprehensible by using the flag <code>-F</code>, which tells <code>ls</code> to add a trailing <code>/</code> to the names of directories:</p> <pre><code>$ ls -F\n</code></pre> <p><code>ls</code> has lots of other options. To find out what they are, we can type:</p> <pre><code>$ man ls\n</code></pre> <p><code>man</code> is the Unix \"manual\" command: it prints a description of a command and its options, and (if you're lucky) provides a few examples of how to use it. To navigate through the <code>man</code> pages, you may use the up and down arrow keys to move line-by-line. Quit the <code>man</code> pages by typing \"q\".</p> <p>The command to change locations is <code>cd</code> followed by a directory name to change our working directory. <code>cd</code> stands for \"change directory\", which is a bit misleading: the command doesn't change the directory, it changes the shell's idea of what directory we are in.</p> <p>Let's say we want to move to the <code>Documents</code> directory we saw above.  We can use the following series of commands to get there:</p> <pre><code>$ cd Documents\n</code></pre> <p>These commands will move us from our home directory onto into the <code>Documents</code> directory. <code>cd</code> doesn't print anything, but if we run <code>pwd</code> after it, we can see that we are now in <code>/home/xj103/Documents</code>.</p> <p>We now know how to go down the directory tree, but how do we go up? There is a shortcut in the shell to move up one directory level that looks like this:</p> <pre><code>$ cd ..\n</code></pre> <p><code>..</code> is a special directory name meaning \"the directory containing this one\", or more succinctly, the parent of the current directory. Sure enough, if we run <code>pwd</code> after running <code>cd ..</code>, we're back in <code>/home/xj103</code>:</p> <pre><code>$ pwd\n</code></pre> <pre><code>/home/xj103\n</code></pre> <p>The special directory <code>..</code> doesn't usually show up when we run <code>ls</code>.  If we want to display it, we can give <code>ls</code> the <code>-a</code> flag:</p> <pre><code>$ ls -F -a\n</code></pre> <p><code>-a</code> stands for \"show all\"; it forces <code>ls</code> to show us file and directory names that begin with <code>.</code>, such as <code>..</code> (which, if we're in <code>/home/xj103/Documents</code>, refers to the <code>/home/xj103</code> directory) As you can see, it also displays another special directory that's just called <code>.</code>, which means \"the current working directory\". It may seem redundant to have a name for it, but we'll see some uses for it soon.</p> <p>Note that in most command line tools, multiple parameters can be combined with a single <code>-</code> and no spaces between the parameters: <code>ls -F -a</code> is equivalent to <code>ls -Fa</code>.</p> <p>These then, are the basic commands for navigating the filesystem on your computer: <code>pwd</code>, <code>ls</code> and <code>cd</code>.  Let's explore some variations on those commands.  What happens if you type <code>cd</code> on its own, without giving a directory?  </p> <pre><code>$ cd\n</code></pre> <p>How can you check what happened?  <code>pwd</code> gives us the answer!  </p> <pre><code>$ pwd\n</code></pre> <pre><code>/home/xj103\n</code></pre> <p>It turns out that <code>cd</code> without an argument will return you to your home directory, which is great if you've gotten lost in your own filesystem.  </p> <pre><code>$ cd \n</code></pre> <p>Check that we've moved to the right place by running <code>pwd</code> and <code>ls -F</code> </p> <p>If we want to move up one level from the data directory, we could use <code>cd ..</code>.  But there is another way to move to any directory, regardless of your current location.  </p> <p>So far, when specifying directory names, or even a directory path (as above), we have been using relative paths.  When you use a relative path with a command like <code>ls</code> or <code>cd</code>, it tries to find that location from where we are, rather than from the root of the file system.  </p> <p>However, it is possible to specify the absolute path to a directory by including its entire path from the root directory, which is indicated by a leading slash.  The leading <code>/</code> tells the computer to follow the path from the root of the file system, so it always refers to exactly one directory, no matter where we are when we run the command.</p> <p>This allows us to move to our <code>examples</code> directory from anywhere on the filesystem.  To find the absolute path we're looking for, we can use <code>pwd</code> and then extract the piece we need to move to <code>examples</code>.  </p> <pre><code>$ pwd\n</code></pre> <pre><code>$ cd /home/xj103/Documents/\n</code></pre> <p>Run <code>pwd</code> and <code>ls -F</code> to ensure that we're in the directory we expect.  </p>"},{"location":"Lecture_2_intro_to_unix/#two-more-shortcuts","title":"Two More Shortcuts","text":"<p>The shell interprets the character <code>~</code> (tilde) at the start of a path to mean \"the current user's home directory\". For example, if my home directory is <code>/home/xj103</code>, then <code>~/rcaes</code> is equivalent to <code>/home/xj103/rcaes</code>. This only works if it is the first character in the path.</p> <p>Another shortcut is the <code>-</code> (dash) character.  <code>cd</code> will translate <code>-</code> into the previous directory I was in, which is faster than having to remember, then type, the full path.  This is a very efficient way of moving back and forth between directories. The difference between <code>cd ..</code> and <code>cd -</code> is that the former brings you up, while the latter brings you back. You can think of it as the Last Channel button on a TV remote.</p>"},{"location":"Lecture_2_intro_to_unix/#tab-completion","title":"Tab Completion","text":"<p>Typing the full path to directories and files can be slow and annoying. Fortunately, we have \"tab completion\" to help us. Try typing <code>cd Doc</code> and then press the <code>&lt;tab&gt;</code>. The system will try to \"auto complete\" your command. Pressing tab twice brings up a list of all the files, and so on. This is called tab completion, and we will see it in many other tools as we go on.</p>"},{"location":"Lecture_2_intro_to_unix/#key-points","title":"Key Points:","text":"<ul> <li>\"The file system is responsible for managing information on the disk.\"</li> <li>\"Information is stored in files, which are stored in directories (folders).\"</li> <li>\"Directories can also store other directories, which forms a directory tree.\"</li> <li>\"<code>cd path</code> changes the current working directory.\"</li> <li>\"<code>ls path</code> prints a listing of a specific file or directory; <code>ls</code> on its own lists the current working directory.\"</li> <li><code>pwd</code> prints the user's current working directory.</li> <li><code>whoami</code> shows the user's current identity.</li> <li><code>/</code> on its own is the root directory of the whole file system.</li> <li>A relative path specifies a location starting from the current location.</li> <li>An absolute path specifies a location from the root of the file system.</li> <li>Directory names in a path are separated with '/' (forward slash) on Unix, but '\\\\' (backslash) on Windows.</li> <li>'..' means 'the directory above the current one'; '.' on its own means 'the current directory'.</li> <li>Most files' names are <code>something.extension</code>. The extension isn't required, and doesn't guarantee anything, but is normally used to indicate the type of data in the file.</li> <li>Most commands take options (flags) which begin with a '-'.</li> </ul>"},{"location":"Lecture_2_intro_to_unix/#working-with-files-and-directories","title":"Working with Files and Directories","text":"<p>We now know how to explore files and directories, but how do we create them in the first place? Let's go back to our home directory and use <code>ls -F</code> to see what it contains:</p> <pre><code>$ cd\n$ pwd\n</code></pre> <pre><code>/home/xj103/\n</code></pre> <p>Let's create a new directory called <code>thesis</code> using the command <code>mkdir thesis</code> (which has no output):</p> <pre><code>$ mkdir thesis\n</code></pre> <p>As you might guess from its name, <code>mkdir</code> means \"make directory\". Since <code>thesis</code> is a relative path (i.e., doesn't have a leading slash), the new directory is created in the current working directory:</p> <pre><code>$ ls -F\n</code></pre>"},{"location":"Lecture_2_intro_to_unix/#good-names-for-files-and-directories","title":"Good names for files and directories","text":"<p>Complicated names of files and directories can make your life painful  when working on the command line. Here we provide a few useful  tips for the names of your files.</p> <ol> <li> <p>Don't use whitespaces.</p> <p>Whitespaces can make a name more meaningful    but since whitespace is used to break arguments on the command line    is better to avoid them on name of files and directories. You can use <code>-</code> (dash) and <code>_</code> (underscore) instead of whitespace.</p> </li> <li> <p>Don't begin the name with <code>-</code> (dash).</p> <p>Commands treat names starting with <code>-</code> as options.</p> </li> <li> <p>Stick with letters, numbers, <code>.</code> (period), <code>-</code> (dash) and <code>_</code> (underscore).</p> <p>Many other characters have special meanings on the command line. We will learn about some of these during this lesson. There are special characters that can cause your command to not work as expected and can even result in data loss.</p> </li> </ol> <p>If you need to refer to names of files or directories that have whitespace  or another non-alphanumeric character, you should surround the name in quotes (<code>\"\"</code>).</p> <p>Since we've just created the <code>thesis</code> directory, there's nothing in it yet:</p> <pre><code>$ ls -F thesis\n</code></pre> <p>Let's change our working directory to <code>thesis</code> using <code>cd</code>. We then create a blank new file called <code>draft.txt</code> using the <code>touch command</code>:</p> <pre><code>$ cd thesis\n$ touch draft.txt\n</code></pre> <p>Now we can edit the file in JupyterLab's text editor. Let's type in a few lines of text. Once we're happy with our text, we save the file, and return to the shell.</p> <p><code>ls</code> now shows that we have created a file called <code>draft.txt</code>:</p> <pre><code>$ ls\ndraft.txt\n</code></pre> <p>Let's tidy up by running <code>rm draft.txt</code>:</p> <pre><code>$ rm draft.txt\n</code></pre> <p>This command removes files (<code>rm</code> is short for \"remove\"). If we run <code>ls</code> again, its output is empty once more, which tells us that our file is gone:</p> <pre><code>$ ls\n</code></pre>"},{"location":"Lecture_2_intro_to_unix/#deleting-is-forever","title":"Deleting Is Forever","text":"<p>The Unix shell doesn't have a trash bin that we can recover deleted files from (though most graphical interfaces to Unix do).  Instead, when we delete files, they are unhooked from the file system so that their storage space on disk can be recycled. Tools for finding and recovering deleted files do exist, but there's no guarantee they'll work in any particular situation, since the computer may recycle the file's disk space right away.</p> <p>Let's re-create that file and then move up one directory to <code>/home/xj103</code> using <code>cd ..</code>:</p> <pre><code>$ touch draft.txt\n$ cd ..\n</code></pre> <p>If we try to remove the entire <code>thesis</code> directory using <code>rm thesis</code>, we get an error message:</p> <pre><code>$ rm thesis\n</code></pre> <pre><code>rm: cannot remove `thesis`: Is a directory\n</code></pre> <p>This happens because <code>rm</code> by default only works on files, not directories.</p> <p>To really get rid of <code>thesis</code> we must also delete the file <code>draft.txt</code>. We can do this with the recursive option for <code>rm</code>:</p> <pre><code>$ rm -r thesis\n</code></pre>"},{"location":"Lecture_2_intro_to_unix/#with-great-power-comes-great-responsibility","title":"With Great Power Comes Great Responsibility","text":"<p>Removing the files in a directory recursively can be very dangerous  operation. If we're concerned about what we might be deleting we can  add the \"interactive\" flag <code>-i</code> to <code>rm</code> which will ask us for confirmation  before each step</p> <pre><code> $ rm -r -i thesis\n rm: descend into directory \u2018thesis\u2019? y\n rm: remove regular file \u2018thesis/draft.txt\u2019? y\n rm: remove directory \u2018thesis\u2019? y\n</code></pre> <p>This removes everything in the directory, then the directory itself, asking  at each step for you to confirm the deletion.</p> <p>Let's create that directory and file one more time.</p> <pre><code>$ mkdir thesis\n$ touch thesis/draft.txt\n$ ls thesis\n</code></pre> <pre><code>draft.txt\n</code></pre> <p><code>draft.txt</code> isn't a particularly informative name, so let's change the file's name using <code>mv</code>, which is short for \"move\":</p> <pre><code>$ mv thesis/draft.txt thesis/quotes.txt\n</code></pre> <p>The first parameter tells <code>mv</code> what we're \"moving\", while the second is where it's to go. In this case, we're moving <code>thesis/draft.txt</code> to <code>thesis/quotes.txt</code>, which has the same effect as renaming the file. Sure enough, <code>ls</code> shows us that <code>thesis</code> now contains one file called <code>quotes.txt</code>:</p> <pre><code>$ ls thesis\n</code></pre> <pre><code>quotes.txt\n</code></pre> <p>One has to be careful when specifying the target file name, since <code>mv</code> will silently overwrite any existing file with the same name, which could lead to data loss. An additional flag, <code>mv -i</code> (or <code>mv --interactive</code>), can be used to make <code>mv</code> ask you for confirmation before overwriting.</p> <p>Just for the sake of consistency, <code>mv</code> also works on directories</p> <p>Let's move <code>quotes.txt</code> into the current working directory. We use <code>mv</code> once again, but this time we'll just use the name of a directory as the second parameter to tell <code>mv</code> that we want to keep the filename, but put the file somewhere new. (This is why the command is called \"move\".) In this case, the directory name we use is the special directory name <code>.</code> that we mentioned earlier.</p> <pre><code>$ mv thesis/quotes.txt .\n</code></pre> <p>The effect is to move the file from the directory it was in to the current working directory. <code>ls</code> now shows us that <code>thesis</code> is empty:</p> <pre><code>$ ls thesis\n</code></pre> <p>Further, <code>ls</code> with a filename or directory name as a parameter only lists that file or directory. We can use this to see that <code>quotes.txt</code> is still in our current directory:</p> <pre><code>$ ls quotes.txt\n</code></pre> <pre><code>quotes.txt\n</code></pre> <p>The <code>cp</code> command works very much like <code>mv</code>, except it copies a file instead of moving it. We can check that it did the right thing using <code>ls</code> with two paths as parameters --- like most Unix commands, <code>ls</code> can be given multiple paths at once:</p> <pre><code>$ cp quotes.txt thesis/quotations.txt\n$ ls \n</code></pre> <pre><code>quotes.txt   thesis/quotations.txt\n</code></pre> <p>To prove that we made a copy, let's delete the <code>quotes.txt</code> file in the current directory and then run that same <code>ls</code> again.</p> <pre><code>$ rm quotes.txt\n$ ls quotes.txt thesis/quotations.txt\n</code></pre> <pre><code>ls: cannot access quotes.txt: No such file or directory\nthesis/quotations.txt\n</code></pre> <p>This time it tells us that it can't find <code>quotes.txt</code> in the current directory, but it does find the copy in <code>thesis</code> that we didn't delete.</p>"},{"location":"Lecture_2_intro_to_unix/#key-points_1","title":"Key Points","text":"<ul> <li><code>cp old new</code> copies a file.</li> <li><code>mkdir path</code> creates a new directory.</li> <li><code>mv old new</code> moves (renames) a file or directory.</li> <li><code>rm path</code> removes (deletes) a file.</li> <li>Use of the Control key may be described in many ways, including <code>Ctrl-X</code>, <code>Control-X</code>, and <code>^X</code>.</li> <li>The shell does not have a trash bin: once something is deleted, it's really gone.</li> <li>Depending on the type of work you do, you may need a more powerful text editor than Nano.</li> </ul>"},{"location":"Lecture_2_intro_to_unix/#learning-more","title":"Learning More","text":"<p>The goal of this lesson was to familiarize you with the basics of working with files and directories. There is a lot more to the unix shell and filexsystem than what we have  covered here! To ge deeper with self study, we recommend the excellent Software Carpentry Unix Shell Lesson, on which the above material was based.</p>"},{"location":"Lecture_3_GitHub/","title":"Summary of useful Git commands","text":""},{"location":"Lecture_3_GitHub/#configuring","title":"Configuring:","text":"<p>Set up your username and email</p> <pre><code>git config --global user.name \"Xiaomeng Jin\"\ngit config --global user.email \"xiaomeng.jin@rutgers.edu\"\n</code></pre>"},{"location":"Lecture_3_GitHub/#branches","title":"Branches:","text":"<p>Branches are an important part of working with Git.  Any commits you make will be made on the branch you're currently \u201cchecked out\u201d to. Use git status to see which branch that is.</p> <pre><code>git branch [branch-name] #creates a new branch\ngit checkout [branch-name] # switch to the specified branch and updates the working directory\ngit merge [branch] # combines the specified branch's history into the current branch.\ngit branch -d [branch-name] #deletes the specified branch\n</code></pre>"},{"location":"Lecture_3_GitHub/#create-repositories","title":"Create repositories:","text":"<p>Start out a new repository:</p> <pre><code>cd my_project\ngit init      \n</code></pre> <p>Or clone a repository using git clone</p> <pre><code>git clone https://github.com/rcaes2023/assignment_1_python-MazvitaChikomo.git\n</code></pre> <p>Or clone a repository using GitHub Command Line OWNER/REPO syntax.</p> <pre><code>gh repo clone rcaes2023/assignment_1_python-MazvitaChikomo\ncd assignment_1_python-MazvitaChikomo\n</code></pre> <p>If you want to get a repository that you don't have permission to push to, you can fork the repository.</p> <pre><code>gh repo fork cli/cli\n</code></pre>"},{"location":"Lecture_3_GitHub/#make-changes","title":"Make Changes","text":"<p>Browse and inspect the evolution of project files</p> <pre><code>git status    # tells you which branch you are at, what files are staged, which ones have been modified, are new,...\ngit log       # view the commit log\ngit diff      # view file content differences\n</code></pre> <p>Version control</p> <pre><code>git add &lt;filenames&gt;  #Snapshots the file in preparation for versioning\ngit commit -m \"your brief commit message goes here\" #Records file snapshots permanently in version history\n</code></pre>"},{"location":"Lecture_3_GitHub/#synchronize-changes","title":"Synchronize changes:","text":"<pre><code>git push #uploads all local branch commits to GitHub\ngit pull # updates your current local working branch with all new commits from the corresponding remote branch on GitHub.\n</code></pre>"},{"location":"Lecture_3_GitHub/#basic-github-workflow","title":"Basic GitHub workflow:","text":"<ul> <li>clone your local repo with <code>gh repo clone &lt;REPO&gt;</code>,</li> <li>make your changes and stage them with <code>git add &lt;filenames&gt;</code>,</li> <li>commit your changes with <code>git commit -m \"your brief commit message goes here\"</code>, and</li> <li>upload the changes to GitHub with<code>git push</code></li> </ul>"},{"location":"Lecture_3_functions_classes_modules/","title":"Lecture 3 Python Functions and Classes","text":"In\u00a0[48]: Copied! <pre># define a function\ndef say_hello():\n\"\"\"Return the word hello.\"\"\"\n    return 'Hello'\n</pre> # define a function def say_hello():     \"\"\"Return the word hello.\"\"\"     return 'Hello' In\u00a0[49]: Copied! <pre># functions are also objects\ntype(say_hello)\n</pre> # functions are also objects type(say_hello) Out[49]: <pre>function</pre> In\u00a0[50]: Copied! <pre># this doesnt call\nsay_hello?\n</pre> # this doesnt call say_hello? <pre>Signature: say_hello()\nDocstring: Return the word hello.\nFile:      /var/folders/7b/6t7qqfj57bb0_ml_y_5bw86r0000gn/T/ipykernel_76551/1650374671.py\nType:      function</pre> In\u00a0[51]: Copied! <pre># this does\nsay_hello()\n</pre> # this does say_hello() Out[51]: <pre>'Hello'</pre> In\u00a0[52]: Copied! <pre># assign the result to something\nres = say_hello()\nres\n</pre> # assign the result to something res = say_hello() res Out[52]: <pre>'Hello'</pre> In\u00a0[12]: Copied! <pre># take some arguments\ndef say_hello_to(name):\n\"\"\"Return a greeting to `name`\"\"\"\n    return 'Hello ' + name\n</pre> # take some arguments def say_hello_to(name):     \"\"\"Return a greeting to `name`\"\"\"     return 'Hello ' + name In\u00a0[13]: Copied! <pre># intended usage\nsay_hello_to('World')\n</pre> # intended usage say_hello_to('World') Out[13]: <pre>'Hello World'</pre> In\u00a0[14]: Copied! <pre>say_hello_to(10)\n</pre> say_hello_to(10) <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[14], line 1\n----&gt; 1 say_hello_to(10)\n\nCell In[12], line 4, in say_hello_to(name)\n      2 def say_hello_to(name):\n      3 \"\"\"Return a greeting to `name`\"\"\"\n----&gt; 4     return 'Hello ' + name\n\nTypeError: can only concatenate str (not \"int\") to str</pre> In\u00a0[15]: Copied! <pre># redefine the function\ndef say_hello_to(name):\n\"\"\"Return a greeting to `name`\"\"\"\n    return 'Hello ' + str(name)\n</pre> # redefine the function def say_hello_to(name):     \"\"\"Return a greeting to `name`\"\"\"     return 'Hello ' + str(name) In\u00a0[16]: Copied! <pre>say_hello_to(10)\n</pre> say_hello_to(10) Out[16]: <pre>'Hello 10'</pre> In\u00a0[17]: Copied! <pre># take an optional keyword argument\ndef say_hello_language(name, chinese=False):\n\"\"\"Say hello in multiple languages.\"\"\"\n    if chinese:\n        greeting = 'Ni Hao '\n    else:\n        greeting = 'Hello '\n    return greeting + name\n</pre> # take an optional keyword argument def say_hello_language(name, chinese=False):     \"\"\"Say hello in multiple languages.\"\"\"     if chinese:         greeting = 'Ni Hao '     else:         greeting = 'Hello '     return greeting + name In\u00a0[18]: Copied! <pre>print(say_hello_language('Matt'))\nprint(say_hello_language('Siyi', chinese=True))\n</pre> print(say_hello_language('Matt')) print(say_hello_language('Siyi', chinese=True))  <pre>Hello Matt\nNi Hao Siyi\n</pre> In\u00a0[19]: Copied! <pre># flexible number of arguments\ndef say_hello_to_everyone(*args):\n    return ['hello ' + str(a) for a in args]\n</pre> # flexible number of arguments def say_hello_to_everyone(*args):     return ['hello ' + str(a) for a in args] In\u00a0[20]: Copied! <pre>say_hello_to_everyone('Matt', 'Siyi', 'Kerry')\n</pre> say_hello_to_everyone('Matt', 'Siyi', 'Kerry') Out[20]: <pre>['hello Matt', 'hello Siyi', 'hello Kerry']</pre> In\u00a0[53]: Copied! <pre># The function doesn't return anything, but it changes the input arguments. \ndef remove_last_from_list(input_list):\n    input_list.pop()\n</pre> # The function doesn't return anything, but it changes the input arguments.  def remove_last_from_list(input_list):     input_list.pop() In\u00a0[22]: Copied! <pre>names = ['Matt', 'Siyi', 'Kerry']\nremove_last_from_list(names)\nprint(names)\nremove_last_from_list(names)\nprint(names)\n</pre> names = ['Matt', 'Siyi', 'Kerry'] remove_last_from_list(names) print(names) remove_last_from_list(names) print(names) <pre>['Matt', 'Siyi']\n['Matt']\n</pre> <p>We can do something similar with a pure function.</p> <p>In general, pure functions are safer and more reliable.</p> In\u00a0[23]: Copied! <pre>def remove_last_from_list_pure(input_list):\n    new_list = input_list.copy()\n    new_list.pop()\n    return new_list\n</pre> def remove_last_from_list_pure(input_list):     new_list = input_list.copy()     new_list.pop()     return new_list In\u00a0[24]: Copied! <pre>names = ['Matt', 'Siyi', 'Kerry']\nnew_names = remove_last_from_list_pure(names)\nprint(names)\nprint(new_names)\n</pre> names = ['Matt', 'Siyi', 'Kerry'] new_names = remove_last_from_list_pure(names) print(names) print(new_names) <pre>['Matt', 'Siyi', 'Kerry']\n['Matt', 'Siyi']\n</pre> <p>We could spend the rest of the day talking about functions, but we have to move on.</p> In\u00a0[25]: Copied! <pre># Create a class named Student with a name. \nclass Student:\n    name = 'Matt'\n</pre> # Create a class named Student with a name.  class Student:     name = 'Matt' In\u00a0[26]: Copied! <pre>print(Student.name)\n</pre> print(Student.name) <pre>Matt\n</pre> In\u00a0[27]: Copied! <pre>class Student:\n    \n    def __init__(self, name):\n        self.name = name\n</pre> class Student:          def __init__(self, name):         self.name = name  In\u00a0[28]: Copied! <pre>s1 = Student('Matt')\ns1\n</pre> s1 = Student('Matt') s1 Out[28]: <pre>&lt;__main__.Student at 0x111dfa250&gt;</pre> <p>Our class only has a single attribute so far:</p> In\u00a0[29]: Copied! <pre>s1.name\n</pre> s1.name Out[29]: <pre>'Matt'</pre> <p>Let's add more, along with some input validation:</p> In\u00a0[54]: Copied! <pre>class Student:\n    \n    def __init__(self, name, age, major):\n        self.name = name.upper()\n        self.major = major\n        self.age = age\n</pre> class Student:          def __init__(self, name, age, major):         self.name = name.upper()         self.major = major         self.age = age          In\u00a0[57]: Copied! <pre>s1 = Student('Matt', 22, 'Environmental Science')\n</pre> s1 = Student('Matt', 22, 'Environmental Science')  In\u00a0[58]: Copied! <pre>s1.major\n</pre> s1.major Out[58]: <pre>'Environmental Science'</pre> In\u00a0[59]: Copied! <pre>s1.name\n</pre> s1.name Out[59]: <pre>'MATT'</pre> In\u00a0[60]: Copied! <pre>s1.age\n</pre> s1.age Out[60]: <pre>22</pre> In\u00a0[61]: Copied! <pre>class Student:\n    \n    def __init__(self, name, age, major):\n        self.name = name.upper()\n        self.major = major\n        \n        if age&lt;0:\n            raise ValueError(f'Invalid age {age}')\n        self.age = age\n</pre> class Student:          def __init__(self, name, age, major):         self.name = name.upper()         self.major = major                  if age&lt;0:             raise ValueError(f'Invalid age {age}')         self.age = age          In\u00a0[62]: Copied! <pre>s1 = Student('Matt', -46, 'Environmental Science')\ns1\n</pre> s1 = Student('Matt', -46, 'Environmental Science') s1 <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[62], line 1\n----&gt; 1 s1 = Student('Matt', -46, 'Environmental Science')\n      2 s1\n\nCell In[61], line 8, in Student.__init__(self, name, age, major)\n      5 self.major = major\n      7 if age&lt;0:\n----&gt; 8     raise ValueError(f'Invalid age {age}')\n      9 self.age = age\n\nValueError: Invalid age -46</pre> In\u00a0[63]: Copied! <pre>s1 = Student('Matt', 22, 'Environmental Science')\ns1\n</pre> s1 = Student('Matt', 22, 'Environmental Science') s1 Out[63]: <pre>&lt;__main__.Student at 0x1123fe6d0&gt;</pre> In\u00a0[36]: Copied! <pre>class Student:\n    \n    def __init__(self, name, age, major):\n        self.name = name.upper()\n        self.major = major\n        \n        if age&lt;0:\n            raise ValueError(f'Invalid age {age}')\n        self.age = age\n    \n    def is_ES(self):\n        return self.major == 'Environmental Science'\n</pre> class Student:          def __init__(self, name, age, major):         self.name = name.upper()         self.major = major                  if age&lt;0:             raise ValueError(f'Invalid age {age}')         self.age = age          def is_ES(self):         return self.major == 'Environmental Science' In\u00a0[37]: Copied! <pre>s1 = Student('Matt', 22, 'Environmental Science')\ns1.is_ES()\n</pre> s1 = Student('Matt', 22, 'Environmental Science') s1.is_ES() Out[37]: <pre>True</pre> In\u00a0[38]: Copied! <pre>s2 = Student('Siyi', 25, 'Ecology')\ns2.is_ES()\n</pre> s2 = Student('Siyi', 25, 'Ecology') s2.is_ES() Out[38]: <pre>False</pre> In\u00a0[39]: Copied! <pre>s1.age = 40\n</pre> s1.age = 40 In\u00a0[40]: Copied! <pre>s1.age\n</pre> s1.age Out[40]: <pre>40</pre> In\u00a0[41]: Copied! <pre>del(s1.age)\n</pre> del(s1.age) In\u00a0[42]: Copied! <pre>s1.age\n</pre> s1.age <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[42], line 1\n----&gt; 1 s1.age\n\nAttributeError: 'Student' object has no attribute 'age'</pre> In\u00a0[43]: Copied! <pre>s2.age\n</pre> s2.age Out[43]: <pre>25</pre> In\u00a0[44]: Copied! <pre>s1.is_ES()\n</pre> s1.is_ES() Out[44]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Lecture_3_functions_classes_modules/#lecture-3-python-functions-and-classes","title":"Lecture 3 Python Functions and Classes\u00b6","text":"<p>For longer and more complex tasks, it is important to organize your code into reuseable elements. For example, if you find yourself cutting and pasting the same or similar lines of code over and over, you probably need to define a function to encapsulate that code and make it reusable. An important principle in programming in DRY: \"don't repeat yourself\". Repetition is tedious and opens you up to errors. Strive for elegance and simplicity in your programs.</p>"},{"location":"Lecture_3_functions_classes_modules/#functions","title":"Functions\u00b6","text":"<p>Functions are a central part of advanced python programming. Functions take some inputs (\"arguments\") and do something in response. Usually functions return something, but not always.</p>"},{"location":"Lecture_3_functions_classes_modules/#pure-vs-impure-functions","title":"Pure vs. Impure Functions\u00b6","text":"<p>Functions that don't modify their arguments or produce any other side-effects are called pure.</p> <p>Functions that modify their arguments or cause other actions to occur are called impure.</p> <p>Below is an impure function.</p>"},{"location":"Lecture_3_functions_classes_modules/#classes","title":"Classes\u00b6","text":"<p>We have worked with many different types of python objects so far: strings, lists, dictionaries, etc. These objects have different attributes and respond in different ways to the built-in functions (<code>len</code>, etc.)</p> <p>Python is an object oriented programming language.</p> <p>Almost everything in Python is an object, with its properties and methods.</p> <p>How can we make our own, custom objects? Answer: by defining classes.</p>"},{"location":"Lecture_3_functions_classes_modules/#a-class-to-represent-a-student","title":"A class to represent a Student\u00b6","text":""},{"location":"Lecture_3_functions_classes_modules/#the-__init__-function","title":"The __init__() function\u00b6","text":"<p>All classes have a function called __init__(), which is always executed when the class is being initiated.</p>"},{"location":"Lecture_3_functions_classes_modules/#now-lets-add-a-custom-method","title":"Now let's add a custom method:\u00b6","text":""},{"location":"Lecture_3_functions_classes_modules/#modify-object-properties","title":"Modify Object Properties\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/","title":"Lecture 4: Numpy and Matplotlib","text":"In\u00a0[4]: Copied! <pre>import numpy as np\n</pre> import numpy as np <p>What did we just do? We imported a package. This brings new variables (mostly functions) into our interpreter. We access them as follows.</p> In\u00a0[3]: Copied! <pre># find out what's in numpy\ndir(np)\n</pre> # find out what's in numpy dir(np) Out[3]: <pre>['ALLOW_THREADS',\n 'AxisError',\n 'BUFSIZE',\n 'CLIP',\n 'ComplexWarning',\n 'DataSource',\n 'ERR_CALL',\n 'ERR_DEFAULT',\n 'ERR_IGNORE',\n 'ERR_LOG',\n 'ERR_PRINT',\n 'ERR_RAISE',\n 'ERR_WARN',\n 'FLOATING_POINT_SUPPORT',\n 'FPE_DIVIDEBYZERO',\n 'FPE_INVALID',\n 'FPE_OVERFLOW',\n 'FPE_UNDERFLOW',\n 'False_',\n 'Inf',\n 'Infinity',\n 'MAXDIMS',\n 'MAY_SHARE_BOUNDS',\n 'MAY_SHARE_EXACT',\n 'MachAr',\n 'ModuleDeprecationWarning',\n 'NAN',\n 'NINF',\n 'NZERO',\n 'NaN',\n 'PINF',\n 'PZERO',\n 'PackageLoader',\n 'RAISE',\n 'RankWarning',\n 'SHIFT_DIVIDEBYZERO',\n 'SHIFT_INVALID',\n 'SHIFT_OVERFLOW',\n 'SHIFT_UNDERFLOW',\n 'ScalarType',\n 'Tester',\n 'TooHardError',\n 'True_',\n 'UFUNC_BUFSIZE_DEFAULT',\n 'UFUNC_PYVALS_NAME',\n 'VisibleDeprecationWarning',\n 'WRAP',\n '_NoValue',\n '__NUMPY_SETUP__',\n '__all__',\n '__builtins__',\n '__cached__',\n '__config__',\n '__doc__',\n '__file__',\n '__git_revision__',\n '__loader__',\n '__mkl_version__',\n '__name__',\n '__package__',\n '__path__',\n '__spec__',\n '__version__',\n '_distributor_init',\n '_globals',\n '_import_tools',\n '_mat',\n 'abs',\n 'absolute',\n 'absolute_import',\n 'add',\n 'add_docstring',\n 'add_newdoc',\n 'add_newdoc_ufunc',\n 'add_newdocs',\n 'alen',\n 'all',\n 'allclose',\n 'alltrue',\n 'amax',\n 'amin',\n 'angle',\n 'any',\n 'append',\n 'apply_along_axis',\n 'apply_over_axes',\n 'arange',\n 'arccos',\n 'arccosh',\n 'arcsin',\n 'arcsinh',\n 'arctan',\n 'arctan2',\n 'arctanh',\n 'argmax',\n 'argmin',\n 'argpartition',\n 'argsort',\n 'argwhere',\n 'around',\n 'array',\n 'array2string',\n 'array_equal',\n 'array_equiv',\n 'array_repr',\n 'array_split',\n 'array_str',\n 'asanyarray',\n 'asarray',\n 'asarray_chkfinite',\n 'ascontiguousarray',\n 'asfarray',\n 'asfortranarray',\n 'asmatrix',\n 'asscalar',\n 'atleast_1d',\n 'atleast_2d',\n 'atleast_3d',\n 'average',\n 'bartlett',\n 'base_repr',\n 'bench',\n 'binary_repr',\n 'bincount',\n 'bitwise_and',\n 'bitwise_not',\n 'bitwise_or',\n 'bitwise_xor',\n 'blackman',\n 'block',\n 'bmat',\n 'bool',\n 'bool8',\n 'bool_',\n 'broadcast',\n 'broadcast_arrays',\n 'broadcast_to',\n 'busday_count',\n 'busday_offset',\n 'busdaycalendar',\n 'byte',\n 'byte_bounds',\n 'bytes0',\n 'bytes_',\n 'c_',\n 'can_cast',\n 'cast',\n 'cbrt',\n 'cdouble',\n 'ceil',\n 'cfloat',\n 'char',\n 'character',\n 'chararray',\n 'choose',\n 'clip',\n 'clongdouble',\n 'clongfloat',\n 'column_stack',\n 'common_type',\n 'compare_chararrays',\n 'compat',\n 'complex',\n 'complex128',\n 'complex256',\n 'complex64',\n 'complex_',\n 'complexfloating',\n 'compress',\n 'concatenate',\n 'conj',\n 'conjugate',\n 'convolve',\n 'copy',\n 'copysign',\n 'copyto',\n 'core',\n 'corrcoef',\n 'correlate',\n 'cos',\n 'cosh',\n 'count_nonzero',\n 'cov',\n 'cross',\n 'csingle',\n 'ctypeslib',\n 'cumprod',\n 'cumproduct',\n 'cumsum',\n 'datetime64',\n 'datetime_as_string',\n 'datetime_data',\n 'deg2rad',\n 'degrees',\n 'delete',\n 'deprecate',\n 'deprecate_with_doc',\n 'diag',\n 'diag_indices',\n 'diag_indices_from',\n 'diagflat',\n 'diagonal',\n 'diff',\n 'digitize',\n 'disp',\n 'divide',\n 'division',\n 'divmod',\n 'dot',\n 'double',\n 'dsplit',\n 'dstack',\n 'dtype',\n 'e',\n 'ediff1d',\n 'einsum',\n 'einsum_path',\n 'emath',\n 'empty',\n 'empty_like',\n 'equal',\n 'errstate',\n 'euler_gamma',\n 'exp',\n 'exp2',\n 'expand_dims',\n 'expm1',\n 'extract',\n 'eye',\n 'fabs',\n 'fastCopyAndTranspose',\n 'fft',\n 'fill_diagonal',\n 'find_common_type',\n 'finfo',\n 'fix',\n 'flatiter',\n 'flatnonzero',\n 'flexible',\n 'flip',\n 'fliplr',\n 'flipud',\n 'float',\n 'float128',\n 'float16',\n 'float32',\n 'float64',\n 'float_',\n 'float_power',\n 'floating',\n 'floor',\n 'floor_divide',\n 'fmax',\n 'fmin',\n 'fmod',\n 'format_parser',\n 'frexp',\n 'frombuffer',\n 'fromfile',\n 'fromfunction',\n 'fromiter',\n 'frompyfunc',\n 'fromregex',\n 'fromstring',\n 'full',\n 'full_like',\n 'fv',\n 'generic',\n 'genfromtxt',\n 'geomspace',\n 'get_array_wrap',\n 'get_include',\n 'get_printoptions',\n 'getbufsize',\n 'geterr',\n 'geterrcall',\n 'geterrobj',\n 'gradient',\n 'greater',\n 'greater_equal',\n 'half',\n 'hamming',\n 'hanning',\n 'heaviside',\n 'histogram',\n 'histogram2d',\n 'histogramdd',\n 'hsplit',\n 'hstack',\n 'hypot',\n 'i0',\n 'identity',\n 'iinfo',\n 'imag',\n 'in1d',\n 'index_exp',\n 'indices',\n 'inexact',\n 'inf',\n 'info',\n 'infty',\n 'inner',\n 'insert',\n 'int',\n 'int0',\n 'int16',\n 'int32',\n 'int64',\n 'int8',\n 'int_',\n 'int_asbuffer',\n 'intc',\n 'integer',\n 'interp',\n 'intersect1d',\n 'intp',\n 'invert',\n 'ipmt',\n 'irr',\n 'is_busday',\n 'isclose',\n 'iscomplex',\n 'iscomplexobj',\n 'isfinite',\n 'isfortran',\n 'isin',\n 'isinf',\n 'isnan',\n 'isnat',\n 'isneginf',\n 'isposinf',\n 'isreal',\n 'isrealobj',\n 'isscalar',\n 'issctype',\n 'issubclass_',\n 'issubdtype',\n 'issubsctype',\n 'iterable',\n 'ix_',\n 'kaiser',\n 'kron',\n 'ldexp',\n 'left_shift',\n 'less',\n 'less_equal',\n 'lexsort',\n 'lib',\n 'linalg',\n 'linspace',\n 'little_endian',\n 'load',\n 'loads',\n 'loadtxt',\n 'log',\n 'log10',\n 'log1p',\n 'log2',\n 'logaddexp',\n 'logaddexp2',\n 'logical_and',\n 'logical_not',\n 'logical_or',\n 'logical_xor',\n 'logspace',\n 'long',\n 'longcomplex',\n 'longdouble',\n 'longfloat',\n 'longlong',\n 'lookfor',\n 'ma',\n 'mafromtxt',\n 'mask_indices',\n 'mat',\n 'math',\n 'matmul',\n 'matrix',\n 'matrixlib',\n 'max',\n 'maximum',\n 'maximum_sctype',\n 'may_share_memory',\n 'mean',\n 'median',\n 'memmap',\n 'meshgrid',\n 'mgrid',\n 'min',\n 'min_scalar_type',\n 'minimum',\n 'mintypecode',\n 'mirr',\n 'mod',\n 'modf',\n 'moveaxis',\n 'msort',\n 'multiply',\n 'nan',\n 'nan_to_num',\n 'nanargmax',\n 'nanargmin',\n 'nancumprod',\n 'nancumsum',\n 'nanmax',\n 'nanmean',\n 'nanmedian',\n 'nanmin',\n 'nanpercentile',\n 'nanprod',\n 'nanstd',\n 'nansum',\n 'nanvar',\n 'nbytes',\n 'ndarray',\n 'ndenumerate',\n 'ndfromtxt',\n 'ndim',\n 'ndindex',\n 'nditer',\n 'negative',\n 'nested_iters',\n 'newaxis',\n 'nextafter',\n 'nonzero',\n 'not_equal',\n 'nper',\n 'npv',\n 'numarray',\n 'number',\n 'obj2sctype',\n 'object',\n 'object0',\n 'object_',\n 'ogrid',\n 'oldnumeric',\n 'ones',\n 'ones_like',\n 'outer',\n 'packbits',\n 'pad',\n 'partition',\n 'percentile',\n 'pi',\n 'piecewise',\n 'pkgload',\n 'place',\n 'pmt',\n 'poly',\n 'poly1d',\n 'polyadd',\n 'polyder',\n 'polydiv',\n 'polyfit',\n 'polyint',\n 'polymul',\n 'polynomial',\n 'polysub',\n 'polyval',\n 'positive',\n 'power',\n 'ppmt',\n 'print_function',\n 'prod',\n 'product',\n 'promote_types',\n 'ptp',\n 'put',\n 'putmask',\n 'pv',\n 'r_',\n 'rad2deg',\n 'radians',\n 'random',\n 'rank',\n 'rate',\n 'ravel',\n 'ravel_multi_index',\n 'real',\n 'real_if_close',\n 'rec',\n 'recarray',\n 'recfromcsv',\n 'recfromtxt',\n 'reciprocal',\n 'record',\n 'remainder',\n 'repeat',\n 'require',\n 'reshape',\n 'resize',\n 'result_type',\n 'right_shift',\n 'rint',\n 'roll',\n 'rollaxis',\n 'roots',\n 'rot90',\n 'round',\n 'round_',\n 'row_stack',\n 's_',\n 'safe_eval',\n 'save',\n 'savetxt',\n 'savez',\n 'savez_compressed',\n 'sctype2char',\n 'sctypeDict',\n 'sctypeNA',\n 'sctypes',\n 'searchsorted',\n 'select',\n 'set_numeric_ops',\n 'set_printoptions',\n 'set_string_function',\n 'setbufsize',\n 'setdiff1d',\n 'seterr',\n 'seterrcall',\n 'seterrobj',\n 'setxor1d',\n 'shape',\n 'shares_memory',\n 'short',\n 'show_config',\n 'sign',\n 'signbit',\n 'signedinteger',\n 'sin',\n 'sinc',\n 'single',\n 'singlecomplex',\n 'sinh',\n 'size',\n 'sometrue',\n 'sort',\n 'sort_complex',\n 'source',\n 'spacing',\n 'split',\n 'sqrt',\n 'square',\n 'squeeze',\n 'stack',\n 'std',\n 'str',\n 'str0',\n 'str_',\n 'string_',\n 'subtract',\n 'sum',\n 'swapaxes',\n 'take',\n 'tan',\n 'tanh',\n 'tensordot',\n 'test',\n 'testing',\n 'tile',\n 'timedelta64',\n 'trace',\n 'tracemalloc_domain',\n 'transpose',\n 'trapz',\n 'tri',\n 'tril',\n 'tril_indices',\n 'tril_indices_from',\n 'trim_zeros',\n 'triu',\n 'triu_indices',\n 'triu_indices_from',\n 'true_divide',\n 'trunc',\n 'typeDict',\n 'typeNA',\n 'typecodes',\n 'typename',\n 'ubyte',\n 'ufunc',\n 'uint',\n 'uint0',\n 'uint16',\n 'uint32',\n 'uint64',\n 'uint8',\n 'uintc',\n 'uintp',\n 'ulonglong',\n 'unicode',\n 'unicode_',\n 'union1d',\n 'unique',\n 'unpackbits',\n 'unravel_index',\n 'unsignedinteger',\n 'unwrap',\n 'ushort',\n 'vander',\n 'var',\n 'vdot',\n 'vectorize',\n 'version',\n 'void',\n 'void0',\n 'vsplit',\n 'vstack',\n 'warnings',\n 'where',\n 'who',\n 'zeros',\n 'zeros_like']</pre> In\u00a0[3]: Copied! <pre># find out what version we have\nnp.__version__\n</pre> # find out what version we have np.__version__ Out[3]: <pre>'1.25.1'</pre> <p>The numpy documentation is crucial!</p> <p>http://docs.scipy.org/doc/numpy/reference/</p> In\u00a0[5]: Copied! <pre># create an array from a list\na = np.array([9,0,2,1,0])\n</pre> # create an array from a list a = np.array([9,0,2,1,0]) In\u00a0[6]: Copied! <pre># find out the datatype\na.dtype\n</pre> # find out the datatype a.dtype Out[6]: <pre>dtype('int64')</pre> In\u00a0[7]: Copied! <pre># find out the shape\na.shape\n</pre> # find out the shape a.shape Out[7]: <pre>(5,)</pre> In\u00a0[8]: Copied! <pre># what is the shape\ntype(a.shape)\n</pre> # what is the shape type(a.shape) Out[8]: <pre>tuple</pre> In\u00a0[9]: Copied! <pre># another array with a different datatype and shape\nb = np.array([[5,3,1,9],[9,2,3,0]], dtype=np.float64)\n</pre> # another array with a different datatype and shape b = np.array([[5,3,1,9],[9,2,3,0]], dtype=np.float64) In\u00a0[98]: Copied! <pre># array with 3 rows x 4 columns\na_2d = np.array([[3,2,0,1],[9,1,8,7],[4,0,1,6]]) \na_2d\n</pre> # array with 3 rows x 4 columns a_2d = np.array([[3,2,0,1],[9,1,8,7],[4,0,1,6]])  a_2d Out[98]: <pre>array([[3, 2, 0, 1],\n       [9, 1, 8, 7],\n       [4, 0, 1, 6]])</pre> In\u00a0[99]: Copied! <pre># check dtype and shape\nb.dtype, b.shape\n</pre> # check dtype and shape b.dtype, b.shape Out[99]: <pre>(dtype('float64'), (2, 4))</pre> <p>Important Concept: The fastest varying dimension is the last dimension! The outer level of the hierarchy is the first dimension. (This is called \"c-style\" indexing)</p> In\u00a0[96]: Copied! <pre># create some uniform arrays\nc = np.zeros((9,9))\nd = np.ones((3,6,3), dtype=np.complex128)\ne = np.full((3,3), np.pi)\ne = np.ones_like(c)\nf = np.zeros_like(d)\n# \ng = np.random.rand(3,4)\n</pre> # create some uniform arrays c = np.zeros((9,9)) d = np.ones((3,6,3), dtype=np.complex128) e = np.full((3,3), np.pi) e = np.ones_like(c) f = np.zeros_like(d) #  g = np.random.rand(3,4) <p>The <code>np.arange()</code> function is used to generate an array with evenly spaced values within a given interval. <code>np.arange()</code> can be used with one, two, or three parameters to specify the start, stop, and step values. If only one value is passed to the function, it will be interpreted as the stop value:</p> In\u00a0[12]: Copied! <pre># create some ranges\nnp.arange(10)\n</pre> # create some ranges np.arange(10) Out[12]: <pre>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre> In\u00a0[13]: Copied! <pre># arange is left inclusive, right exclusive\nnp.arange(2,4,0.25)\n</pre> # arange is left inclusive, right exclusive np.arange(2,4,0.25) Out[13]: <pre>array([2.  , 2.25, 2.5 , 2.75, 3.  , 3.25, 3.5 , 3.75])</pre> <p>Similarly, the <code>np.linspace()</code> function is used to construct an array with evenly spaced numbers over a given interval. However, instead of the step parameter, <code>np.linspace()</code> takes a num parameter to specify the number of samples within the given interval:</p> In\u00a0[14]: Copied! <pre># linearly spaced\nnp.linspace(2,4,20)\n</pre> # linearly spaced np.linspace(2,4,20) Out[14]: <pre>array([2.        , 2.10526316, 2.21052632, 2.31578947, 2.42105263,\n       2.52631579, 2.63157895, 2.73684211, 2.84210526, 2.94736842,\n       3.05263158, 3.15789474, 3.26315789, 3.36842105, 3.47368421,\n       3.57894737, 3.68421053, 3.78947368, 3.89473684, 4.        ])</pre> <p>Note that unlike <code>np.arange()</code>, <code>np.linspace()</code> includes the stop value by default (this can be changed by passing <code>endpoint=True</code>). Finally, it should be noted that while we could have used <code>np.arange()</code> to generate the same array in the above example, it is recommended to use <code>np.linspace()</code> when a non-integer step (e.g. 0.25) is desired.</p> In\u00a0[87]: Copied! <pre>np.linspace(2,4,20, endpoint = False)\n</pre> np.linspace(2,4,20, endpoint = False) Out[87]: <pre>array([2. , 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3. , 3.1, 3.2,\n       3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9])</pre> In\u00a0[3]: Copied! <pre>x = np.linspace(-4, 4, 9)\n \ny = np.linspace(-5, 5, 11)\n \nx_2d, y_2d = np.meshgrid(x, y)\n</pre> x = np.linspace(-4, 4, 9)   y = np.linspace(-5, 5, 11)   x_2d, y_2d = np.meshgrid(x, y) In\u00a0[4]: Copied! <pre>x_2d\n</pre> x_2d Out[4]: <pre>array([[-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.]])</pre> In\u00a0[5]: Copied! <pre>y_2d\n</pre> y_2d Out[5]: <pre>array([[-5., -5., -5., -5., -5., -5., -5., -5., -5.],\n       [-4., -4., -4., -4., -4., -4., -4., -4., -4.],\n       [-3., -3., -3., -3., -3., -3., -3., -3., -3.],\n       [-2., -2., -2., -2., -2., -2., -2., -2., -2.],\n       [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.],\n       [ 4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.],\n       [ 5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.]])</pre> In\u00a0[7]: Copied! <pre># get some individual elements of xx\nx_2d[0,0], x_2d[-1,-1], x_2d[3,-5]\n</pre> # get some individual elements of xx x_2d[0,0], x_2d[-1,-1], x_2d[3,-5] Out[7]: <pre>(-4.0, 4.0, 0.0)</pre> In\u00a0[8]: Copied! <pre># get some whole rows and columns\nx_2d[0].shape, x_2d[:,-1].shape\n</pre> # get some whole rows and columns x_2d[0].shape, x_2d[:,-1].shape Out[8]: <pre>((9,), (11,))</pre> In\u00a0[9]: Copied! <pre># get some ranges\nx_2d[3:10,3:5].shape\n</pre> # get some ranges x_2d[3:10,3:5].shape Out[9]: <pre>(7, 2)</pre> <p>There are many advanced ways to index arrays. You can read about them in the manual. Here is one example.</p> In\u00a0[10]: Copied! <pre># use a boolean array as an index\nidx = x_2d&lt;0\nx_2d[idx].shape\n</pre> # use a boolean array as an index idx = x_2d&lt;0 x_2d[idx].shape Out[10]: <pre>(44,)</pre> In\u00a0[11]: Copied! <pre># two dimensional grids\nx = np.linspace(-2*np.pi, 2*np.pi, 100)\ny = np.linspace(-np.pi, np.pi, 50)\nxx, yy = np.meshgrid(x, y)\nxx.shape, yy.shape\n</pre> # two dimensional grids x = np.linspace(-2*np.pi, 2*np.pi, 100) y = np.linspace(-np.pi, np.pi, 50) xx, yy = np.meshgrid(x, y) xx.shape, yy.shape Out[11]: <pre>((50, 100), (50, 100))</pre> In\u00a0[12]: Copied! <pre>f = np.sin(xx) * np.cos(0.5*yy)\n</pre> f = np.sin(xx) * np.cos(0.5*yy) <p>At this point you might be getting curious what these arrays \"look\" like. So we need to introduce some visualization.</p> In\u00a0[7]: Copied! <pre>from matplotlib import pyplot as plt\n# %matplotlib inline\n</pre> from matplotlib import pyplot as plt # %matplotlib inline In\u00a0[14]: Copied! <pre>plt.pcolormesh(f)\n</pre> plt.pcolormesh(f) Out[14]: <pre>&lt;matplotlib.collections.QuadMesh at 0x1143ebc10&gt;</pre> In\u00a0[15]: Copied! <pre># transpose\nplt.pcolormesh(f.T)\n</pre> # transpose plt.pcolormesh(f.T) Out[15]: <pre>&lt;matplotlib.collections.QuadMesh at 0x11457d760&gt;</pre> In\u00a0[16]: Copied! <pre># Flip the array up/down (reverse the order of the rows)\nplt.pcolormesh(np.flipud(f))\n</pre> # Flip the array up/down (reverse the order of the rows) plt.pcolormesh(np.flipud(f))  Out[16]: <pre>&lt;matplotlib.collections.QuadMesh at 0x1145e6e80&gt;</pre> In\u00a0[17]: Copied! <pre># reshape an array (wrong size)\ng = np.reshape(f, (8,9))\n</pre> # reshape an array (wrong size) g = np.reshape(f, (8,9)) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[17], line 2\n      1 # reshape an array (wrong size)\n----&gt; 2 g = np.reshape(f, (8,9))\n\nFile /opt/anaconda3/envs/research_computing_scipy/lib/python3.9/site-packages/numpy/core/fromnumeric.py:285, in reshape(a, newshape, order)\n    200 @array_function_dispatch(_reshape_dispatcher)\n    201 def reshape(a, newshape, order='C'):\n    202 \"\"\"\n    203     Gives a new shape to an array without changing its data.\n    204 \n   (...)\n    283            [5, 6]])\n    284     \"\"\"\n--&gt; 285     return _wrapfunc(a, 'reshape', newshape, order=order)\n\nFile /opt/anaconda3/envs/research_computing_scipy/lib/python3.9/site-packages/numpy/core/fromnumeric.py:59, in _wrapfunc(obj, method, *args, **kwds)\n     56     return _wrapit(obj, method, *args, **kwds)\n     58 try:\n---&gt; 59     return bound(*args, **kwds)\n     60 except TypeError:\n     61     # A TypeError occurs if the object does have such a method in its\n     62     # class, but its signature is not identical to that of NumPy's. This\n   (...)\n     66     # Call _wrapit from within the except clause to ensure a potential\n     67     # exception has a traceback chain.\n     68     return _wrapit(obj, method, *args, **kwds)\n\nValueError: cannot reshape array of size 5000 into shape (8,9)</pre> In\u00a0[18]: Copied! <pre># reshape an array (right size) and mess it up\nprint(f.size)\ng = np.reshape(f, (200,25))\nplt.pcolormesh(g)\n</pre> # reshape an array (right size) and mess it up print(f.size) g = np.reshape(f, (200,25)) plt.pcolormesh(g) <pre>5000\n</pre> Out[18]: <pre>&lt;matplotlib.collections.QuadMesh at 0x1147de760&gt;</pre> In\u00a0[19]: Copied! <pre>f.shape\n</pre> f.shape Out[19]: <pre>(50, 100)</pre> In\u00a0[20]: Copied! <pre>np.tile(f,(6,1)).shape\n</pre> np.tile(f,(6,1)).shape Out[20]: <pre>(300, 100)</pre> In\u00a0[21]: Copied! <pre># tile an array\nplt.pcolormesh(np.tile(f,(6,1)))\n</pre> # tile an array plt.pcolormesh(np.tile(f,(6,1))) Out[21]: <pre>&lt;matplotlib.collections.QuadMesh at 0x114948a60&gt;</pre> In\u00a0[23]: Copied! <pre>from IPython.display import Image\nImage(url='http://scipy-lectures.github.io/_images/numpy_broadcasting.png',\n     width=720)\n</pre> from IPython.display import Image Image(url='http://scipy-lectures.github.io/_images/numpy_broadcasting.png',      width=720) Out[23]: In\u00a0[24]: Copied! <pre># multiply f by x\nprint(f.shape, x.shape)\ng = f * x\nprint(g.shape)\n</pre> # multiply f by x print(f.shape, x.shape) g = f * x print(g.shape) <pre>(50, 100) (100,)\n(50, 100)\n</pre> In\u00a0[25]: Copied! <pre># multiply f by y\nprint(f.shape, y.shape)\nh = f * y\nprint(h.shape)\n</pre> # multiply f by y print(f.shape, y.shape) h = f * y print(h.shape) <pre>(50, 100) (50,)\n</pre> <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[25], line 3\n      1 # multiply f by y\n      2 print(f.shape, y.shape)\n----&gt; 3 h = f * y\n      4 print(h.shape)\n\nValueError: operands could not be broadcast together with shapes (50,100) (50,) </pre> In\u00a0[26]: Copied! <pre># use newaxis special syntax\nh = f * y[:,np.newaxis]\nprint(h.shape)\n</pre> # use newaxis special syntax h = f * y[:,np.newaxis] print(h.shape) <pre>(50, 100)\n</pre> In\u00a0[27]: Copied! <pre># sum\ng.sum()\n</pre> # sum g.sum() Out[27]: <pre>-3083.038387807155</pre> In\u00a0[28]: Copied! <pre># mean\ng.mean()\n</pre> # mean g.mean() Out[28]: <pre>-0.616607677561431</pre> In\u00a0[29]: Copied! <pre># std\ng.std()\n</pre> # std g.std() Out[29]: <pre>1.6402280119141424</pre> In\u00a0[30]: Copied! <pre># apply on just one axis\n\n# Mean of each row (calculated across columns)\ng_xmean = g.mean(axis=1)\n\n# Mean of each column (calculated across rows)\n\ng_ymean = g.mean(axis=0)\n</pre> # apply on just one axis  # Mean of each row (calculated across columns) g_xmean = g.mean(axis=1)  # Mean of each column (calculated across rows)  g_ymean = g.mean(axis=0) In\u00a0[31]: Copied! <pre>plt.plot(x, g_ymean)\n</pre> plt.plot(x, g_ymean) Out[31]: <pre>[&lt;matplotlib.lines.Line2D at 0x114a38880&gt;]</pre> In\u00a0[32]: Copied! <pre>plt.plot(g_xmean, y)\n</pre> plt.plot(g_xmean, y) Out[32]: <pre>[&lt;matplotlib.lines.Line2D at 0x114a8e6a0&gt;]</pre> <p>Most real-world datasets \u2013 environmental or otherwise \u2013 have data gaps. Data can be missing for any number of reasons, including observations not being recorded or data corruption. While a cell corresponding to a data gap may just be left blank in a spreadsheet, when imported into Python, there must be some way to handle \"blank\" or missing values.</p> <p>Missing data should not be replaced with zeros, as 0 can be a valid value for many datasets, (e.g. temperature, precipitation, etc.). Instead, the convention is to fill all missing data with the constant NaN. NaN stands for \"Not a Number\" and is implemented in NumPy as np.nan.</p> <p>NaNs are handled differently by different packages. In NumPy, all computations involving NaN values will return nan:</p> In\u00a0[33]: Copied! <pre>data = np.array([[2.,2.7,1.89],\n                 [1.1, 0.0, np.nan],\n                 [3.2, 0.74, 2.1]])\n</pre> data = np.array([[2.,2.7,1.89],                  [1.1, 0.0, np.nan],                  [3.2, 0.74, 2.1]]) In\u00a0[34]: Copied! <pre>np.mean(data)\n</pre> np.mean(data) Out[34]: <pre>nan</pre> In\u00a0[35]: Copied! <pre>np.nanmean(data)\n</pre> np.nanmean(data) Out[35]: <pre>1.71625</pre> In\u00a0[36]: Copied! <pre>fig = plt.figure()\n</pre> fig = plt.figure() <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre> In\u00a0[37]: Copied! <pre>fig = plt.figure(figsize=(13, 5))\n</pre> fig = plt.figure(figsize=(13, 5)) <pre>&lt;Figure size 1300x500 with 0 Axes&gt;</pre> In\u00a0[38]: Copied! <pre>fig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1])\n</pre> fig = plt.figure() ax = fig.add_axes([0, 0, 1, 1]) In\u00a0[39]: Copied! <pre>fig = plt.figure()\nax = fig.add_axes([0, 0, 0.5, 1])\n</pre> fig = plt.figure() ax = fig.add_axes([0, 0, 0.5, 1]) In\u00a0[40]: Copied! <pre>fig = plt.figure()\nax1 = fig.add_axes([0, 0, 0.5, 1])\nax2 = fig.add_axes([0.6, 0, 0.3, 0.5], facecolor='g')\n</pre> fig = plt.figure() ax1 = fig.add_axes([0, 0, 0.5, 1]) ax2 = fig.add_axes([0.6, 0, 0.3, 0.5], facecolor='g') In\u00a0[41]: Copied! <pre>fig = plt.figure()\naxes = fig.subplots(nrows=2, ncols=3)\n</pre> fig = plt.figure() axes = fig.subplots(nrows=2, ncols=3) In\u00a0[43]: Copied! <pre>fig = plt.figure(figsize=(12, 6))\naxes = fig.subplots(nrows=2, ncols=3)\n</pre> fig = plt.figure(figsize=(12, 6)) axes = fig.subplots(nrows=2, ncols=3) In\u00a0[44]: Copied! <pre>axes\n</pre> axes Out[44]: <pre>array([[&lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;]], dtype=object)</pre> <p>There is a shorthand for doing this all at once.</p> <p>This is our recommended way to create new figures!</p> In\u00a0[45]: Copied! <pre>fig, ax = plt.subplots()\n</pre> fig, ax = plt.subplots() In\u00a0[46]: Copied! <pre>ax\n</pre> ax Out[46]: <pre>&lt;Axes: &gt;</pre> In\u00a0[47]: Copied! <pre>fig, axes = plt.subplots(ncols=2, figsize=(8, 4), subplot_kw={'facecolor': 'g'})\n</pre> fig, axes = plt.subplots(ncols=2, figsize=(8, 4), subplot_kw={'facecolor': 'g'}) In\u00a0[48]: Copied! <pre>axes\n</pre> axes Out[48]: <pre>array([&lt;Axes: &gt;, &lt;Axes: &gt;], dtype=object)</pre> In\u00a0[49]: Copied! <pre># create some data to plot\nimport numpy as np\nx = np.linspace(-np.pi, np.pi, 100)\ny = np.cos(x)\nz = np.sin(6*x)\n</pre> # create some data to plot import numpy as np x = np.linspace(-np.pi, np.pi, 100) y = np.cos(x) z = np.sin(6*x) In\u00a0[50]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(x, y)\n</pre> fig, ax = plt.subplots() ax.plot(x, y) Out[50]: <pre>[&lt;matplotlib.lines.Line2D at 0x114ca27c0&gt;]</pre> <p>This does the same thing as</p> In\u00a0[51]: Copied! <pre>plt.plot(x, y)\n</pre> plt.plot(x, y) Out[51]: <pre>[&lt;matplotlib.lines.Line2D at 0x173539e80&gt;]</pre> <p>This starts to matter when we have multiple axes to worry about.</p> In\u00a0[52]: Copied! <pre>fig, axes = plt.subplots(figsize=(8, 4), ncols=2)\nax0, ax1 = axes\nax0.plot(x, y)\nax1.plot(x, z)\n</pre> fig, axes = plt.subplots(figsize=(8, 4), ncols=2) ax0, ax1 = axes ax0.plot(x, y) ax1.plot(x, z) Out[52]: <pre>[&lt;matplotlib.lines.Line2D at 0x1736068e0&gt;]</pre> In\u00a0[53]: Copied! <pre>fig, axes = plt.subplots(figsize=(8, 4), ncols=2)\nax0, ax1 = axes\n\nax0.plot(x, y)\nax0.set_xlabel('x')\nax0.set_ylabel('y')\nax0.set_title('x vs. y')\n\nax1.plot(x, z)\nax1.set_xlabel('x')\nax1.set_ylabel('z')\nax1.set_title('x vs. z')\n\n# squeeze everything in\nplt.tight_layout()\n</pre> fig, axes = plt.subplots(figsize=(8, 4), ncols=2) ax0, ax1 = axes  ax0.plot(x, y) ax0.set_xlabel('x') ax0.set_ylabel('y') ax0.set_title('x vs. y')  ax1.plot(x, z) ax1.set_xlabel('x') ax1.set_ylabel('z') ax1.set_title('x vs. z')  # squeeze everything in plt.tight_layout() In\u00a0[54]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(x, y, x, z)\n</pre> fig, ax = plt.subplots() ax.plot(x, y, x, z) Out[54]: <pre>[&lt;matplotlib.lines.Line2D at 0x1737b42e0&gt;,\n &lt;matplotlib.lines.Line2D at 0x1737b4340&gt;]</pre> <p>It's simple to switch axes</p> In\u00a0[55]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(y, x, z, x)\n</pre> fig, ax = plt.subplots() ax.plot(y, x, z, x) Out[55]: <pre>[&lt;matplotlib.lines.Line2D at 0x173839670&gt;,\n &lt;matplotlib.lines.Line2D at 0x1738396d0&gt;]</pre> In\u00a0[56]: Copied! <pre>fig, axes = plt.subplots(figsize=(16, 5), ncols=3)\naxes[0].plot(x, y, linestyle='dashed')\naxes[0].plot(x, z, linestyle='--')\n\naxes[1].plot(x, y, linestyle='dotted')\naxes[1].plot(x, z, linestyle=':')\n\naxes[2].plot(x, y, linestyle='dashdot', linewidth=5)\naxes[2].plot(x, z, linestyle='-.', linewidth=0.5)\n</pre> fig, axes = plt.subplots(figsize=(16, 5), ncols=3) axes[0].plot(x, y, linestyle='dashed') axes[0].plot(x, z, linestyle='--')  axes[1].plot(x, y, linestyle='dotted') axes[1].plot(x, z, linestyle=':')  axes[2].plot(x, y, linestyle='dashdot', linewidth=5) axes[2].plot(x, z, linestyle='-.', linewidth=0.5)  Out[56]: <pre>[&lt;matplotlib.lines.Line2D at 0x1739476d0&gt;]</pre> In\u00a0[57]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(x, y, color='k')\nax.plot(x, z, color='r')\n</pre> fig, ax = plt.subplots() ax.plot(x, y, color='k') ax.plot(x, z, color='r') Out[57]: <pre>[&lt;matplotlib.lines.Line2D at 0x173a47850&gt;]</pre> <p>Other ways to specify colors:</p> In\u00a0[58]: Copied! <pre>fig, axes = plt.subplots(figsize=(16, 5), ncols=3)\n\n# grayscale\naxes[0].plot(x, y, color='0.8')\naxes[0].plot(x, z, color='0.2')\n\n# RGB tuple\naxes[1].plot(x, y, color=(1, 0, 0.7))\naxes[1].plot(x, z, color=(0, 0.4, 0.3))\n\n# HTML hex code\naxes[2].plot(x, y, color='#00dcba')\naxes[2].plot(x, z, color='#b029ee')\n</pre> fig, axes = plt.subplots(figsize=(16, 5), ncols=3)  # grayscale axes[0].plot(x, y, color='0.8') axes[0].plot(x, z, color='0.2')  # RGB tuple axes[1].plot(x, y, color=(1, 0, 0.7)) axes[1].plot(x, z, color=(0, 0.4, 0.3))  # HTML hex code axes[2].plot(x, y, color='#00dcba') axes[2].plot(x, z, color='#b029ee') Out[58]: <pre>[&lt;matplotlib.lines.Line2D at 0x173b536d0&gt;]</pre> <p>There is a default color cycle built into matplotlib.</p> In\u00a0[59]: Copied! <pre>plt.rcParams['axes.prop_cycle']\n</pre> plt.rcParams['axes.prop_cycle'] Out[59]: 'color''#1f77b4''#ff7f0e''#2ca02c''#d62728''#9467bd''#8c564b''#e377c2''#7f7f7f''#bcbd22''#17becf' In\u00a0[60]: Copied! <pre>fig, ax = plt.subplots(figsize=(12, 10))\nfor factor in np.linspace(0.2, 1, 11):\n    ax.plot(x, factor*y)\n</pre> fig, ax = plt.subplots(figsize=(12, 10)) for factor in np.linspace(0.2, 1, 11):     ax.plot(x, factor*y) In\u00a0[61]: Copied! <pre>fig, axes = plt.subplots(figsize=(12, 5), ncols=2)\n\naxes[0].plot(x[:20], y[:20], marker='.')\naxes[0].plot(x[:20], z[:20], marker='o')\n\naxes[1].plot(x[:20], z[:20], marker='^',\n             markersize=10, markerfacecolor='r',\n             markeredgecolor='k')\n</pre> fig, axes = plt.subplots(figsize=(12, 5), ncols=2)  axes[0].plot(x[:20], y[:20], marker='.') axes[0].plot(x[:20], z[:20], marker='o')  axes[1].plot(x[:20], z[:20], marker='^',              markersize=10, markerfacecolor='r',              markeredgecolor='k') Out[61]: <pre>[&lt;matplotlib.lines.Line2D at 0x114e7e310&gt;]</pre> In\u00a0[73]: Copied! <pre>fig, ax = plt.subplots(figsize=(12, 7))\nax.plot(x, y)\n\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('A complicated math function: $f(x) = \\cos(x)$')\n\nax.set_xticks(np.pi * np.array([-1, 0, 1]))\nax.set_xticklabels(['$-\\pi$', '0', '$\\pi$'])\nax.set_yticks([-1, 0, 1])\n\nax.set_yticks(np.arange(-1, 1.1, 0.2), minor=True)\n#ax.set_xticks(np.arange(-3, 3.1, 0.2), minor=True)\n\nax.grid(which='minor', linestyle='--')\nax.grid(which='major', linewidth=2)\n</pre> fig, ax = plt.subplots(figsize=(12, 7)) ax.plot(x, y)  ax.set_xlabel('x') ax.set_ylabel('y') ax.set_title('A complicated math function: $f(x) = \\cos(x)$')  ax.set_xticks(np.pi * np.array([-1, 0, 1])) ax.set_xticklabels(['$-\\pi$', '0', '$\\pi$']) ax.set_yticks([-1, 0, 1])  ax.set_yticks(np.arange(-1, 1.1, 0.2), minor=True) #ax.set_xticks(np.arange(-3, 3.1, 0.2), minor=True)  ax.grid(which='minor', linestyle='--') ax.grid(which='major', linewidth=2)  In\u00a0[63]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(x, y, x, z)\nax.set_xlim(-5, 5)\nax.set_ylim(-3, 3)\n</pre> fig, ax = plt.subplots() ax.plot(x, y, x, z) ax.set_xlim(-5, 5) ax.set_ylim(-3, 3) Out[63]: <pre>(-3.0, 3.0)</pre> In\u00a0[64]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(x, y, x, z)\nax.set_xlim(-5, 5)\nax.set_ylim(-100, 100)\n</pre> fig, ax = plt.subplots() ax.plot(x, y, x, z) ax.set_xlim(-5, 5) ax.set_ylim(-100, 100) Out[64]: <pre>(-100.0, 100.0)</pre> In\u00a0[65]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(x, y)\nax.text(-3, 0.3, 'hello world')\nax.annotate('the maximum', xy=(0, 1),\n             xytext=(0, 0), arrowprops={'facecolor': 'k'})\n</pre> fig, ax = plt.subplots() ax.plot(x, y) ax.text(-3, 0.3, 'hello world') ax.annotate('the maximum', xy=(0, 1),              xytext=(0, 0), arrowprops={'facecolor': 'k'}) Out[65]: <pre>Text(0, 0, 'the maximum')</pre> In\u00a0[78]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(x, y)\nax.text(0.1, 0.9, 'hello world', transform=ax.transAxes)\nax.annotate('the maximum', xy=(0, 1),\n             xytext=(0, 0), arrowprops={'facecolor': 'k'})\n</pre> fig, ax = plt.subplots() ax.plot(x, y) ax.text(0.1, 0.9, 'hello world', transform=ax.transAxes) ax.annotate('the maximum', xy=(0, 1),              xytext=(0, 0), arrowprops={'facecolor': 'k'}) Out[78]: <pre>Text(0, 0, 'the maximum')</pre> In\u00a0[66]: Copied! <pre>fig, ax = plt.subplots()\n\nsplot = ax.scatter(y, z, c=x, s=(100*z**2 + 5))\nfig.colorbar(splot)\n</pre> fig, ax = plt.subplots()  splot = ax.scatter(y, z, c=x, s=(100*z**2 + 5)) fig.colorbar(splot) Out[66]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x173e08af0&gt;</pre> In\u00a0[67]: Copied! <pre>labels = ['first', 'second', 'third']\nvalues = [10, 5, 30]\n\nfig, axes = plt.subplots(figsize=(10, 5), ncols=2)\naxes[0].bar(labels, values)\naxes[1].barh(labels, values)\n</pre> labels = ['first', 'second', 'third'] values = [10, 5, 30]  fig, axes = plt.subplots(figsize=(10, 5), ncols=2) axes[0].bar(labels, values) axes[1].barh(labels, values) Out[67]: <pre>&lt;BarContainer object of 3 artists&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[5]: Copied! <pre>x1d = np.linspace(-2*np.pi, 2*np.pi, 100)\ny1d = np.linspace(-np.pi, np.pi, 50)\nxx, yy = np.meshgrid(x1d, y1d)\nf = np.cos(xx) * np.sin(yy)\nprint(f.shape)\n</pre> x1d = np.linspace(-2*np.pi, 2*np.pi, 100) y1d = np.linspace(-np.pi, np.pi, 50) xx, yy = np.meshgrid(x1d, y1d) f = np.cos(xx) * np.sin(yy) print(f.shape) <pre>(50, 100)\n</pre> In\u00a0[8]: Copied! <pre>fig, ax = plt.subplots(figsize=(12,4), ncols=2)\nax[0].imshow(f)\nax[1].imshow(f, origin='lower')\n</pre> fig, ax = plt.subplots(figsize=(12,4), ncols=2) ax[0].imshow(f) ax[1].imshow(f, origin='lower') Out[8]: <pre>&lt;matplotlib.image.AxesImage at 0x10c79f3d0&gt;</pre> In\u00a0[9]: Copied! <pre>fig, ax = plt.subplots(ncols=2, figsize=(12, 5))\npc0 = ax[0].pcolormesh(x1d, y1d, f)\npc1 = ax[1].pcolormesh(xx, yy, f)\nfig.colorbar(pc0, ax=ax[0])\nfig.colorbar(pc1, ax=ax[1])\n</pre> fig, ax = plt.subplots(ncols=2, figsize=(12, 5)) pc0 = ax[0].pcolormesh(x1d, y1d, f) pc1 = ax[1].pcolormesh(xx, yy, f) fig.colorbar(pc0, ax=ax[0]) fig.colorbar(pc1, ax=ax[1])  Out[9]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x10c9d2e20&gt;</pre> In\u00a0[13]: Copied! <pre>x_sm, y_sm, f_sm = xx[:10, :10], yy[:10, :10], f[:10, :10]\n\nfig, ax = plt.subplots(figsize=(12,5), ncols=2)\n\n# last row and column ignored!\nax[0].pcolormesh(x_sm, y_sm, f_sm, edgecolors='k', shading = 'nearest')\n\n# same!\nax[1].pcolormesh(x_sm, y_sm, f_sm[:-1, :-1], edgecolors='k', shading = 'flat')\n</pre> x_sm, y_sm, f_sm = xx[:10, :10], yy[:10, :10], f[:10, :10]  fig, ax = plt.subplots(figsize=(12,5), ncols=2)  # last row and column ignored! ax[0].pcolormesh(x_sm, y_sm, f_sm, edgecolors='k', shading = 'nearest')  # same! ax[1].pcolormesh(x_sm, y_sm, f_sm[:-1, :-1], edgecolors='k', shading = 'flat')  Out[13]: <pre>&lt;matplotlib.collections.QuadMesh at 0x10d8555b0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Lecture_4_numpy_and_matplotlib/#lecture-4-numpy-and-matplotlib","title":"Lecture 4: Numpy and Matplotlib\u00b6","text":"<p>These are two of the most fundamental parts of the scientific python \"ecosystem\". Most everything else is built on top of them.</p>"},{"location":"Lecture_4_numpy_and_matplotlib/#to-install-matplotlib","title":"To install Matplotlib:\u00b6","text":"<p>Open a Terminal window, activate rcaes_env: </p> <p>conda activate rcaes_env  conda install -c conda-forge matplotlib </p>"},{"location":"Lecture_4_numpy_and_matplotlib/#numpy","title":"Numpy\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#ndarrays","title":"NDArrays\u00b6","text":"<p>The core class is the numpy ndarray (n-dimensional array). The n-dimensional array object in NumPy is referred to as an ndarray, a multidimensional container of homogeneous items \u2013 i.e. all values in the array are the same type and size. These arrays can be one-dimensional (one row or column vector), two-dimensional (m rows x n columns), or three-dimensional (arrays within arrays).</p>"},{"location":"Lecture_4_numpy_and_matplotlib/#create-array-from-a-list","title":"Create array from a list\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#create-arrays-using-functions","title":"Create arrays using functions\u00b6","text":"<p>There are lots of ways to create arrays.</p>"},{"location":"Lecture_4_numpy_and_matplotlib/#create-two-dimensional-grids","title":"Create two-dimensional grids\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#indexing","title":"Indexing\u00b6","text":"<p>Basic indexing is similar to lists</p>"},{"location":"Lecture_4_numpy_and_matplotlib/#array-operations","title":"Array Operations\u00b6","text":"<p>There are a huge number of operations available on arrays. All the familiar arithemtic operators are applied on an element-by-element basis.</p>"},{"location":"Lecture_4_numpy_and_matplotlib/#basic-math","title":"Basic Math\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#manipulating-array-dimensions","title":"Manipulating array dimensions\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#broadcasting","title":"Broadcasting\u00b6","text":"<p>Broadcasting is an efficient way to multiply arrays of different sizes</p>"},{"location":"Lecture_4_numpy_and_matplotlib/#reduction-operations","title":"Reduction Operations\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#missing-data","title":"Missing data\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#more-matplotlib","title":"More Matplotlib\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#figure-and-axes","title":"Figure and Axes\u00b6","text":"<p>The figure is the highest level of organization of matplotlib objects. If we want, we can create a figure explicitly.</p>"},{"location":"Lecture_4_numpy_and_matplotlib/#subplots","title":"Subplots\u00b6","text":"<p>Subplot syntax is one way to specify the creation of multiple axes.</p>"},{"location":"Lecture_4_numpy_and_matplotlib/#drawing-into-axes","title":"Drawing into Axes\u00b6","text":"<p>All plots are drawn into axes. It is easiest to understand how matplotlib works if you use the object-oriented style.</p>"},{"location":"Lecture_4_numpy_and_matplotlib/#labeling-plots","title":"Labeling Plots\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#customizing-line-plots","title":"Customizing Line Plots\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#line-styles","title":"Line Styles\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#colors","title":"Colors\u00b6","text":"<p>As described in the colors documentation, there are some special codes for commonly used colors:</p> <ul> <li>b: blue</li> <li>g: green</li> <li>r: red</li> <li>c: cyan</li> <li>m: magenta</li> <li>y: yellow</li> <li>k: black</li> <li>w: white</li> </ul>"},{"location":"Lecture_4_numpy_and_matplotlib/#markers","title":"Markers\u00b6","text":"<p>There are lots of different markers availabile in matplotlib!</p>"},{"location":"Lecture_4_numpy_and_matplotlib/#label-ticks-and-gridlines","title":"Label, Ticks, and Gridlines\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#axis-limits","title":"Axis Limits\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#text-annotations","title":"Text Annotations\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#other-1d-plots","title":"Other 1D Plots\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#scatter-plots","title":"Scatter Plots\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#bar-plots","title":"Bar Plots\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#2d-plotting-methods","title":"2D Plotting Methods\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#imshow","title":"imshow\u00b6","text":""},{"location":"Lecture_4_numpy_and_matplotlib/#pcolormesh","title":"pcolormesh\u00b6","text":""},{"location":"Lecture_6_Pandas_Basics/","title":"Lecture 6: Pandas Basics","text":"<p>Let's start by importing pandas library</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n</pre> import pandas as pd import numpy as np from matplotlib import pyplot as plt In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>A Series represents a one-dimensional array of data. The main difference between a Series and numpy array is that a Series has an index. The index contains the labels that we use to access the data.</p> <p>There are many ways to create a Series. We will just show a few.</p> In\u00a0[16]: Copied! <pre># Create a series from list \nnames = ['Xiaomeng', 'Siyi','Siyi', 'Matt']\nvalues = [32, 25,26, 22]\nages = pd.Series(values, index=names)\nages\n</pre> # Create a series from list  names = ['Xiaomeng', 'Siyi','Siyi', 'Matt'] values = [32, 25,26, 22] ages = pd.Series(values, index=names) ages Out[16]: <pre>Xiaomeng    32\nSiyi        25\nSiyi        26\nMatt        22\ndtype: int64</pre> In\u00a0[17]: Copied! <pre>ages.plot(kind='bar')\n</pre> ages.plot(kind='bar') Out[17]: <pre>&lt;Axes: &gt;</pre> <p>Arithmetic operations and most numpy function can be applied to Series.</p> <p>An important point is that the Series keep their index during such operations.</p> In\u00a0[19]: Copied! <pre>np.sqrt(ages)\n</pre> np.sqrt(ages) Out[19]: <pre>Xiaomeng    5.656854\nSiyi        5.000000\nSiyi        5.099020\nMatt        4.690416\ndtype: float64</pre> <p>We can access the underlying index object if we need to:</p> In\u00a0[7]: Copied! <pre>ages.index\n</pre> ages.index Out[7]: <pre>Index(['Xiaomeng', 'Siyi', 'Siyi', 'Matt'], dtype='object')</pre> <p>We can get values back out using the index via the <code>.loc</code> attribute</p> In\u00a0[8]: Copied! <pre>ages.loc['Xiaomeng']\n</pre> ages.loc['Xiaomeng'] Out[8]: <pre>32</pre> In\u00a0[9]: Copied! <pre>ages.loc['Siyi']\n</pre> ages.loc['Siyi'] Out[9]: <pre>Siyi    25\nSiyi    26\ndtype: int64</pre> <p>Or by raw position using <code>.iloc</code></p> In\u00a0[20]: Copied! <pre>ages.iloc[2]\n</pre> ages.iloc[2] Out[20]: <pre>26</pre> <p>We can pass a list or array to loc to get multiple rows back:</p> In\u00a0[21]: Copied! <pre>ages.loc[['Matt', 'Siyi']]\n</pre> ages.loc[['Matt', 'Siyi']] Out[21]: <pre>Matt    22\nSiyi    25\nSiyi    26\ndtype: int64</pre> <p>And we can even use slice notation</p> In\u00a0[22]: Copied! <pre>ages.loc['Xiaomeng':'Matt']\n</pre> ages.loc['Xiaomeng':'Matt'] Out[22]: <pre>Xiaomeng    32\nSiyi        25\nSiyi        26\nMatt        22\ndtype: int64</pre> In\u00a0[23]: Copied! <pre>ages.iloc[:2]\n</pre> ages.iloc[:2] Out[23]: <pre>Xiaomeng    32\nSiyi        25\ndtype: int64</pre> <p>If we need to, we can always get the raw data back out as well</p> In\u00a0[24]: Copied! <pre>ages.values # a numpy array\n</pre> ages.values # a numpy array Out[24]: <pre>array([32, 25, 26, 22])</pre> In\u00a0[25]: Copied! <pre>ages.index # a pandas Index object\n</pre> ages.index # a pandas Index object Out[25]: <pre>Index(['Xiaomeng', 'Siyi', 'Siyi', 'Matt'], dtype='object')</pre> In\u00a0[26]: Copied! <pre># first we create a dictionary\ndata = {'age': [32, 25, 22],\n        'height': [160, np.NaN, np.NaN],\n        'is_teacher': [True, False, False]}\ndf = pd.DataFrame(data, index=['Xiaomeng', 'Siyi', 'Matt'])\ndf\n</pre> # first we create a dictionary data = {'age': [32, 25, 22],         'height': [160, np.NaN, np.NaN],         'is_teacher': [True, False, False]} df = pd.DataFrame(data, index=['Xiaomeng', 'Siyi', 'Matt']) df Out[26]: age height is_teacher Xiaomeng 32 160.0 True Siyi 25 NaN False Matt 22 NaN False In\u00a0[31]: Copied! <pre># You can set the style of the table\ndf.style.highlight_max()\n</pre> # You can set the style of the table df.style.highlight_max() Out[31]: age height is_teacher Xiaomeng 32 160.000000 True Siyi 25 nan False Matt 22 nan False <p>Pandas handles missing data very elegantly, keeping track of it through all calculations.</p> <p>A wide range of statistical functions are available on both Series and DataFrames.</p> In\u00a0[32]: Copied! <pre>df.min()\n</pre> df.min() Out[32]: <pre>age              22\nheight        160.0\nis_teacher    False\ndtype: object</pre> In\u00a0[33]: Copied! <pre>df.mean()\n</pre> df.mean() Out[33]: <pre>age            26.333333\nheight        160.000000\nis_teacher      0.333333\ndtype: float64</pre> In\u00a0[34]: Copied! <pre>df.count()\n</pre> df.count() Out[34]: <pre>age           3\nheight        1\nis_teacher    3\ndtype: int64</pre> In\u00a0[35]: Copied! <pre>df.std()\n</pre> df.std() Out[35]: <pre>age           5.131601\nheight             NaN\nis_teacher    0.577350\ndtype: float64</pre> In\u00a0[36]: Copied! <pre>df.describe()\n</pre> df.describe() Out[36]: age height count 3.000000 1.0 mean 26.333333 160.0 std 5.131601 NaN min 22.000000 160.0 25% 23.500000 160.0 50% 25.000000 160.0 75% 28.500000 160.0 max 32.000000 160.0 <p>We can get a single column as a Series using python's getitem syntax on the DataFrame object.</p> In\u00a0[37]: Copied! <pre>df['height']\n</pre> df['height'] Out[37]: <pre>Xiaomeng    160.0\nSiyi          NaN\nMatt          NaN\nName: height, dtype: float64</pre> <p>...or using attribute syntax.</p> In\u00a0[38]: Copied! <pre>df.height\n</pre> df.height Out[38]: <pre>Xiaomeng    160.0\nSiyi          NaN\nMatt          NaN\nName: height, dtype: float64</pre> <p>Indexing works very similar to series</p> In\u00a0[39]: Copied! <pre>df.loc['Xiaomeng']\n</pre> df.loc['Xiaomeng'] Out[39]: <pre>age              32\nheight        160.0\nis_teacher     True\nName: Xiaomeng, dtype: object</pre> In\u00a0[40]: Copied! <pre>df.iloc[2]\n</pre> df.iloc[2] Out[40]: <pre>age              22\nheight          NaN\nis_teacher    False\nName: Matt, dtype: object</pre> <p>But we can also specify the column we want to access</p> In\u00a0[41]: Copied! <pre>df.loc['Xiaomeng', 'age']\n</pre> df.loc['Xiaomeng', 'age'] Out[41]: <pre>32</pre> In\u00a0[42]: Copied! <pre>df.iloc[:2, 0]\n</pre> df.iloc[:2, 0] Out[42]: <pre>Xiaomeng    32\nSiyi        25\nName: age, dtype: int64</pre> <p>If we make a calculation using columns from the DataFrame, it will keep the same index:</p> <p>Which we can easily add as another column to the DataFrame:</p> In\u00a0[43]: Copied! <pre>2023 - df['age']\n</pre> 2023 - df['age'] Out[43]: <pre>Xiaomeng    1991\nSiyi        1998\nMatt        2001\nName: age, dtype: int64</pre> In\u00a0[44]: Copied! <pre>df['year'] = 2023 - df['age']\ndf\n</pre> df['year'] = 2023 - df['age'] df Out[44]: age height is_teacher year Xiaomeng 32 160.0 True 1991 Siyi 25 NaN False 1998 Matt 22 NaN False 2001 In\u00a0[45]: Copied! <pre># Modify values\ndf.loc['Siyi', 'height'] = 165\n</pre> # Modify values df.loc['Siyi', 'height'] = 165  In\u00a0[46]: Copied! <pre># Don't run it many times, it will keep adding values.\u00a0\ndf.loc['Xiaomeng', 'age'] += 1\ndf\n</pre> # Don't run it many times, it will keep adding values.\u00a0 df.loc['Xiaomeng', 'age'] += 1 df Out[46]: age height is_teacher year Xiaomeng 33 160.0 True 1991 Siyi 25 165.0 False 1998 Matt 22 NaN False 2001 In\u00a0[47]: Copied! <pre>df.loc['Kerry'] = [25, np.NaN, False, 1998]\n</pre> df.loc['Kerry'] = [25, np.NaN, False, 1998]  In\u00a0[48]: Copied! <pre>df\n</pre> df Out[48]: age height is_teacher year Xiaomeng 33 160.0 True 1991 Siyi 25 165.0 False 1998 Matt 22 NaN False 2001 Kerry 25 NaN False 1998 In\u00a0[49]: Copied! <pre>education = pd.Series(['PhD', 'masters', 'bachelor','PhD'],\n                     index=['Xiaomeng', 'Siyi', 'Matt', 'Lisa'],\n                     name='education')\neducation\n</pre> education = pd.Series(['PhD', 'masters', 'bachelor','PhD'],                      index=['Xiaomeng', 'Siyi', 'Matt', 'Lisa'],                      name='education') education Out[49]: <pre>Xiaomeng         PhD\nSiyi         masters\nMatt        bachelor\nLisa             PhD\nName: education, dtype: object</pre> In\u00a0[50]: Copied! <pre># returns a new DataFrame\ndf_join = df.join(education)\ndf_join\n</pre> # returns a new DataFrame df_join = df.join(education) df_join Out[50]: age height is_teacher year education Xiaomeng 33 160.0 True 1991 PhD Siyi 25 165.0 False 1998 masters Matt 22 NaN False 2001 bachelor Kerry 25 NaN False 1998 NaN In\u00a0[52]: Copied! <pre>df_join_right = df.join(education, how = 'right')\ndf_join_right\n</pre> df_join_right = df.join(education, how = 'right') df_join_right Out[52]: age height is_teacher year education Xiaomeng 33.0 160.0 True 1991.0 PhD Siyi 25.0 165.0 False 1998.0 masters Matt 22.0 NaN False 2001.0 bachelor Lisa NaN NaN NaN NaN PhD In\u00a0[53]: Copied! <pre>df_combined = pd.concat([df_join, df_join_right])\n</pre> df_combined = pd.concat([df_join, df_join_right]) In\u00a0[54]: Copied! <pre>df_combined\n</pre> df_combined Out[54]: age height is_teacher year education Xiaomeng 33.0 160.0 True 1991.0 PhD Siyi 25.0 165.0 False 1998.0 masters Matt 22.0 NaN False 2001.0 bachelor Kerry 25.0 NaN False 1998.0 NaN Xiaomeng 33.0 160.0 True 1991.0 PhD Siyi 25.0 165.0 False 1998.0 masters Matt 22.0 NaN False 2001.0 bachelor Lisa NaN NaN NaN NaN PhD In\u00a0[55]: Copied! <pre>df_combined.drop_duplicates()\n</pre> df_combined.drop_duplicates() Out[55]: age height is_teacher year education Xiaomeng 33.0 160.0 True 1991.0 PhD Siyi 25.0 165.0 False 1998.0 masters Matt 22.0 NaN False 2001.0 bachelor Kerry 25.0 NaN False 1998.0 NaN Lisa NaN NaN NaN NaN PhD In\u00a0[56]: Copied! <pre>### This is equivalent to:\ndf.join(education, how = 'outer')\n</pre> ### This is equivalent to: df.join(education, how = 'outer') Out[56]: age height is_teacher year education Kerry 25.0 NaN False 1998.0 NaN Lisa NaN NaN NaN NaN PhD Matt 22.0 NaN False 2001.0 bachelor Siyi 25.0 165.0 False 1998.0 masters Xiaomeng 33.0 160.0 True 1991.0 PhD <p>As you can see, pd.read_csv() has quite a few parameters. Don't be overwhelmed \u2013 most of these are optional arguments that allow you to specify exactly how your data file is structured and which part(s) you want to import. In particular, the sep parameter allows the user to specify the type of delimiter used in the file. The default is a comma, but you can actually pass other common delimiters (such as sep='\\t', which is a tab) to import other delimited files. The only required argument is a string specifying the filepath of your file.</p> In\u00a0[\u00a0]: Copied! <pre>pd.read_csv?\n</pre> pd.read_csv? In\u00a0[66]: Copied! <pre>df = pd.read_csv('../Assignments/Assignment_3/Millbrook_NY_daily_weather.csv')\n</pre> df = pd.read_csv('../Assignments/Assignment_3/Millbrook_NY_daily_weather.csv') In\u00a0[67]: Copied! <pre># Show the first five rows. \ndf.head()\n</pre> # Show the first five rows.  df.head() Out[67]: LST_DATE WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 0 2016-01-01 64756 2.422 -73.74 41.79 3.4 -0.5 1.5 1.3 0.0 ... 0.233 0.204 0.155 0.147 4.2 4.4 5.1 6.0 7.6 NaN 1 2016-01-02 64756 2.422 -73.74 41.79 2.9 -3.6 -0.4 -0.3 0.0 ... 0.227 0.199 0.152 0.144 2.8 3.1 4.2 5.7 7.4 NaN 2 2016-01-03 64756 2.422 -73.74 41.79 5.1 -1.8 1.6 1.1 0.0 ... 0.223 0.196 0.151 0.141 2.6 2.8 3.8 5.2 7.2 NaN 3 2016-01-04 64756 2.422 -73.74 41.79 0.5 -14.4 -6.9 -7.5 0.0 ... 0.220 0.194 0.148 0.139 1.7 2.1 3.4 4.9 6.9 NaN 4 2016-01-05 64756 2.422 -73.74 41.79 -5.2 -15.5 -10.3 -11.7 0.0 ... 0.213 0.191 0.148 0.138 0.4 0.9 2.4 4.3 6.6 NaN <p>5 rows \u00d7 29 columns</p> In\u00a0[68]: Copied! <pre># Show the first five rows. \ndf.tail()\n</pre> # Show the first five rows.  df.tail() Out[68]: LST_DATE WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 2552 2022-12-27 64756 2.622 -73.74 41.79 -0.8 -8.0 -4.4 -3.8 0.0 ... NaN NaN 0.164 0.157 -0.4 -0.2 0.5 2.2 4.0 NaN 2553 2022-12-28 64756 2.622 -73.74 41.79 7.4 -6.1 0.7 1.3 0.0 ... NaN NaN 0.162 0.156 -0.4 -0.3 0.4 2.1 3.8 NaN 2554 2022-12-29 64756 2.622 -73.74 41.79 10.7 -1.8 4.4 5.0 0.0 ... NaN NaN 0.159 0.155 -0.3 -0.3 0.3 1.9 3.7 NaN 2555 2022-12-30 64756 2.622 -73.74 41.79 16.6 4.9 10.7 10.3 0.0 ... NaN NaN 0.159 0.154 -0.2 -0.2 0.3 1.8 3.6 NaN 2556 2022-12-31 64756 2.622 -73.74 41.79 13.2 2.7 7.9 10.2 5.0 ... NaN NaN 0.160 0.153 -0.1 -0.2 0.3 1.8 3.4 NaN <p>5 rows \u00d7 29 columns</p> In\u00a0[69]: Copied! <pre># Both df.head() and df.tail() can also accept an integer argument, e.g. df.head(n), where the first n rows will be printed.\n\n\ndf.head(10)\n</pre> # Both df.head() and df.tail() can also accept an integer argument, e.g. df.head(n), where the first n rows will be printed.   df.head(10) Out[69]: LST_DATE WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 0 2016-01-01 64756 2.422 -73.74 41.79 3.4 -0.5 1.5 1.3 0.0 ... 0.233 0.204 0.155 0.147 4.2 4.4 5.1 6.0 7.6 NaN 1 2016-01-02 64756 2.422 -73.74 41.79 2.9 -3.6 -0.4 -0.3 0.0 ... 0.227 0.199 0.152 0.144 2.8 3.1 4.2 5.7 7.4 NaN 2 2016-01-03 64756 2.422 -73.74 41.79 5.1 -1.8 1.6 1.1 0.0 ... 0.223 0.196 0.151 0.141 2.6 2.8 3.8 5.2 7.2 NaN 3 2016-01-04 64756 2.422 -73.74 41.79 0.5 -14.4 -6.9 -7.5 0.0 ... 0.220 0.194 0.148 0.139 1.7 2.1 3.4 4.9 6.9 NaN 4 2016-01-05 64756 2.422 -73.74 41.79 -5.2 -15.5 -10.3 -11.7 0.0 ... 0.213 0.191 0.148 0.138 0.4 0.9 2.4 4.3 6.6 NaN 5 2016-01-06 64756 2.422 -73.74 41.79 5.0 -16.4 -5.7 -5.9 0.0 ... NaN 0.186 0.146 0.137 -0.1 0.4 1.7 3.6 6.2 NaN 6 2016-01-07 64756 2.422 -73.74 41.79 4.6 -11.7 -3.6 -4.8 0.0 ... NaN 0.183 0.144 NaN -0.2 0.2 1.4 3.2 5.8 NaN 7 2016-01-08 64756 2.422 -73.74 41.79 6.9 -11.8 -2.5 -2.0 0.0 ... NaN 0.180 0.144 0.137 -0.3 0.0 1.1 2.8 5.5 NaN 8 2016-01-09 64756 2.422 -73.74 41.79 6.6 -0.1 3.2 4.1 0.0 ... NaN 0.179 0.141 0.136 -0.1 0.1 1.0 2.6 5.2 NaN 9 2016-01-10 64756 2.422 -73.74 41.79 15.7 3.4 9.5 9.0 24.2 ... NaN 0.204 0.149 0.134 0.6 0.5 1.1 2.4 4.9 NaN <p>10 rows \u00d7 29 columns</p> In\u00a0[70]: Copied! <pre>df.describe()\n</pre> df.describe() Out[70]: WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC SOLARAD_DAILY ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 count 2557.0 2557.000000 2557.00 2.557000e+03 2545.000000 2545.000000 2545.000000 2545.000000 2547.000000 2545.000000 ... 2173.000000 2281.000000 2545.000000 2427.000000 2545.000000 2545.000000 2542.000000 2545.000000 2544.000000 0.0 mean 64756.0 2.549059 -73.74 4.179000e+01 15.973635 4.037642 10.003733 10.130059 3.216726 12.908444 ... 0.189869 0.192877 0.150320 0.156753 12.382318 12.353084 12.155901 12.077367 11.994340 NaN std 0.0 0.518602 0.00 7.106817e-15 10.648586 9.568906 9.865217 9.721821 8.131592 8.015033 ... 0.071355 0.073708 0.029615 0.022373 9.531614 9.468884 8.960360 8.190971 7.334731 NaN min 64756.0 -9.000000 -73.74 4.179000e+01 -12.300000 -26.000000 -18.400000 -19.200000 0.000000 0.030000 ... 0.029000 0.030000 0.070000 0.023000 -1.800000 -1.600000 -0.500000 0.500000 1.400000 NaN 25% 64756.0 2.422000 -73.74 4.179000e+01 6.900000 -3.100000 2.000000 2.100000 0.000000 6.070000 ... 0.141000 0.144000 0.133000 0.145000 2.700000 2.800000 3.125000 3.800000 4.700000 NaN 50% 64756.0 2.622000 -73.74 4.179000e+01 16.900000 3.900000 10.200000 10.500000 0.000000 11.820000 ... 0.216000 0.211000 0.157000 0.160000 12.300000 12.200000 12.000000 12.000000 11.900000 NaN 75% 64756.0 2.622000 -73.74 4.179000e+01 25.200000 11.900000 18.700000 18.800000 2.150000 19.450000 ... 0.244000 0.246000 0.170000 0.170000 21.800000 21.700000 21.075000 20.100000 19.100000 NaN max 64756.0 2.622000 -73.74 4.179000e+01 36.500000 23.400000 28.900000 28.400000 133.400000 31.250000 ... 0.359000 0.335000 0.223000 0.218000 28.600000 28.500000 27.100000 25.400000 23.500000 NaN <p>8 rows \u00d7 27 columns</p> In\u00a0[71]: Copied! <pre>#  Basic information about the DataFrame\ndf.info()\n</pre> #  Basic information about the DataFrame df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2557 entries, 0 to 2556\nData columns (total 29 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   LST_DATE                 2557 non-null   object \n 1   WBANNO                   2557 non-null   int64  \n 2   CRX_VN                   2557 non-null   float64\n 3   LONGITUDE                2557 non-null   float64\n 4   LATITUDE                 2557 non-null   float64\n 5   T_DAILY_MAX              2545 non-null   float64\n 6   T_DAILY_MIN              2545 non-null   float64\n 7   T_DAILY_MEAN             2545 non-null   float64\n 8   T_DAILY_AVG              2545 non-null   float64\n 9   P_DAILY_CALC             2547 non-null   float64\n 10  SOLARAD_DAILY            2545 non-null   float64\n 11  SUR_TEMP_DAILY_TYPE      2557 non-null   object \n 12  SUR_TEMP_DAILY_MAX       2545 non-null   float64\n 13  SUR_TEMP_DAILY_MIN       2545 non-null   float64\n 14  SUR_TEMP_DAILY_AVG       2545 non-null   float64\n 15  RH_DAILY_MAX             2545 non-null   float64\n 16  RH_DAILY_MIN             2545 non-null   float64\n 17  RH_DAILY_AVG             2545 non-null   float64\n 18  SOIL_MOISTURE_5_DAILY    2164 non-null   float64\n 19  SOIL_MOISTURE_10_DAILY   2173 non-null   float64\n 20  SOIL_MOISTURE_20_DAILY   2281 non-null   float64\n 21  SOIL_MOISTURE_50_DAILY   2545 non-null   float64\n 22  SOIL_MOISTURE_100_DAILY  2427 non-null   float64\n 23  SOIL_TEMP_5_DAILY        2545 non-null   float64\n 24  SOIL_TEMP_10_DAILY       2545 non-null   float64\n 25  SOIL_TEMP_20_DAILY       2542 non-null   float64\n 26  SOIL_TEMP_50_DAILY       2545 non-null   float64\n 27  SOIL_TEMP_100_DAILY      2544 non-null   float64\n 28  Unnamed: 28              0 non-null      float64\ndtypes: float64(26), int64(1), object(2)\nmemory usage: 579.4+ KB\n</pre> In\u00a0[75]: Copied! <pre>## Note that the SUR_TEMP_DAILY_TYPE doesn't have numerical values, we need to remove this column\n\n#del df['SUR_TEMP_DAILY_TYPE']\n\ndf = df.drop('SUR_TEMP_DAILY_TYPE', axis = 1)\n</pre> ## Note that the SUR_TEMP_DAILY_TYPE doesn't have numerical values, we need to remove this column  #del df['SUR_TEMP_DAILY_TYPE']  df = df.drop('SUR_TEMP_DAILY_TYPE', axis = 1) In\u00a0[76]: Copied! <pre>df.info()\n</pre> df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2557 entries, 0 to 2556\nData columns (total 28 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   LST_DATE                 2557 non-null   object \n 1   WBANNO                   2557 non-null   int64  \n 2   CRX_VN                   2557 non-null   float64\n 3   LONGITUDE                2557 non-null   float64\n 4   LATITUDE                 2557 non-null   float64\n 5   T_DAILY_MAX              2545 non-null   float64\n 6   T_DAILY_MIN              2545 non-null   float64\n 7   T_DAILY_MEAN             2545 non-null   float64\n 8   T_DAILY_AVG              2545 non-null   float64\n 9   P_DAILY_CALC             2547 non-null   float64\n 10  SOLARAD_DAILY            2545 non-null   float64\n 11  SUR_TEMP_DAILY_MAX       2545 non-null   float64\n 12  SUR_TEMP_DAILY_MIN       2545 non-null   float64\n 13  SUR_TEMP_DAILY_AVG       2545 non-null   float64\n 14  RH_DAILY_MAX             2545 non-null   float64\n 15  RH_DAILY_MIN             2545 non-null   float64\n 16  RH_DAILY_AVG             2545 non-null   float64\n 17  SOIL_MOISTURE_5_DAILY    2164 non-null   float64\n 18  SOIL_MOISTURE_10_DAILY   2173 non-null   float64\n 19  SOIL_MOISTURE_20_DAILY   2281 non-null   float64\n 20  SOIL_MOISTURE_50_DAILY   2545 non-null   float64\n 21  SOIL_MOISTURE_100_DAILY  2427 non-null   float64\n 22  SOIL_TEMP_5_DAILY        2545 non-null   float64\n 23  SOIL_TEMP_10_DAILY       2545 non-null   float64\n 24  SOIL_TEMP_20_DAILY       2542 non-null   float64\n 25  SOIL_TEMP_50_DAILY       2545 non-null   float64\n 26  SOIL_TEMP_100_DAILY      2544 non-null   float64\n 27  Unnamed: 28              0 non-null      float64\ndtypes: float64(26), int64(1), object(1)\nmemory usage: 559.5+ KB\n</pre> In\u00a0[77]: Copied! <pre># Return the column names\ndf.columns\n</pre> # Return the column names df.columns Out[77]: <pre>Index(['LST_DATE', 'WBANNO', 'CRX_VN', 'LONGITUDE', 'LATITUDE', 'T_DAILY_MAX',\n       'T_DAILY_MIN', 'T_DAILY_MEAN', 'T_DAILY_AVG', 'P_DAILY_CALC',\n       'SOLARAD_DAILY', 'SUR_TEMP_DAILY_MAX', 'SUR_TEMP_DAILY_MIN',\n       'SUR_TEMP_DAILY_AVG', 'RH_DAILY_MAX', 'RH_DAILY_MIN', 'RH_DAILY_AVG',\n       'SOIL_MOISTURE_5_DAILY', 'SOIL_MOISTURE_10_DAILY',\n       'SOIL_MOISTURE_20_DAILY', 'SOIL_MOISTURE_50_DAILY',\n       'SOIL_MOISTURE_100_DAILY', 'SOIL_TEMP_5_DAILY', 'SOIL_TEMP_10_DAILY',\n       'SOIL_TEMP_20_DAILY', 'SOIL_TEMP_50_DAILY', 'SOIL_TEMP_100_DAILY',\n       'Unnamed: 28'],\n      dtype='object')</pre> In\u00a0[78]: Copied! <pre># Return the index range (number of rows)\n\ndf.index\n</pre> # Return the index range (number of rows)  df.index Out[78]: <pre>RangeIndex(start=0, stop=2557, step=1)</pre> In\u00a0[79]: Copied! <pre># Return the DataFrame shape\ndf.shape\n</pre> # Return the DataFrame shape df.shape Out[79]: <pre>(2557, 28)</pre> In\u00a0[80]: Copied! <pre># Return the DataFrame values as a Numpy array\n\ndf.values\n</pre> # Return the DataFrame values as a Numpy array  df.values Out[80]: <pre>array([['2016-01-01', 64756, 2.422, ..., 6.0, 7.6, nan],\n       ['2016-01-02', 64756, 2.422, ..., 5.7, 7.4, nan],\n       ['2016-01-03', 64756, 2.422, ..., 5.2, 7.2, nan],\n       ...,\n       ['2022-12-29', 64756, 2.622, ..., 1.9, 3.7, nan],\n       ['2022-12-30', 64756, 2.622, ..., 1.8, 3.6, nan],\n       ['2022-12-31', 64756, 2.622, ..., 1.8, 3.4, nan]], dtype=object)</pre> <p>df.iloc acts just like the index operator works with arrays.</p> In\u00a0[81]: Copied! <pre># Using iloc\n\ndf.iloc[5,5]\n</pre> # Using iloc  df.iloc[5,5] Out[81]: <pre>5.0</pre> <p>In addition to indexing a single value, df.iloc can be used to select multiple rows and columns via slicing: df.iloc[row_start:row_end:row_step, col_start:col_end:col_step].</p> In\u00a0[83]: Copied! <pre># Select first three rows, and last 12 columns\ndf.iloc[:3,12:]\n</pre> # Select first three rows, and last 12 columns df.iloc[:3,12:]  Out[83]: SUR_TEMP_DAILY_MIN SUR_TEMP_DAILY_AVG RH_DAILY_MAX RH_DAILY_MIN RH_DAILY_AVG SOIL_MOISTURE_5_DAILY SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 0 -4.2 0.7 90.5 51.7 69.1 0.256 0.233 0.204 0.155 0.147 4.2 4.4 5.1 6.0 7.6 NaN 1 -7.8 -1.4 77.7 44.8 57.4 0.248 0.227 0.199 0.152 0.144 2.8 3.1 4.2 5.7 7.4 NaN 2 -5.7 -0.4 72.7 48.7 61.0 0.243 0.223 0.196 0.151 0.141 2.6 2.8 3.8 5.2 7.2 NaN In\u00a0[84]: Copied! <pre># First 5 columns, every 40th row\ndf.iloc[::40,:5]\n</pre> # First 5 columns, every 40th row df.iloc[::40,:5] Out[84]: LST_DATE WBANNO CRX_VN LONGITUDE LATITUDE 0 2016-01-01 64756 2.422 -73.74 41.79 40 2016-02-10 64756 2.422 -73.74 41.79 80 2016-03-21 64756 2.422 -73.74 41.79 120 2016-04-30 64756 2.422 -73.74 41.79 160 2016-06-09 64756 2.422 -73.74 41.79 ... ... ... ... ... ... 2360 2022-06-18 64756 2.622 -73.74 41.79 2400 2022-07-28 64756 2.622 -73.74 41.79 2440 2022-09-06 64756 2.622 -73.74 41.79 2480 2022-10-16 64756 2.622 -73.74 41.79 2520 2022-11-25 64756 2.622 -73.74 41.79 <p>64 rows \u00d7 5 columns</p> <p>When indexing a single row, df.loc (like df.iloc) transforms the row into a Series, with the column names as the index:</p> In\u00a0[85]: Copied! <pre># Index a row\ndf.iloc[1]\n</pre> # Index a row df.iloc[1] Out[85]: <pre>LST_DATE                   2016-01-02\nWBANNO                          64756\nCRX_VN                          2.422\nLONGITUDE                      -73.74\nLATITUDE                        41.79\nT_DAILY_MAX                       2.9\nT_DAILY_MIN                      -3.6\nT_DAILY_MEAN                     -0.4\nT_DAILY_AVG                      -0.3\nP_DAILY_CALC                      0.0\nSOLARAD_DAILY                    6.25\nSUR_TEMP_DAILY_MAX                8.7\nSUR_TEMP_DAILY_MIN               -7.8\nSUR_TEMP_DAILY_AVG               -1.4\nRH_DAILY_MAX                     77.7\nRH_DAILY_MIN                     44.8\nRH_DAILY_AVG                     57.4\nSOIL_MOISTURE_5_DAILY           0.248\nSOIL_MOISTURE_10_DAILY          0.227\nSOIL_MOISTURE_20_DAILY          0.199\nSOIL_MOISTURE_50_DAILY          0.152\nSOIL_MOISTURE_100_DAILY         0.144\nSOIL_TEMP_5_DAILY                 2.8\nSOIL_TEMP_10_DAILY                3.1\nSOIL_TEMP_20_DAILY                4.2\nSOIL_TEMP_50_DAILY                5.7\nSOIL_TEMP_100_DAILY               7.4\nUnnamed: 28                       NaN\nName: 1, dtype: object</pre> <p>In addition to df.iloc, rows of a DataFrame can be accessed using df.loc, which \"locates\" rows based on their labels. Unless you have set a custom index (which we will see later), the row \"labels\" are the same as the integer index.</p> In\u00a0[86]: Copied! <pre>df.loc[1]\n</pre> df.loc[1] Out[86]: <pre>LST_DATE                   2016-01-02\nWBANNO                          64756\nCRX_VN                          2.422\nLONGITUDE                      -73.74\nLATITUDE                        41.79\nT_DAILY_MAX                       2.9\nT_DAILY_MIN                      -3.6\nT_DAILY_MEAN                     -0.4\nT_DAILY_AVG                      -0.3\nP_DAILY_CALC                      0.0\nSOLARAD_DAILY                    6.25\nSUR_TEMP_DAILY_MAX                8.7\nSUR_TEMP_DAILY_MIN               -7.8\nSUR_TEMP_DAILY_AVG               -1.4\nRH_DAILY_MAX                     77.7\nRH_DAILY_MIN                     44.8\nRH_DAILY_AVG                     57.4\nSOIL_MOISTURE_5_DAILY           0.248\nSOIL_MOISTURE_10_DAILY          0.227\nSOIL_MOISTURE_20_DAILY          0.199\nSOIL_MOISTURE_50_DAILY          0.152\nSOIL_MOISTURE_100_DAILY         0.144\nSOIL_TEMP_5_DAILY                 2.8\nSOIL_TEMP_10_DAILY                3.1\nSOIL_TEMP_20_DAILY                4.2\nSOIL_TEMP_50_DAILY                5.7\nSOIL_TEMP_100_DAILY               7.4\nUnnamed: 28                       NaN\nName: 1, dtype: object</pre> In\u00a0[87]: Copied! <pre># Select multiple rows by position\ndf.iloc[100:200]\n</pre> # Select multiple rows by position df.iloc[100:200] Out[87]: LST_DATE WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 100 2016-04-10 64756 2.422 -73.74 41.79 9.7 -3.1 3.3 3.5 0.0 ... 0.222 0.197 0.151 0.133 6.6 6.3 6.1 6.2 6.6 NaN 101 2016-04-11 64756 2.422 -73.74 41.79 12.7 4.3 8.5 8.1 1.4 ... 0.218 0.194 0.150 0.134 7.3 7.1 6.8 6.5 6.6 NaN 102 2016-04-12 64756 2.422 -73.74 41.79 13.0 -0.9 6.0 8.4 12.5 ... 0.250 0.210 0.154 0.135 9.0 8.6 7.7 6.9 6.7 NaN 103 2016-04-13 64756 2.422 -73.74 41.79 12.6 -3.1 4.7 5.1 0.0 ... 0.236 0.207 0.156 0.137 8.5 8.2 7.8 7.3 6.9 NaN 104 2016-04-14 64756 2.422 -73.74 41.79 15.1 -0.2 7.5 7.3 0.0 ... 0.224 0.199 0.153 0.142 9.3 9.0 8.3 7.5 7.1 NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 195 2016-07-14 64756 2.422 -73.74 41.79 30.4 21.6 26.0 24.5 4.2 ... 0.070 0.053 0.099 NaN 25.9 25.7 24.3 22.6 20.9 NaN 196 2016-07-15 64756 2.422 -73.74 41.79 31.5 18.9 25.2 25.3 0.0 ... 0.067 0.053 0.099 NaN 26.7 26.3 24.5 22.7 21.0 NaN 197 2016-07-16 64756 2.422 -73.74 41.79 30.4 16.3 23.4 23.1 0.0 ... 0.062 0.052 0.099 NaN 26.5 26.2 24.7 23.0 21.2 NaN 198 2016-07-17 64756 2.422 -73.74 41.79 30.7 16.7 23.7 23.5 0.0 ... 0.057 0.050 0.097 NaN 27.3 27.0 25.0 23.1 21.3 NaN 199 2016-07-18 64756 2.422 -73.74 41.79 33.4 15.5 24.4 22.8 4.4 ... 0.052 0.047 0.096 NaN 26.2 26.1 24.9 23.3 21.5 NaN <p>100 rows \u00d7 28 columns</p> <p>Slicing using df.loc is similar to df.iloc, with the exception that the stop value is inclusive:</p> In\u00a0[88]: Copied! <pre>df.loc[100:200]\n</pre> df.loc[100:200] Out[88]: LST_DATE WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 100 2016-04-10 64756 2.422 -73.74 41.79 9.7 -3.1 3.3 3.5 0.0 ... 0.222 0.197 0.151 0.133 6.6 6.3 6.1 6.2 6.6 NaN 101 2016-04-11 64756 2.422 -73.74 41.79 12.7 4.3 8.5 8.1 1.4 ... 0.218 0.194 0.150 0.134 7.3 7.1 6.8 6.5 6.6 NaN 102 2016-04-12 64756 2.422 -73.74 41.79 13.0 -0.9 6.0 8.4 12.5 ... 0.250 0.210 0.154 0.135 9.0 8.6 7.7 6.9 6.7 NaN 103 2016-04-13 64756 2.422 -73.74 41.79 12.6 -3.1 4.7 5.1 0.0 ... 0.236 0.207 0.156 0.137 8.5 8.2 7.8 7.3 6.9 NaN 104 2016-04-14 64756 2.422 -73.74 41.79 15.1 -0.2 7.5 7.3 0.0 ... 0.224 0.199 0.153 0.142 9.3 9.0 8.3 7.5 7.1 NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 196 2016-07-15 64756 2.422 -73.74 41.79 31.5 18.9 25.2 25.3 0.0 ... 0.067 0.053 0.099 NaN 26.7 26.3 24.5 22.7 21.0 NaN 197 2016-07-16 64756 2.422 -73.74 41.79 30.4 16.3 23.4 23.1 0.0 ... 0.062 0.052 0.099 NaN 26.5 26.2 24.7 23.0 21.2 NaN 198 2016-07-17 64756 2.422 -73.74 41.79 30.7 16.7 23.7 23.5 0.0 ... 0.057 0.050 0.097 NaN 27.3 27.0 25.0 23.1 21.3 NaN 199 2016-07-18 64756 2.422 -73.74 41.79 33.4 15.5 24.4 22.8 4.4 ... 0.052 0.047 0.096 NaN 26.2 26.1 24.9 23.3 21.5 NaN 200 2016-07-19 64756 2.422 -73.74 41.79 27.1 12.0 19.6 20.3 0.0 ... 0.051 0.046 0.095 NaN 25.2 25.2 24.3 23.1 21.6 NaN <p>101 rows \u00d7 28 columns</p> <p>In addition to integer indexing with df.iloc, columns can be accessed in two ways: dot notation . or square brackets []. The former takes advantage of the fact that the columns are effectively \"attributes\" of the DataFrame and returns a Series:</p> In\u00a0[89]: Copied! <pre>df.T_DAILY_MEAN\n</pre> df.T_DAILY_MEAN Out[89]: <pre>0        1.5\n1       -0.4\n2        1.6\n3       -6.9\n4      -10.3\n        ... \n2552    -4.4\n2553     0.7\n2554     4.4\n2555    10.7\n2556     7.9\nName: T_DAILY_MEAN, Length: 2557, dtype: float64</pre> In\u00a0[90]: Copied! <pre># Indexing a DataFrame column. Returns a Series\n\ndf['T_DAILY_MEAN']\n</pre> # Indexing a DataFrame column. Returns a Series  df['T_DAILY_MEAN'] Out[90]: <pre>0        1.5\n1       -0.4\n2        1.6\n3       -6.9\n4      -10.3\n        ... \n2552    -4.4\n2553     0.7\n2554     4.4\n2555    10.7\n2556     7.9\nName: T_DAILY_MEAN, Length: 2557, dtype: float64</pre> <p>Using single brackets, the result is a Series. However, using double brackets, it is possible to return the column as a DataFrame:</p> In\u00a0[91]: Copied! <pre># Indexing a column as a DataFrame\n\ndf[['T_DAILY_MEAN']]\n</pre> # Indexing a column as a DataFrame  df[['T_DAILY_MEAN']] Out[91]: T_DAILY_MEAN 0 1.5 1 -0.4 2 1.6 3 -6.9 4 -10.3 ... ... 2552 -4.4 2553 0.7 2554 4.4 2555 10.7 2556 7.9 <p>2557 rows \u00d7 1 columns</p> In\u00a0[92]: Copied! <pre># Indexing multiple columns\n\ndf[['T_DAILY_MAX','T_DAILY_MIN', 'T_DAILY_MEAN', 'T_DAILY_AVG']]\n</pre> # Indexing multiple columns  df[['T_DAILY_MAX','T_DAILY_MIN', 'T_DAILY_MEAN', 'T_DAILY_AVG']] Out[92]: T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG 0 3.4 -0.5 1.5 1.3 1 2.9 -3.6 -0.4 -0.3 2 5.1 -1.8 1.6 1.1 3 0.5 -14.4 -6.9 -7.5 4 -5.2 -15.5 -10.3 -11.7 ... ... ... ... ... 2552 -0.8 -8.0 -4.4 -3.8 2553 7.4 -6.1 0.7 1.3 2554 10.7 -1.8 4.4 5.0 2555 16.6 4.9 10.7 10.3 2556 13.2 2.7 7.9 10.2 <p>2557 rows \u00d7 4 columns</p> <p>Like the data we are working with in this exercise, many environmental datasets include timed records. The standard datetime library is the primary way of manipulating dates and times in Python, but there are additional third-party packages that provide additional support.</p> <p>A few worth exploring are dateutil, an extension of the datetime library useful for parsing timestamps, and pytz, which provides a smooth way of tackling time zones.</p> <p>Though we will not review datetime objects in depth here, it is useful to understand the basics of how to deal with datetime objects in Python as you will no doubt encounter them in the future.</p> <p>For now, we will focus on a few pandas functions built on the datetime library to handle datetime objects.</p> <p>The pd.date_range() function allows you to build a DatetimeIndex with a fixed frequency. This can be done by specifying a start date and an end date as follows:</p> In\u00a0[93]: Copied! <pre>pd.date_range('4/1/2017','4/30/2017')\n</pre> pd.date_range('4/1/2017','4/30/2017') Out[93]: <pre>DatetimeIndex(['2017-04-01', '2017-04-02', '2017-04-03', '2017-04-04',\n               '2017-04-05', '2017-04-06', '2017-04-07', '2017-04-08',\n               '2017-04-09', '2017-04-10', '2017-04-11', '2017-04-12',\n               '2017-04-13', '2017-04-14', '2017-04-15', '2017-04-16',\n               '2017-04-17', '2017-04-18', '2017-04-19', '2017-04-20',\n               '2017-04-21', '2017-04-22', '2017-04-23', '2017-04-24',\n               '2017-04-25', '2017-04-26', '2017-04-27', '2017-04-28',\n               '2017-04-29', '2017-04-30'],\n              dtype='datetime64[ns]', freq='D')</pre> In\u00a0[94]: Copied! <pre># Specify start and end, monthly frequency\n\npd.date_range('4/1/2017','4/30/2018', freq = 'M')\n</pre> # Specify start and end, monthly frequency  pd.date_range('4/1/2017','4/30/2018', freq = 'M') Out[94]: <pre>DatetimeIndex(['2017-04-30', '2017-05-31', '2017-06-30', '2017-07-31',\n               '2017-08-31', '2017-09-30', '2017-10-31', '2017-11-30',\n               '2017-12-31', '2018-01-31', '2018-02-28', '2018-03-31',\n               '2018-04-30'],\n              dtype='datetime64[ns]', freq='M')</pre> In\u00a0[96]: Copied! <pre># Specify start and end, 5min frequency\n# datetime64 is 64-bit integer, which represents an offset from 1970-01-01T00:00:00\n\npd.date_range('4/1/2017','4/30/2018', freq = '5min')\n</pre> # Specify start and end, 5min frequency # datetime64 is 64-bit integer, which represents an offset from 1970-01-01T00:00:00  pd.date_range('4/1/2017','4/30/2018', freq = '5min') Out[96]: <pre>DatetimeIndex(['2017-04-01 00:00:00', '2017-04-01 00:05:00',\n               '2017-04-01 00:10:00', '2017-04-01 00:15:00',\n               '2017-04-01 00:20:00', '2017-04-01 00:25:00',\n               '2017-04-01 00:30:00', '2017-04-01 00:35:00',\n               '2017-04-01 00:40:00', '2017-04-01 00:45:00',\n               ...\n               '2018-04-29 23:15:00', '2018-04-29 23:20:00',\n               '2018-04-29 23:25:00', '2018-04-29 23:30:00',\n               '2018-04-29 23:35:00', '2018-04-29 23:40:00',\n               '2018-04-29 23:45:00', '2018-04-29 23:50:00',\n               '2018-04-29 23:55:00', '2018-04-30 00:00:00'],\n              dtype='datetime64[ns]', length=113473, freq='5T')</pre> In\u00a0[97]: Copied! <pre>df.columns\n</pre> df.columns Out[97]: <pre>Index(['LST_DATE', 'WBANNO', 'CRX_VN', 'LONGITUDE', 'LATITUDE', 'T_DAILY_MAX',\n       'T_DAILY_MIN', 'T_DAILY_MEAN', 'T_DAILY_AVG', 'P_DAILY_CALC',\n       'SOLARAD_DAILY', 'SUR_TEMP_DAILY_MAX', 'SUR_TEMP_DAILY_MIN',\n       'SUR_TEMP_DAILY_AVG', 'RH_DAILY_MAX', 'RH_DAILY_MIN', 'RH_DAILY_AVG',\n       'SOIL_MOISTURE_5_DAILY', 'SOIL_MOISTURE_10_DAILY',\n       'SOIL_MOISTURE_20_DAILY', 'SOIL_MOISTURE_50_DAILY',\n       'SOIL_MOISTURE_100_DAILY', 'SOIL_TEMP_5_DAILY', 'SOIL_TEMP_10_DAILY',\n       'SOIL_TEMP_20_DAILY', 'SOIL_TEMP_50_DAILY', 'SOIL_TEMP_100_DAILY',\n       'Unnamed: 28'],\n      dtype='object')</pre> In\u00a0[98]: Copied! <pre>df['LST_DATE']\n</pre> df['LST_DATE'] Out[98]: <pre>0       2016-01-01\n1       2016-01-02\n2       2016-01-03\n3       2016-01-04\n4       2016-01-05\n           ...    \n2552    2022-12-27\n2553    2022-12-28\n2554    2022-12-29\n2555    2022-12-30\n2556    2022-12-31\nName: LST_DATE, Length: 2557, dtype: object</pre> <p>While the values certainly resemble datetime objects, they are stored in pandas as \"objects,\" which basically means that pandas doesn't recognize the data type \u2013 it doesn't know how to handle them. Using the pd.to_datetime() function, we can convert this column to datetime objects:</p> In\u00a0[99]: Copied! <pre>pd.to_datetime(df['LST_DATE'])\n</pre> pd.to_datetime(df['LST_DATE']) Out[99]: <pre>0      2016-01-01\n1      2016-01-02\n2      2016-01-03\n3      2016-01-04\n4      2016-01-05\n          ...    \n2552   2022-12-27\n2553   2022-12-28\n2554   2022-12-29\n2555   2022-12-30\n2556   2022-12-31\nName: LST_DATE, Length: 2557, dtype: datetime64[ns]</pre> In\u00a0[100]: Copied! <pre># Set the LST_DATE as datetime object, you can also do so by setting the parse_dates when read in the csv data. \ndf['LST_DATE'] = pd.to_datetime(df['LST_DATE'])\n</pre> # Set the LST_DATE as datetime object, you can also do so by setting the parse_dates when read in the csv data.  df['LST_DATE'] = pd.to_datetime(df['LST_DATE']) In\u00a0[101]: Copied! <pre># Set the Date column as index:\n\ndf = df.set_index('LST_DATE')\n</pre> # Set the Date column as index:  df = df.set_index('LST_DATE') In\u00a0[102]: Copied! <pre># Now it's more intuitive to interpret the data:\n\ndf['T_DAILY_MEAN']\n</pre> # Now it's more intuitive to interpret the data:  df['T_DAILY_MEAN'] Out[102]: <pre>LST_DATE\n2016-01-01     1.5\n2016-01-02    -0.4\n2016-01-03     1.6\n2016-01-04    -6.9\n2016-01-05   -10.3\n              ... \n2022-12-27    -4.4\n2022-12-28     0.7\n2022-12-29     4.4\n2022-12-30    10.7\n2022-12-31     7.9\nName: T_DAILY_MEAN, Length: 2557, dtype: float64</pre> In\u00a0[103]: Copied! <pre>df['T_DAILY_MEAN'].plot()\n</pre> df['T_DAILY_MEAN'].plot() Out[103]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> In\u00a0[104]: Copied! <pre># Reset the index\ndf = df.reset_index()\n</pre> # Reset the index df = df.reset_index() In\u00a0[105]: Copied! <pre># X axis is not labeled with date\ndf['T_DAILY_MEAN'].plot()\n</pre> # X axis is not labeled with date df['T_DAILY_MEAN'].plot() Out[105]: <pre>&lt;Axes: &gt;</pre> In\u00a0[106]: Copied! <pre>df = df.set_index('LST_DATE')\n</pre> df = df.set_index('LST_DATE') <p>Now that we have a DatetimeIndex, we can access specific attributes of the datetime objects like the year, day, hour, etc. To do this, we add the desired time period using dot notation: df.index.attribute. For a full list of attributes, see the pd.DatetimeIndex documentation. For example:</p> In\u00a0[107]: Copied! <pre># Get the hour of each record\n\ndf.index.hour\n</pre> # Get the hour of each record  df.index.hour Out[107]: <pre>Index([0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       ...\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n      dtype='int32', name='LST_DATE', length=2557)</pre> In\u00a0[108]: Copied! <pre># Get the year of each record, and assign this to a new column\n\ndf['year'] = df.index.year\n</pre> # Get the year of each record, and assign this to a new column  df['year'] = df.index.year  In\u00a0[109]: Copied! <pre>df\n</pre> df Out[109]: WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC SOLARAD_DAILY ... SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 year LST_DATE 2016-01-01 64756 2.422 -73.74 41.79 3.4 -0.5 1.5 1.3 0.0 1.69 ... 0.204 0.155 0.147 4.2 4.4 5.1 6.0 7.6 NaN 2016 2016-01-02 64756 2.422 -73.74 41.79 2.9 -3.6 -0.4 -0.3 0.0 6.25 ... 0.199 0.152 0.144 2.8 3.1 4.2 5.7 7.4 NaN 2016 2016-01-03 64756 2.422 -73.74 41.79 5.1 -1.8 1.6 1.1 0.0 5.69 ... 0.196 0.151 0.141 2.6 2.8 3.8 5.2 7.2 NaN 2016 2016-01-04 64756 2.422 -73.74 41.79 0.5 -14.4 -6.9 -7.5 0.0 9.17 ... 0.194 0.148 0.139 1.7 2.1 3.4 4.9 6.9 NaN 2016 2016-01-05 64756 2.422 -73.74 41.79 -5.2 -15.5 -10.3 -11.7 0.0 9.34 ... 0.191 0.148 0.138 0.4 0.9 2.4 4.3 6.6 NaN 2016 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2022-12-27 64756 2.622 -73.74 41.79 -0.8 -8.0 -4.4 -3.8 0.0 4.00 ... NaN 0.164 0.157 -0.4 -0.2 0.5 2.2 4.0 NaN 2022 2022-12-28 64756 2.622 -73.74 41.79 7.4 -6.1 0.7 1.3 0.0 7.73 ... NaN 0.162 0.156 -0.4 -0.3 0.4 2.1 3.8 NaN 2022 2022-12-29 64756 2.622 -73.74 41.79 10.7 -1.8 4.4 5.0 0.0 6.66 ... NaN 0.159 0.155 -0.3 -0.3 0.3 1.9 3.7 NaN 2022 2022-12-30 64756 2.622 -73.74 41.79 16.6 4.9 10.7 10.3 0.0 5.39 ... NaN 0.159 0.154 -0.2 -0.2 0.3 1.8 3.6 NaN 2022 2022-12-31 64756 2.622 -73.74 41.79 13.2 2.7 7.9 10.2 5.0 1.25 ... NaN 0.160 0.153 -0.1 -0.2 0.3 1.8 3.4 NaN 2022 <p>2557 rows \u00d7 28 columns</p> In\u00a0[110]: Copied! <pre># Get the unique year values\ndf.index.year.unique()\n</pre> # Get the unique year values df.index.year.unique() Out[110]: <pre>Index([2016, 2017, 2018, 2019, 2020, 2021, 2022], dtype='int32', name='LST_DATE')</pre> In\u00a0[113]: Copied! <pre>df['T_DAILY_MAX'].plot()\n</pre> df['T_DAILY_MAX'].plot() Out[113]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> In\u00a0[114]: Copied! <pre>fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(14,14))\n\ndf['T_DAILY_MAX'].plot(ax=ax[0,0])\ndf['P_DAILY_CALC'].plot(ax=ax[0,1])\ndf[['SOIL_MOISTURE_5_DAILY','SOIL_MOISTURE_10_DAILY','SOIL_MOISTURE_20_DAILY']].boxplot(ax=ax[1,0])\nax[1, 0].set_xticklabels(ax[1, 0].get_xticklabels(), rotation=90);\ndf[['T_DAILY_MAX','T_DAILY_MIN']].plot(ax=ax[1,1])\n</pre> fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(14,14))  df['T_DAILY_MAX'].plot(ax=ax[0,0]) df['P_DAILY_CALC'].plot(ax=ax[0,1]) df[['SOIL_MOISTURE_5_DAILY','SOIL_MOISTURE_10_DAILY','SOIL_MOISTURE_20_DAILY']].boxplot(ax=ax[1,0]) ax[1, 0].set_xticklabels(ax[1, 0].get_xticklabels(), rotation=90); df[['T_DAILY_MAX','T_DAILY_MIN']].plot(ax=ax[1,1])    Out[114]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> In\u00a0[116]: Copied! <pre># monthly reampler object\nrs_obj = df.resample('M')\nrs_obj\n</pre> # monthly reampler object rs_obj = df.resample('M') rs_obj Out[116]: <pre>&lt;pandas.core.resample.DatetimeIndexResampler object at 0x182eb5ac0&gt;</pre> In\u00a0[117]: Copied! <pre>rs_obj.mean()\n</pre> rs_obj.mean() Out[117]: WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC SOLARAD_DAILY ... SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 year LST_DATE 2016-01-31 64756.0 2.422 -73.74 41.79 3.503226 -7.193548 -1.858065 -1.761290 1.106452 6.740000 ... 0.198903 0.146742 0.138700 0.487097 0.674194 1.429032 2.632258 4.593548 NaN 2016.0 2016-02-29 64756.0 2.422 -73.74 41.79 5.848276 -6.382759 -0.262069 0.072414 4.920690 8.379310 ... 0.202136 0.150586 0.139520 1.158621 1.158621 1.344828 1.810345 2.827586 NaN 2016.0 2016-03-31 64756.0 2.422 -73.74 41.79 13.122581 -0.638710 6.251613 6.393548 0.890323 14.435484 ... 0.186258 0.144839 0.134581 6.529032 6.370968 5.987097 5.593548 5.267742 NaN 2016.0 2016-04-30 64756.0 2.422 -73.74 41.79 14.613333 0.840000 7.726667 8.300000 2.033333 17.865000 ... 0.179200 0.142267 0.133700 10.166667 9.926667 9.410000 8.773333 8.076667 NaN 2016.0 2016-05-31 64756.0 2.422 -73.74 41.79 21.058065 8.051613 14.577419 14.751613 2.812903 17.959355 ... 0.176935 0.141516 0.135032 16.796774 16.448387 15.416129 14.083871 12.445161 NaN 2016.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2022-08-31 64756.0 2.622 -73.74 41.79 31.280645 16.145161 23.709677 23.235484 0.938710 19.360323 ... 0.072581 0.081000 0.113581 26.316129 26.054839 25.116129 23.645161 22.390323 NaN 2022.0 2022-09-30 64756.0 2.622 -73.74 41.79 23.266667 10.700000 16.970000 16.823333 4.263333 14.341333 ... 0.200300 0.121967 0.108500 20.686667 20.650000 20.593333 20.896667 20.746667 NaN 2022.0 2022-10-31 64756.0 2.622 -73.74 41.79 17.025806 3.867742 10.441935 10.351613 2.580645 10.186774 ... 0.260903 0.161194 0.133742 13.241935 13.177419 13.483871 14.574194 15.383871 NaN 2022.0 2022-11-30 64756.0 2.622 -73.74 41.79 12.380000 0.453333 6.420000 6.910000 2.956667 7.440000 ... 0.268000 0.164967 0.152633 8.196667 8.230000 8.882759 10.240000 11.470000 NaN 2022.0 2022-12-31 64756.0 2.622 -73.74 41.79 5.051613 -4.961290 0.038710 0.467742 3.751613 5.166452 ... 0.279500 0.172452 0.165065 2.070968 2.183871 2.709677 4.341935 5.867742 NaN 2022.0 <p>84 rows \u00d7 28 columns</p> <p>We can chain all of that together</p> In\u00a0[118]: Copied! <pre>df_mm = df.resample('M').mean()\ndf_mm[['T_DAILY_MIN', 'T_DAILY_MEAN', 'T_DAILY_MAX']].plot()\n</pre> df_mm = df.resample('M').mean() df_mm[['T_DAILY_MIN', 'T_DAILY_MEAN', 'T_DAILY_MAX']].plot() Out[118]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> <p>In addition to manipulating individual columns, you can apply a function to an entire Series or DataFrame using the pandas function df.apply(). For example, consider our original DataFrame df, which consists of temperature values in \u00b0C:</p> In\u00a0[121]: Copied! <pre>def convert_CtoF(degC):\n\"\"\" Converts a temperature to from Celsius to Fahrenheit\n\n    Parameters\n    ----------\n        degC : float\n            Temperature value in \u00b0C\n\n    Returns\n    -------\n        degF : float\n            Temperature value in \u00b0F\n    \"\"\"\n\n    degF = (degC *(9/5)) + 32\n\n    return degF\n</pre> def convert_CtoF(degC):     \"\"\" Converts a temperature to from Celsius to Fahrenheit      Parameters     ----------         degC : float             Temperature value in \u00b0C      Returns     -------         degF : float             Temperature value in \u00b0F     \"\"\"      degF = (degC *(9/5)) + 32      return degF In\u00a0[122]: Copied! <pre>df['T_DAILY_MEAN'].apply(convert_CtoF)\n</pre> df['T_DAILY_MEAN'].apply(convert_CtoF) Out[122]: <pre>LST_DATE\n2016-01-01    34.70\n2016-01-02    31.28\n2016-01-03    34.88\n2016-01-04    19.58\n2016-01-05    13.46\n              ...  \n2022-12-27    24.08\n2022-12-28    33.26\n2022-12-29    39.92\n2022-12-30    51.26\n2022-12-31    46.22\nName: T_DAILY_MEAN, Length: 2557, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Lecture_6_Pandas_Basics/#lecture-6-pandas-basics","title":"Lecture 6: Pandas Basics\u00b6","text":""},{"location":"Lecture_6_Pandas_Basics/#pandas-data-structures-series","title":"Pandas Data Structures: Series\u00b6","text":""},{"location":"Lecture_6_Pandas_Basics/#indexing","title":"Indexing\u00b6","text":""},{"location":"Lecture_6_Pandas_Basics/#pandas-data-structures-dataframe","title":"Pandas Data Structures: DataFrame\u00b6","text":"<p>There is a lot more to Series, but they are limit to a single \"column\". A more useful Pandas data structure is the DataFrame. A DataFrame is basically a bunch of series that share the same index. It's a lot like a table in a spreadsheet.</p> <p>Below we create a DataFrame.</p>"},{"location":"Lecture_6_Pandas_Basics/#statistics","title":"Statistics\u00b6","text":""},{"location":"Lecture_6_Pandas_Basics/#add-new-rows","title":"Add new rows\u00b6","text":""},{"location":"Lecture_6_Pandas_Basics/#merge-with-new-series","title":"Merge with new series\u00b6","text":""},{"location":"Lecture_6_Pandas_Basics/#using-concat-to-append-new-rows","title":"Using concat to append new rows\u00b6","text":""},{"location":"Lecture_6_Pandas_Basics/#working-with-real-data-in-pandas","title":"Working with real data in Pandas\u00b6","text":""},{"location":"Lecture_6_Pandas_Basics/#importing-read_csv","title":"Importing: read_csv()\u00b6","text":""},{"location":"Lecture_6_Pandas_Basics/#examine-the-dataframe","title":"Examine the dataframe\u00b6","text":""},{"location":"Lecture_6_Pandas_Basics/#indexing","title":"Indexing\u00b6","text":""},{"location":"Lecture_6_Pandas_Basics/#indexing-columns","title":"Indexing columns\u00b6","text":""},{"location":"Lecture_6_Pandas_Basics/#working-with-datetime-objects","title":"Working with Datetime objects\u00b6","text":""},{"location":"Lecture_6_Pandas_Basics/#dealing-with-existing-timestamps","title":"Dealing with existing timestamps\u00b6","text":""},{"location":"Lecture_6_Pandas_Basics/#plotting-values","title":"Plotting Values\u00b6","text":"<p>We can now quickly make plots of the data</p>"},{"location":"Lecture_6_Pandas_Basics/#resampling","title":"Resampling\u00b6","text":"<p>Since pandas understands time, we can use it to do resampling.</p>"},{"location":"Lecture_6_Pandas_Basics/#applying-functions","title":"Applying Functions\u00b6","text":""},{"location":"Lecture_7_Pandas_Advanced/","title":"Lecture 7: Advaned Pandas","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n</pre> import pandas as pd import numpy as np from matplotlib import pyplot as plt <p>Download wildfire data <code>Spatial_Database_Big_Wildfires_US_all.csv</code> from Canvas, and upload the data to your current working directory.</p> In\u00a0[27]: Copied! <pre>df = pd.read_csv('Spatial_Database_Big_Wildfires_US_all.csv', parse_dates=['DISCOVERY_DATE'])\n</pre> df = pd.read_csv('Spatial_Database_Big_Wildfires_US_all.csv', parse_dates=['DISCOVERY_DATE'])  In\u00a0[28]: Copied! <pre>df.head()\n</pre> df.head() Out[28]: FOD_ID FPA_ID SOURCE_SYSTEM_TYPE SOURCE_SYSTEM NWCG_REPORTING_UNIT_ID NWCG_REPORTING_UNIT_NAME FIRE_CODE FIRE_NAME MTBS_FIRE_NAME COMPLEX_NAME ... CONT_DOY FIRE_SIZE FIRE_SIZE_CLASS LATITUDE LONGITUDE OWNER_DESCR STATE COUNTY FIPS_CODE FIPS_NAME 0 17 FS-1418878 FED FS-FIRESTAT USCAENF Eldorado National Forest NaN POWER POWER NaN ... 295.0 16823.0 G 38.523333 -120.211667 USFS CA 5 6005.0 Amador County 1 18 FS-1418881 FED FS-FIRESTAT USCAENF Eldorado National Forest BHA3 FREDS FREDS NaN ... 291.0 7700.0 G 38.780000 -120.260000 USFS CA 17 6017.0 El Dorado County 2 40 FS-1418920 FED FS-FIRESTAT USNCNCF National Forests in North Carolina BKC8 AUSTIN CREEK NaN NaN ... 44.0 125.0 D 36.001667 -81.590000 MISSING/NOT SPECIFIED NC 27 37027.0 Caldwell County 3 119 FS-1419153 FED FS-FIRESTAT USNENBF Nebraska National Forest BEW8 THOMPSON BUTTE NaN NaN ... 198.0 119.0 D 43.899167 -102.954722 USFS SD 103 46103.0 Pennington County 4 120 FS-1419156 FED FS-FIRESTAT USNENBF Nebraska National Forest BEW8 CHARLES DRAW NaN NaN ... 197.0 119.0 D 43.892778 -102.948056 USFS SD 103 46103.0 Pennington County <p>5 rows \u00d7 28 columns</p> In\u00a0[29]: Copied! <pre>df.columns\n</pre> df.columns Out[29]: <pre>Index(['FOD_ID', 'FPA_ID', 'SOURCE_SYSTEM_TYPE', 'SOURCE_SYSTEM',\n       'NWCG_REPORTING_UNIT_ID', 'NWCG_REPORTING_UNIT_NAME', 'FIRE_CODE',\n       'FIRE_NAME', 'MTBS_FIRE_NAME', 'COMPLEX_NAME', 'FIRE_YEAR',\n       'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME',\n       'NWCG_CAUSE_CLASSIFICATION', 'NWCG_GENERAL_CAUSE',\n       'NWCG_CAUSE_AGE_CATEGORY', 'CONT_DATE', 'CONT_DOY', 'FIRE_SIZE',\n       'FIRE_SIZE_CLASS', 'LATITUDE', 'LONGITUDE', 'OWNER_DESCR', 'STATE',\n       'COUNTY', 'FIPS_CODE', 'FIPS_NAME'],\n      dtype='object')</pre> In\u00a0[30]: Copied! <pre>df = df.set_index('FOD_ID')\n</pre> df = df.set_index('FOD_ID') In\u00a0[31]: Copied! <pre>df[['DISCOVERY_DATE','DISCOVERY_DOY','DISCOVERY_TIME' ]]\n</pre> df[['DISCOVERY_DATE','DISCOVERY_DOY','DISCOVERY_TIME' ]] Out[31]: DISCOVERY_DATE DISCOVERY_DOY DISCOVERY_TIME FOD_ID 17 2004-10-06 280 1415.0 18 2004-10-13 287 1618.0 40 2005-02-12 43 1520.0 119 2005-07-16 197 1715.0 120 2005-07-16 197 1730.0 ... ... ... ... 400732975 2019-08-09 221 2134.0 400732976 2020-03-01 61 1330.0 400732977 2020-05-13 134 1300.0 400732982 2020-08-17 230 755.0 400732984 2020-11-20 325 1110.0 <p>60713 rows \u00d7 3 columns</p> In\u00a0[32]: Copied! <pre>df\n</pre> df Out[32]: FPA_ID SOURCE_SYSTEM_TYPE SOURCE_SYSTEM NWCG_REPORTING_UNIT_ID NWCG_REPORTING_UNIT_NAME FIRE_CODE FIRE_NAME MTBS_FIRE_NAME COMPLEX_NAME FIRE_YEAR ... CONT_DOY FIRE_SIZE FIRE_SIZE_CLASS LATITUDE LONGITUDE OWNER_DESCR STATE COUNTY FIPS_CODE FIPS_NAME FOD_ID 17 FS-1418878 FED FS-FIRESTAT USCAENF Eldorado National Forest NaN POWER POWER NaN 2004 ... 295.0 16823.0 G 38.523333 -120.211667 USFS CA 5 6005.0 Amador County 18 FS-1418881 FED FS-FIRESTAT USCAENF Eldorado National Forest BHA3 FREDS FREDS NaN 2004 ... 291.0 7700.0 G 38.780000 -120.260000 USFS CA 17 6017.0 El Dorado County 40 FS-1418920 FED FS-FIRESTAT USNCNCF National Forests in North Carolina BKC8 AUSTIN CREEK NaN NaN 2005 ... 44.0 125.0 D 36.001667 -81.590000 MISSING/NOT SPECIFIED NC 27 37027.0 Caldwell County 119 FS-1419153 FED FS-FIRESTAT USNENBF Nebraska National Forest BEW8 THOMPSON BUTTE NaN NaN 2005 ... 198.0 119.0 D 43.899167 -102.954722 USFS SD 103 46103.0 Pennington County 120 FS-1419156 FED FS-FIRESTAT USNENBF Nebraska National Forest BEW8 CHARLES DRAW NaN NaN 2005 ... 197.0 119.0 D 43.892778 -102.948056 USFS SD 103 46103.0 Pennington County ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 400732975 ICS209_2019_10805427 INTERAGCY IA-ICS209 USORMAF Malheur National Forest MMY5 204 COW 204 COW NaN 2019 ... NaN 9668.0 G 44.285050 -118.459800 USFS OR Baker 41001.0 Baker County 400732976 ICS209_2020_11699713 INTERAGCY IA-ICS209 USOKCNA Cherokee Nation Tribe NaN CAMERA NaN NaN 2020 ... NaN 401.0 E 36.303830 -94.903820 UNDEFINED FEDERAL OK Delaware 40041.0 Delaware County 400732977 ICS209_2020_11703752 INTERAGCY IA-ICS209 USFLFLS Florida Forest Service NaN 22ND AVE SE NaN NaN 2020 ... NaN 1000.0 F 26.191111 -81.523889 Private FL Collier 12021.0 Collier County 400732982 ICS209_2020_11831809 INTERAGCY IA-ICS209 USWAMCR Mid Columbia National Wildlife Refuge Complex NaN TAYLOR POND TAYLOR POND NaN 2020 ... 233.0 24892.0 G 46.670340 -120.114500 UNDEFINED FEDERAL WA Yakima 53077.0 Yakima County 400732984 ICS209_2020_11977822 INTERAGCY IA-ICS209 USVAVAF George Washington &amp; Jefferson National Forests NaN MIDDLE MOUNTAIN NaN NaN 2020 ... 327.0 105.0 D 38.578900 -79.148450 UNDEFINED FEDERAL VA Rockingham 51165.0 Rockingham County <p>60713 rows \u00d7 27 columns</p> In\u00a0[33]: Copied! <pre>df.groupby('STATE').FPA_ID.count().nlargest(10).plot(kind='bar', figsize=(12,6))\n</pre> df.groupby('STATE').FPA_ID.count().nlargest(10).plot(kind='bar', figsize=(12,6)) Out[33]: <pre>&lt;Axes: xlabel='STATE'&gt;</pre> In\u00a0[34]: Copied! <pre>df.groupby(df.STATE)\n</pre> df.groupby(df.STATE) Out[34]: <pre>&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x17f832520&gt;</pre> <p>There is a shortcut for doing this with dataframes: you just pass the column name:</p> In\u00a0[35]: Copied! <pre>df.groupby('STATE')\n</pre> df.groupby('STATE') Out[35]: <pre>&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x17f7c5340&gt;</pre> In\u00a0[36]: Copied! <pre>gb = df.groupby('STATE')\ngb\n</pre> gb = df.groupby('STATE') gb Out[36]: <pre>&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x17f832490&gt;</pre> <p>The length tells us how many groups were found:</p> In\u00a0[37]: Copied! <pre>len(gb)\n</pre> len(gb) Out[37]: <pre>50</pre> <p>All of the groups are available as a dictionary via the <code>.groups</code> attribute:</p> In\u00a0[38]: Copied! <pre>groups = gb.groups\nlen(groups)\n</pre> groups = gb.groups len(groups) Out[38]: <pre>50</pre> In\u00a0[39]: Copied! <pre>groups['NJ']\n</pre> groups['NJ'] Out[39]: <pre>Index([   247140,    384407,    578890,    579164,    579296,    580135,\n          580956,    581165,    582330,    582680,\n       ...\n       400271442, 400389794, 400482043, 400528139, 400531379, 400587134,\n       400594776, 400602511, 400613352, 400632920],\n      dtype='int64', name='FOD_ID', length=111)</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[40]: Copied! <pre>list(groups.keys())\n</pre> list(groups.keys()) Out[40]: <pre>['AK',\n 'AL',\n 'AR',\n 'AZ',\n 'CA',\n 'CO',\n 'CT',\n 'DE',\n 'FL',\n 'GA',\n 'HI',\n 'IA',\n 'ID',\n 'IL',\n 'IN',\n 'KS',\n 'KY',\n 'LA',\n 'MA',\n 'MD',\n 'ME',\n 'MI',\n 'MN',\n 'MO',\n 'MS',\n 'MT',\n 'NC',\n 'ND',\n 'NE',\n 'NH',\n 'NJ',\n 'NM',\n 'NV',\n 'NY',\n 'OH',\n 'OK',\n 'OR',\n 'PA',\n 'PR',\n 'SC',\n 'SD',\n 'TN',\n 'TX',\n 'UT',\n 'VA',\n 'VT',\n 'WA',\n 'WI',\n 'WV',\n 'WY']</pre> In\u00a0[41]: Copied! <pre>for key, group in gb:\n    display(group.head())\n    print(f'The key is \"{key}\"')\n    break\n</pre> for key, group in gb:     display(group.head())     print(f'The key is \"{key}\"')     break FPA_ID SOURCE_SYSTEM_TYPE SOURCE_SYSTEM NWCG_REPORTING_UNIT_ID NWCG_REPORTING_UNIT_NAME FIRE_CODE FIRE_NAME MTBS_FIRE_NAME COMPLEX_NAME FIRE_YEAR ... CONT_DOY FIRE_SIZE FIRE_SIZE_CLASS LATITUDE LONGITUDE OWNER_DESCR STATE COUNTY FIPS_CODE FIPS_NAME FOD_ID 6689 FS-1431539 FED FS-FIRESTAT USAKTNF Tongass National Forest BRD1 MUSKEG NaN NaN 2005 ... 126.0 305.0 E 59.087222 -135.441389 STATE OR PRIVATE AK 220 2220.0 Sitka City and Borough 109456 FS-334441 FED FS-FIRESTAT USAKTNF Tongass National Forest NaN MILL NaN NaN 1998 ... 189.0 118.0 D 55.681667 -132.615000 STATE OR PRIVATE AK NaN NaN NaN 147361 FS-374211 FED FS-FIRESTAT USAKCGF Chugach National Forest NaN KENAI LAKE KENAI LAKE NaN 2001 ... 188.0 3260.0 F 60.410278 -149.473611 USFS AK NaN NaN NaN 174677 W-374459 FED DOI-WFMI USAKAKA Alaska Regional Office B391 B391 532391 NaN 1995 ... 226.0 2850.0 F 66.832700 -160.736100 TRIBAL AK NaN NaN NaN 213301 W-36457 FED DOI-WFMI USAKAKD Alaska Fire Service A029 203029 NaN NaN 1992 ... 126.0 170.0 D 57.065900 -154.085700 BIA AK NaN NaN NaN <p>5 rows \u00d7 27 columns</p> <pre>The key is \"AK\"\n</pre> <p>And you can get a specific group by key.</p> In\u00a0[42]: Copied! <pre>gb.get_group('NJ')\n</pre> gb.get_group('NJ') Out[42]: FPA_ID SOURCE_SYSTEM_TYPE SOURCE_SYSTEM NWCG_REPORTING_UNIT_ID NWCG_REPORTING_UNIT_NAME FIRE_CODE FIRE_NAME MTBS_FIRE_NAME COMPLEX_NAME FIRE_YEAR ... CONT_DOY FIRE_SIZE FIRE_SIZE_CLASS LATITUDE LONGITUDE OWNER_DESCR STATE COUNTY FIPS_CODE FIPS_NAME FOD_ID 247140 W-234848 FED DOI-WFMI USPADWP Delaware Water Gap National Recreation Area NaN WORTHINGTO WORTHINGTO NaN 1999 ... 102.0 623.0 E 40.995895 -75.120000 NPS NJ NaN NaN NaN 384407 FWS-2007NJERRDFR2 FED FWS-FMIS USNJERR Edwin B. Forsythe National Wildlife Refuge DFR2 NJ NJFFS WF ASSIST WARREN GROVE WARREN GROVE NaN 2007 ... 141.0 17050.0 G 39.707500 -74.309722 STATE NJ NaN NaN NaN 578890 SFO-2006NJDEPA032704 NONFED ST-NASF USNJNJS New Jersey Forest Fire Service NaN NaN NaN NaN 2006 ... NaN 104.0 D 40.304400 -74.201100 PRIVATE NJ Middlesex 34023.0 Middlesex County 579164 SFO-2006NJDEPB012703 NONFED ST-NASF USNJNJS New Jersey Forest Fire Service NaN RARITAN CENTER NaN NaN 2006 ... NaN 450.0 E 40.296100 -74.214000 PRIVATE NJ Middlesex 34023.0 Middlesex County 579296 SFO-2006NJDEPB032108 NONFED ST-NASF USNJNJS New Jersey Forest Fire Service NaN SUNRISE LAKE NaN NaN 2006 ... NaN 136.0 D 39.482800 -74.510900 STATE NJ Burlington 34005.0 Burlington County ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 400587134 SFO-2020NJDEPA03-200223155509 NONFED ST-NASF USNJNJS New Jersey Forest Fire Service NaN NaN NaN NaN 2020 ... NaN 103.0 D 40.970480 -75.117710 MISSING/NOT SPECIFIED NJ Warren 34041.0 Warren County 400594776 SFO-2020NJDEPC03-200409234633 NONFED ST-NASF USNJNJS New Jersey Forest Fire Service NaN NaN SPLIT DITCH NaN 2020 ... NaN 1518.0 F 39.312640 -75.090240 MISSING/NOT SPECIFIED NJ Cumberland 34011.0 Cumberland County 400602511 SFO-2020NJDEPC06-200519235337 NONFED ST-NASF USNJNJS New Jersey Forest Fire Service NaN NaN BIG TIMBER NaN 2020 ... NaN 2107.0 F 39.651250 -74.892050 MISSING/NOT SPECIFIED NJ Camden 34007.0 Camden County 400613352 SFO-2020NJDEPB09-200709201959 NONFED ST-NASF USNJNJS New Jersey Forest Fire Service NaN NaN NaN NaN 2020 ... NaN 204.0 D 40.111570 -74.412330 MISSING/NOT SPECIFIED NJ Ocean 34029.0 Ocean County 400632920 ICS209_2019_10720324 INTERAGCY IA-ICS209 USNJNJS New Jersey Forest Fire Service NaN SPRING HILL FIRE SPRING HILL FIRE NaN 2019 ... NaN 11638.0 G 39.770000 -74.450000 MISSING/NOT SPECIFIED NJ Burlington 34005.0 Burlington County <p>111 rows \u00d7 27 columns</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>By default, the operation is applied to every column. That's usually not what we want. We can use both <code>.</code> or <code>[]</code> syntax to select a specific column to operate on. Then we get back a series.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[43]: Copied! <pre># Find out the biggest fire size in each state. \ngb.FIRE_SIZE.aggregate(np.max)\n</pre> # Find out the biggest fire size in each state.  gb.FIRE_SIZE.aggregate(np.max) Out[43]: <pre>STATE\nAK    606945.0\nAL      4394.0\nAR      8294.0\nAZ    538049.0\nCA    589368.0\nCO    208913.0\nCT      1300.0\nDE      2000.0\nFL    158000.0\nGA    309200.0\nHI     25000.0\nIA      4000.0\nID    367785.0\nIL      1475.0\nIN      1549.0\nKS     55000.0\nKY     10000.0\nLA     12877.0\nMA       800.0\nMD      3208.0\nME      1092.0\nMI     21069.0\nMN     92682.0\nMO      4761.0\nMS      5717.0\nMT    270723.3\nNC     45294.0\nND     51627.0\nNE     74500.0\nNH       329.0\nNJ     19225.0\nNM    297845.0\nNV    416821.2\nNY      5050.0\nOH      2865.0\nOK    662700.0\nOR    558198.3\nPA      7949.2\nPR      2702.0\nSC     19130.0\nSD     83508.0\nTN     17140.0\nTX    479549.0\nUT    357185.0\nVA     16021.0\nVT       270.0\nWA    255575.0\nWI      7499.4\nWV      5832.0\nWY    176878.0\nName: FIRE_SIZE, dtype: float64</pre> In\u00a0[44]: Copied! <pre>gb.max( numeric_only=True)\n</pre> gb.max( numeric_only=True) Out[44]: FIRE_YEAR DISCOVERY_DOY DISCOVERY_TIME CONT_DOY FIRE_SIZE LATITUDE LONGITUDE FIPS_CODE STATE AK 2020 363 2359.0 365.0 606945.0 70.138100 -132.333333 2290.0 AL 2020 365 2356.0 362.0 4394.0 34.999182 -85.102382 1133.0 AR 2020 365 2330.0 365.0 8294.0 36.499280 -90.531700 5149.0 AZ 2020 362 2359.0 362.0 538049.0 37.050000 -109.020278 4027.0 CA 2020 365 2356.0 366.0 589368.0 41.998889 -114.250800 6115.0 CO 2020 350 2330.0 365.0 208913.0 41.000000 -102.067461 8125.0 CT 2017 316 2201.0 144.0 1300.0 41.781171 -72.475777 9009.0 DE 2005 107 1400.0 103.0 2000.0 39.250000 -75.305800 NaN FL 2020 366 2359.0 365.0 158000.0 30.991900 -80.090000 12133.0 GA 2020 365 2326.0 365.0 309200.0 34.986715 -80.973820 13321.0 HI 2020 361 2337.0 366.0 25000.0 22.135260 -154.903534 15009.0 IA 2020 359 2328.0 359.0 4000.0 43.483347 -90.425753 19195.0 ID 2020 337 2358.0 349.0 367785.0 48.993611 -111.050000 16087.0 IL 2020 350 2248.0 350.0 1475.0 42.400000 -87.804337 17197.0 IN 2020 322 2112.0 322.0 1549.0 41.615600 -85.245469 18137.0 KS 2020 365 2345.0 365.0 55000.0 39.999700 -94.612500 20209.0 KY 2020 358 2359.0 360.0 10000.0 38.717621 -81.990090 21237.0 LA 2020 366 2350.0 364.0 12877.0 32.998195 -89.708889 22127.0 MA 2016 320 1700.0 320.0 800.0 42.719058 -70.524722 25027.0 MD 2020 365 2341.0 365.0 3208.0 39.705251 -75.330750 24047.0 ME 2020 330 2141.0 330.0 1092.0 47.333333 -67.012675 23031.0 MI 2020 309 2100.0 309.0 21069.0 47.420000 -82.835379 26165.0 MN 2020 354 2359.0 347.0 92682.0 48.998941 -90.031667 27171.0 MO 2020 365 2358.0 360.0 4761.0 40.593912 -89.508361 29229.0 MS 2020 365 2330.0 365.0 5717.0 35.034700 -88.166400 28163.0 MT 2020 354 2351.0 365.0 270723.3 48.999200 -104.005700 30111.0 NC 2020 361 2345.0 363.0 45294.0 36.548333 -75.563890 37199.0 ND 2020 353 2350.0 353.0 51627.0 48.995400 -96.867199 38105.0 NE 2020 365 2359.0 365.0 74500.0 43.042806 -95.590920 31175.0 NH 2016 319 1530.0 320.0 329.0 44.071389 -71.197222 33005.0 NJ 2020 353 2300.0 350.0 19225.0 41.187500 -74.063100 34041.0 NM 2020 365 2340.0 365.0 297845.0 37.030278 -102.970000 35061.0 NV 2020 364 2345.0 365.0 416821.2 41.996190 -114.046212 32510.0 NY 2020 324 2339.0 327.0 5050.0 44.876262 -72.708313 36113.0 OH 2020 320 2000.0 323.0 2865.0 41.680000 -81.195625 39163.0 OK 2020 365 2359.0 366.0 662700.0 36.999700 -93.649444 40295.0 OR 2020 352 2358.0 366.0 558198.3 46.066000 -116.516111 41071.0 PA 2020 333 2228.0 334.0 7949.2 41.774440 -74.311800 42129.0 PR 2018 365 2145.0 239.0 2702.0 18.425734 -65.325278 72147.0 SC 2020 365 2300.0 346.0 19130.0 35.150000 -78.766700 45091.0 SD 2020 365 2315.0 365.0 83508.0 45.989340 -96.476360 46137.0 TN 2020 364 2353.0 366.0 17140.0 36.614350 -81.703333 47185.0 TX 2020 365 2330.0 366.0 479549.0 36.615150 -93.541944 48507.0 UT 2020 340 2359.0 356.0 357185.0 41.994444 -109.050700 49057.0 VA 2020 362 2345.0 348.0 16021.0 39.166800 -75.648060 51810.0 VT 2015 142 1220.0 143.0 270.0 43.948333 -72.320833 50027.0 WA 2020 327 2355.0 365.0 255575.0 48.999900 -116.943056 53077.0 WI 2020 345 2153.0 335.0 7499.4 46.583333 -87.629000 55141.0 WV 2020 364 2359.0 365.0 5832.0 39.411365 -78.643929 54109.0 WY 2020 352 2337.0 352.0 176878.0 45.138658 -104.068800 56045.0 In\u00a0[45]: Copied! <pre>gb.FIRE_SIZE.aggregate(np.max).nlargest(10)\n</pre> gb.FIRE_SIZE.aggregate(np.max).nlargest(10) Out[45]: <pre>STATE\nOK    662700.0\nAK    606945.0\nCA    589368.0\nOR    558198.3\nAZ    538049.0\nTX    479549.0\nNV    416821.2\nID    367785.0\nUT    357185.0\nGA    309200.0\nName: FIRE_SIZE, dtype: float64</pre> <p>There are shortcuts for common aggregation functions:</p> In\u00a0[46]: Copied! <pre>gb.FIRE_SIZE.max().nlargest(10)\n</pre> gb.FIRE_SIZE.max().nlargest(10) Out[46]: <pre>STATE\nOK    662700.0\nAK    606945.0\nCA    589368.0\nOR    558198.3\nAZ    538049.0\nTX    479549.0\nNV    416821.2\nID    367785.0\nUT    357185.0\nGA    309200.0\nName: FIRE_SIZE, dtype: float64</pre> In\u00a0[47]: Copied! <pre>gb.FIRE_SIZE.mean().nlargest(10)\n</pre> gb.FIRE_SIZE.mean().nlargest(10) Out[47]: <pre>STATE\nAK    16440.375603\nNV     6138.109191\nOR     5595.310524\nWA     5071.806130\nID     4657.836385\nCA     4086.919845\nMT     3571.759463\nAZ     3084.376006\nUT     2874.009960\nCO     2809.130881\nName: FIRE_SIZE, dtype: float64</pre> <p>We can also apply multiple functions at once:</p> In\u00a0[48]: Copied! <pre>gb.FIRE_SIZE.aggregate([np.max, np.mean])\n</pre> gb.FIRE_SIZE.aggregate([np.max, np.mean]) Out[48]: max mean STATE AK 606945.0 16440.375603 AL 4394.0 270.382601 AR 8294.0 332.404468 AZ 538049.0 3084.376006 CA 589368.0 4086.919845 CO 208913.0 2809.130881 CT 1300.0 285.846154 DE 2000.0 705.000000 FL 158000.0 1273.882815 GA 309200.0 1447.480901 HI 25000.0 1499.055450 IA 4000.0 307.217379 ID 367785.0 4657.836385 IL 1475.0 279.925000 IN 1549.0 287.605128 KS 55000.0 1432.532363 KY 10000.0 365.679315 LA 12877.0 616.683622 MA 800.0 298.357143 MD 3208.0 498.654240 ME 1092.0 275.581250 MI 21069.0 1021.730381 MN 92682.0 849.779631 MO 4761.0 290.755380 MS 5717.0 294.757127 MT 270723.3 3571.759463 NC 45294.0 726.763361 ND 51627.0 691.042681 NE 74500.0 1785.755841 NH 329.0 233.250000 NJ 19225.0 1247.720721 NM 297845.0 2738.802666 NV 416821.2 6138.109191 NY 5050.0 510.412069 OH 2865.0 295.750000 OK 662700.0 1124.858015 OR 558198.3 5595.310524 PA 7949.2 580.139743 PR 2702.0 350.566406 SC 19130.0 342.640423 SD 83508.0 1091.602361 TN 17140.0 394.399034 TX 479549.0 1584.134866 UT 357185.0 2874.009960 VA 16021.0 616.936929 VT 270.0 175.666667 WA 255575.0 5071.806130 WI 7499.4 425.588632 WV 5832.0 372.253441 WY 176878.0 2761.473651 In\u00a0[49]: Copied! <pre>gb.FIRE_SIZE.aggregate([np.median, np.mean]).nlargest(10, 'mean').plot(kind='bar')\n</pre> gb.FIRE_SIZE.aggregate([np.median, np.mean]).nlargest(10, 'mean').plot(kind='bar') Out[49]: <pre>&lt;Axes: xlabel='STATE'&gt;</pre> In\u00a0[50]: Copied! <pre>df.columns\n</pre> df.columns Out[50]: <pre>Index(['FPA_ID', 'SOURCE_SYSTEM_TYPE', 'SOURCE_SYSTEM',\n       'NWCG_REPORTING_UNIT_ID', 'NWCG_REPORTING_UNIT_NAME', 'FIRE_CODE',\n       'FIRE_NAME', 'MTBS_FIRE_NAME', 'COMPLEX_NAME', 'FIRE_YEAR',\n       'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME',\n       'NWCG_CAUSE_CLASSIFICATION', 'NWCG_GENERAL_CAUSE',\n       'NWCG_CAUSE_AGE_CATEGORY', 'CONT_DATE', 'CONT_DOY', 'FIRE_SIZE',\n       'FIRE_SIZE_CLASS', 'LATITUDE', 'LONGITUDE', 'OWNER_DESCR', 'STATE',\n       'COUNTY', 'FIPS_CODE', 'FIPS_NAME'],\n      dtype='object')</pre> In\u00a0[51]: Copied! <pre>df['NWCG_CAUSE_CLASSIFICATION'].unique()\n</pre> df['NWCG_CAUSE_CLASSIFICATION'].unique() Out[51]: <pre>array(['Human', 'Natural', 'Missing data/not specified/undetermined'],\n      dtype=object)</pre> In\u00a0[52]: Copied! <pre>df['NWCG_GENERAL_CAUSE'].unique()\n</pre> df['NWCG_GENERAL_CAUSE'].unique() Out[52]: <pre>array(['Equipment and vehicle use',\n       'Power generation/transmission/distribution',\n       'Debris and open burning', 'Natural',\n       'Missing data/not specified/undetermined',\n       'Recreation and ceremony', 'Smoking',\n       'Railroad operations and maintenance', 'Arson/incendiarism',\n       'Fireworks', 'Other causes', 'Misuse of fire by a minor',\n       'Firearms and explosives use'], dtype=object)</pre> In\u00a0[53]: Copied! <pre>df.groupby('NWCG_GENERAL_CAUSE').count()['FPA_ID'].nlargest(10).plot(kind = 'bar')\n</pre> df.groupby('NWCG_GENERAL_CAUSE').count()['FPA_ID'].nlargest(10).plot(kind = 'bar') Out[53]: <pre>&lt;Axes: xlabel='NWCG_GENERAL_CAUSE'&gt;</pre> In\u00a0[54]: Copied! <pre>df.groupby('NWCG_CAUSE_CLASSIFICATION').count()['FPA_ID'].nlargest().plot(kind = 'bar')\n</pre> df.groupby('NWCG_CAUSE_CLASSIFICATION').count()['FPA_ID'].nlargest().plot(kind = 'bar') Out[54]: <pre>&lt;Axes: xlabel='NWCG_CAUSE_CLASSIFICATION'&gt;</pre> In\u00a0[55]: Copied! <pre>df.groupby('FIRE_YEAR').FPA_ID.count().plot()\n</pre> df.groupby('FIRE_YEAR').FPA_ID.count().plot() Out[55]: <pre>&lt;Axes: xlabel='FIRE_YEAR'&gt;</pre> In\u00a0[59]: Copied! <pre>gb = df.groupby('STATE')\n\ndf_CA = gb.get_group('CA')\n</pre> gb = df.groupby('STATE')  df_CA = gb.get_group('CA') In\u00a0[60]: Copied! <pre>df_CA.groupby('FIRE_YEAR').FIRE_SIZE.sum().plot()\n</pre> df_CA.groupby('FIRE_YEAR').FIRE_SIZE.sum().plot() Out[60]: <pre>&lt;Axes: xlabel='FIRE_YEAR'&gt;</pre> In\u00a0[61]: Copied! <pre>gb = df.groupby(['STATE','FIRE_YEAR'])\nlen(gb)\n</pre> gb = df.groupby(['STATE','FIRE_YEAR']) len(gb) Out[61]: <pre>1231</pre> In\u00a0[62]: Copied! <pre>list(gb.groups.keys())[:100:10]\n</pre> list(gb.groups.keys())[:100:10] Out[62]: <pre>[('AK', 1992),\n ('AK', 2002),\n ('AK', 2012),\n ('AL', 1993),\n ('AL', 2003),\n ('AL', 2013),\n ('AR', 1994),\n ('AR', 2004),\n ('AR', 2014),\n ('AZ', 1995)]</pre> In\u00a0[63]: Copied! <pre>gb.FIRE_SIZE.sum()\n</pre> gb.FIRE_SIZE.sum() Out[63]: <pre>STATE  FIRE_YEAR\nAK     1992         141007.000\n       1993         684669.800\n       1994         259901.600\n       1995          42526.000\n       1996         596706.400\n                       ...    \nWY     2016         254804.700\n       2017         118803.020\n       2018         220718.000\n       2019          41022.200\n       2020         285791.255\nName: FIRE_SIZE, Length: 1231, dtype: float64</pre> In\u00a0[64]: Copied! <pre>### Select group with multiple index must use tuple!\ngb.get_group(('CA', 2003))\n</pre> ### Select group with multiple index must use tuple! gb.get_group(('CA', 2003)) Out[64]: FPA_ID SOURCE_SYSTEM_TYPE SOURCE_SYSTEM NWCG_REPORTING_UNIT_ID NWCG_REPORTING_UNIT_NAME FIRE_CODE FIRE_NAME MTBS_FIRE_NAME COMPLEX_NAME FIRE_YEAR ... CONT_DOY FIRE_SIZE FIRE_SIZE_CLASS LATITUDE LONGITUDE OWNER_DESCR STATE COUNTY FIPS_CODE FIPS_NAME FOD_ID 157551 FS-385211 FED FS-FIRESTAT USCAINF Inyo National Forest NaN DEXTER DEXTER WFU NaN 2003 ... 286.0 2515.0 F 37.831389 -118.795000 USFS CA NaN NaN NaN 158728 FS-386431 FED FS-FIRESTAT USCAPNF Plumas National Forest 4300 ROWLAND NaN NaN 2003 ... 163.0 114.0 D 39.951389 -120.068889 USFS CA NaN NaN NaN 159533 FS-387254 FED FS-FIRESTAT USCASTF Stanislaus National Forest 7648 MUDD MUD WFU MUD COMPLEX 2003 ... 300.0 4102.0 F 38.424722 -119.961111 USFS CA NaN NaN NaN 159534 FS-387255 FED FS-FIRESTAT USCASTF Stanislaus National Forest 5555 WHITT WHITT MUD COMPLEX 2003 ... 300.0 1014.0 F 38.378056 -119.999722 USFS CA NaN NaN NaN 160202 FS-388122 FED FS-FIRESTAT USCALPF Los Padres National Forest 2996 DEL VENTURI NaN NaN 2003 ... 225.0 861.0 E 36.071111 -121.390000 MISSING/NOT SPECIFIED CA NaN NaN NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 15000827 ICS209_2003_CA-KRN-37559 INTERAGCY IA-ICS209 USCAKRN Kern County Fire Department NaN HILLSIDE UNNAMED NaN 2003 ... 198.0 835.0 E 35.168056 -118.468056 MISSING/NOT SPECIFIED CA KERN 6029.0 Kern County 15000828 ICS209_2003_CA-KRN-38766 INTERAGCY IA-ICS209 USCAKRN Kern County Fire Department NaN MERCURY UNNAMED NaN 2003 ... 203.0 340.0 E 35.200556 -118.518611 MISSING/NOT SPECIFIED CA KERN 6029.0 Kern County 15000829 ICS209_2003_CA-LAC-03004054 INTERAGCY IA-ICS209 USCALAC Los Angeles County Fire Department NaN AIRPORT NaN NaN 2003 ... 8.0 245.0 D 33.407778 -118.402500 MISSING/NOT SPECIFIED CA LOS ANGELES 6037.0 Los Angeles County 201940026 ICS209_2003-CA-KRN-0333259 INTERAGCY IA-ICS209 USCAKRN Kern County Fire Department NaN TEJON TEJON NaN 2003 ... 183.0 1155.0 F 34.871389 -118.882778 MISSING/NOT SPECIFIED CA Kern 6029.0 Kern County 400280041 ICS209_2003_CA-KRN-33853 INTERAGCY IA-ICS209 USCAKRN Kern County Fire Department NaN GRAPEVINE GRAPEVINE NaN 2003 ... NaN 1830.0 F 34.916944 -118.918333 MISSING/NOT SPECIFIED CA Kern 6029.0 Kern County <p>210 rows \u00d7 27 columns</p> In\u00a0[65]: Copied! <pre>gb.get_group(('CA', 2003)).describe()\n</pre> gb.get_group(('CA', 2003)).describe() Out[65]: FIRE_YEAR DISCOVERY_DATE DISCOVERY_DOY DISCOVERY_TIME CONT_DOY FIRE_SIZE LATITUDE LONGITUDE FIPS_CODE count 210.0 210 210.000000 210.000000 198.000000 210.000000 210.000000 210.000000 116.000000 mean 2003.0 2003-08-04 13:42:51.428571392 216.571429 1317.295238 223.934343 4823.423333 37.258685 -120.151254 6052.965517 min 2003.0 2003-01-06 00:00:00 6.000000 10.000000 7.000000 101.000000 32.566700 -124.091100 6001.000000 25% 2003.0 2003-07-05 00:00:00 186.000000 1129.000000 185.000000 200.000000 35.580192 -121.577778 6029.000000 50% 2003.0 2003-07-30 12:00:00 211.500000 1348.500000 216.000000 360.000000 37.188207 -120.191944 6045.000000 75% 2003.0 2003-09-03 00:00:00 246.000000 1532.250000 255.000000 1224.500000 39.145239 -118.755139 6073.000000 max 2003.0 2003-11-28 00:00:00 332.000000 2320.000000 365.000000 280059.000000 41.998333 -114.660556 6113.000000 std 0.0 NaN 50.829739 418.434515 59.655787 22885.276576 2.302258 1.918151 30.874763 In\u00a0[66]: Copied! <pre>### Find out the largest fire in CA, 2020\ndf.loc[gb['FIRE_SIZE'].idxmax().loc['CA',2020]]\n</pre> ### Find out the largest fire in CA, 2020 df.loc[gb['FIRE_SIZE'].idxmax().loc['CA',2020]] Out[66]: <pre>FPA_ID                           IRW-2020-CAMNF-000730\nSOURCE_SYSTEM_TYPE                           INTERAGCY\nSOURCE_SYSTEM                                 IA-IRWIN\nNWCG_REPORTING_UNIT_ID                         USCAMNF\nNWCG_REPORTING_UNIT_NAME     Mendocino National Forest\nFIRE_CODE                                         NFP4\nFIRE_NAME                                          DOE\nMTBS_FIRE_NAME                          AUGUST COMPLEX\nCOMPLEX_NAME                            AUGUST COMPLEX\nFIRE_YEAR                                         2020\nDISCOVERY_DATE                     2020-08-16 00:00:00\nDISCOVERY_DOY                                      229\nDISCOVERY_TIME                                     NaN\nNWCG_CAUSE_CLASSIFICATION                      Natural\nNWCG_GENERAL_CAUSE                             Natural\nNWCG_CAUSE_AGE_CATEGORY                            NaN\nCONT_DATE                                   11/11/2020\nCONT_DOY                                         316.0\nFIRE_SIZE                                     589368.0\nFIRE_SIZE_CLASS                                      G\nLATITUDE                                     39.765255\nLONGITUDE                                  -122.672914\nOWNER_DESCR                                       USFS\nSTATE                                               CA\nCOUNTY                                           Glenn\nFIPS_CODE                                       6021.0\nFIPS_NAME                                 Glenn County\nName: 400629554, dtype: object</pre> In\u00a0[57]: Copied! <pre>df.groupby(['FIRE_YEAR','NWCG_CAUSE_CLASSIFICATION']).FPA_ID.count().unstack('NWCG_CAUSE_CLASSIFICATION').plot(kind = 'bar', stacked = True)\n</pre> df.groupby(['FIRE_YEAR','NWCG_CAUSE_CLASSIFICATION']).FPA_ID.count().unstack('NWCG_CAUSE_CLASSIFICATION').plot(kind = 'bar', stacked = True) Out[57]: <pre>&lt;Axes: xlabel='FIRE_YEAR'&gt;</pre> In\u00a0[56]: Copied! <pre>df.groupby(['FIRE_YEAR','NWCG_CAUSE_CLASSIFICATION']).FPA_ID.count().plot(kind = 'bar')\n</pre> df.groupby(['FIRE_YEAR','NWCG_CAUSE_CLASSIFICATION']).FPA_ID.count().plot(kind = 'bar') Out[56]: <pre>&lt;Axes: xlabel='FIRE_YEAR,NWCG_CAUSE_CLASSIFICATION'&gt;</pre> In\u00a0[70]: Copied! <pre>df.groupby('NWCG_GENERAL_CAUSE').aggregate(total_size = ('FIRE_SIZE', 'sum'))\n</pre>  df.groupby('NWCG_GENERAL_CAUSE').aggregate(total_size = ('FIRE_SIZE', 'sum')) Out[70]: total_size NWCG_GENERAL_CAUSE Arson/incendiarism 8.403065e+06 Debris and open burning 5.211647e+06 Equipment and vehicle use 9.101695e+06 Firearms and explosives use 6.280094e+05 Fireworks 5.020915e+05 Missing data/not specified/undetermined 3.140607e+07 Misuse of fire by a minor 3.598069e+05 Natural 1.044037e+08 Other causes 4.257120e+05 Power generation/transmission/distribution 3.368717e+06 Railroad operations and maintenance 7.531811e+05 Recreation and ceremony 4.408299e+06 Smoking 7.751060e+05 In\u00a0[71]: Copied! <pre>df.groupby('NWCG_GENERAL_CAUSE').aggregate(total_incidents = ('FPA_ID', 'count'))\n</pre> df.groupby('NWCG_GENERAL_CAUSE').aggregate(total_incidents = ('FPA_ID', 'count')) Out[71]: total_incidents NWCG_GENERAL_CAUSE Arson/incendiarism 10056 Debris and open burning 7299 Equipment and vehicle use 5165 Firearms and explosives use 283 Fireworks 433 Missing data/not specified/undetermined 14204 Misuse of fire by a minor 345 Natural 18802 Other causes 117 Power generation/transmission/distribution 1322 Railroad operations and maintenance 858 Recreation and ceremony 1237 Smoking 592 In\u00a0[72]: Copied! <pre># Applying different functions to dataframe columns\ndf.groupby(['NWCG_GENERAL_CAUSE']).aggregate({'FIRE_SIZE':'sum', 'FPA_ID':'count'})\n</pre> # Applying different functions to dataframe columns df.groupby(['NWCG_GENERAL_CAUSE']).aggregate({'FIRE_SIZE':'sum', 'FPA_ID':'count'}) Out[72]: FIRE_SIZE FPA_ID NWCG_GENERAL_CAUSE Arson/incendiarism 8.403065e+06 10056 Debris and open burning 5.211647e+06 7299 Equipment and vehicle use 9.101695e+06 5165 Firearms and explosives use 6.280094e+05 283 Fireworks 5.020915e+05 433 Missing data/not specified/undetermined 3.140607e+07 14204 Misuse of fire by a minor 3.598069e+05 345 Natural 1.044037e+08 18802 Other causes 4.257120e+05 117 Power generation/transmission/distribution 3.368717e+06 1322 Railroad operations and maintenance 7.531811e+05 858 Recreation and ceremony 4.408299e+06 1237 Smoking 7.751060e+05 592 In\u00a0[73]: Copied! <pre># Take the 1st row from each group\ndf.groupby(['FIRE_YEAR']).nth(10)\n</pre> # Take the 1st row from each group df.groupby(['FIRE_YEAR']).nth(10) Out[73]: FPA_ID SOURCE_SYSTEM_TYPE SOURCE_SYSTEM NWCG_REPORTING_UNIT_ID NWCG_REPORTING_UNIT_NAME FIRE_CODE FIRE_NAME MTBS_FIRE_NAME COMPLEX_NAME FIRE_YEAR ... CONT_DOY FIRE_SIZE FIRE_SIZE_CLASS LATITUDE LONGITUDE OWNER_DESCR STATE COUNTY FIPS_CODE FIPS_NAME FOD_ID 745 FS-1420517 FED FS-FIRESTAT USMTCGF Custer Gallatin National Forest BK2U LAMPKIN NaN NaN 2005 ... 89.0 236.0 D 45.744722 -104.180000 USFS MT 11 30011.0 Carter County 7175 FS-1432509 FED FS-FIRESTAT USMSMNF National Forests in Mississippi B76Z OP-6 NaN NaN 2006 ... 13.0 200.0 D 31.091944 -89.079167 USFS MS 111 28111.0 Perry County 18178 FS-1447814 FED FS-FIRESTAT USGACHF Chattahoochee-Oconee National Forest C67F FORESTRY TRAIL NaN NaN 2007 ... 83.0 117.0 D 34.573333 -85.168889 STATE OR PRIVATE GA 55 13055.0 Chattooga County 26970 FS-1458681 FED FS-FIRESTAT USMOMTF Mark Twain National Forest D4LW BOWLING PIN NaN NaN 2008 ... 84.0 113.0 D 37.748611 -90.998889 STATE OR PRIVATE MO 221 29221.0 Washington County 34327 FS-1475293 FED FS-FIRESTAT USNMCIF Cibola National Forest ER0H ELENA ELENA NaN 2009 ... 70.0 2400.0 F 35.636944 -99.819167 STATE OR PRIVATE OK 129 40129.0 Roger Mills County 43014 FS-256761 FED FS-FIRESTAT USMTKNF Kootenai National Forest NaN THREE GOATS NaN NaN 1992 ... 222.0 209.0 D 48.401667 -116.080000 USFS ID NaN NaN NaN 54983 FS-279512 FED FS-FIRESTAT USNMCIF Cibola National Forest NaN NaN NaN NaN 1993 ... 175.0 106.0 D 34.133333 -107.951667 USFS NM NaN NaN NaN 61790 FS-286434 FED FS-FIRESTAT USMTBRF Bitterroot National Forest NaN LITTLE CLEARWATER BITTER-NEZ COMPLEX (MAGRUDER) BITTER-NEZ COMPLEX 1994 ... 297.0 785.0 E 45.710000 -114.838333 USFS ID NaN NaN NaN 77216 FS-301978 FED FS-FIRESTAT USCORGF Rio Grande National Forest NaN JOHN NaN NaN 1995 ... 211.0 201.0 D 37.066667 -105.816667 USFS CO NaN NaN NaN 85678 FS-310483 FED FS-FIRESTAT USLAKIF Kisatchie National Forest NaN 085 SIMMONS ROAD NaN NaN 1996 ... 75.0 124.0 D 31.650000 -92.533333 USFS LA NaN NaN NaN 97767 FS-322642 FED FS-FIRESTAT USSCFMF Francis Marion &amp; Sumter National Forests NaN BUCK HALL NaN NaN 1997 ... 91.0 151.0 D 33.033333 -79.550000 USFS SC NaN NaN NaN 105736 FS-330703 FED FS-FIRESTAT USFLFNF National Forests in Florida 5422 OAK OAK NaN 1998 ... 198.0 20100.0 G 30.340000 -82.506667 USFS FL NaN NaN NaN 115178 FS-340214 FED FS-FIRESTAT USCASTF Stanislaus National Forest 7687 PILOT PILOT NaN 1999 ... 242.0 4028.0 F 37.823056 -120.017500 STATE OR PRIVATE CA NaN NaN NaN 125447 FS-350535 FED FS-FIRESTAT USAZCOF Coconino National Forest 2300 GULF NaN NaN 2000 ... 203.0 1000.0 F 34.583889 -111.467500 USFS AZ 5 4005.0 Coconino County 137257 FS-363242 FED FS-FIRESTAT USFLFNF National Forests in Florida 5220 HARMS ROAD/TLH SUPPO NaN NaN 2001 ... 67.0 209.0 D 30.037222 -84.498056 STATE OR PRIVATE FL 129 12129.0 Wakulla County 147261 FS-374101 FED FS-FIRESTAT USKYDBF Daniel Boone National Forest 4563 CANE CREEK CANE CREEK NaN 2002 ... 102.0 990.0 E 36.778889 -84.315000 USFS KY 235 21235.0 Whitley County 157440 FS-385097 FED FS-FIRESTAT USMIHIF Hiawatha National Forest NaN LOVEGROVE NaN NaN 2003 ... 119.0 172.0 D 46.145833 -84.954167 STATE OR PRIVATE MI NaN NaN NaN 166506 FS-394950 FED FS-FIRESTAT USAZTNF Tonto National Forest A02B WEBBER WEBBER NaN 2004 ... 115.0 4311.0 F 34.426111 -111.367500 USFS AZ 7 4007.0 Gila County 1326854 SFO-2010-CACDFTUU000251 NONFED ST-CACDF USCATUU Tulare Unit NaN CURVE NaN NaN 2010 ... NaN 108.0 D 36.031944 -118.858056 MISSING/NOT SPECIFIED CA TULARE 6107.0 Tulare County 20020200 FS-1493543 FED FS-FIRESTAT USMOMTF Mark Twain National Forest F1CR BETHEL NaN NaN 2011 ... 71.0 433.0 E 36.788611 -92.763333 USFS MO NaN NaN NaN 201430181 FS-1509564 FED FS-FIRESTAT USMTBRF Bitterroot National Forest EKS4 BURNT STRIP BURNT STRIP NaN 2012 ... 296.0 3570.0 F 45.847778 -114.646667 USFS ID 049 16049.0 Idaho County 201760265 FS-1519193 FED FS-FIRESTAT USCATNF Tahoe National Forest HU11 AMERICAN AMERICAN NaN 2013 ... 281.0 27440.0 G 39.118889 -120.646111 USFS CA 061 6061.0 Placer County 300000132 FS-1527353 FED FS-FIRESTAT USIDNCF Nez Perce - Clearwater National Forests JDT1 ELEVATOR ELEVATOR MOUNTAIN SELWAY COMPLEX 2014 ... 297.0 4227.0 F 45.970833 -114.818056 USFS ID Idaho 16049.0 Idaho County 300200527 FS-6352433 FED FS-FIRESTAT USIDIPF Idaho Panhandle National Forest J114 GRASSY MOUNTAIN NaN GRIZZLY COMPLEX 2015 ... NaN 860.0 E 47.785000 -116.205278 USFS ID 079 16079.0 Shoshone County 400000402 FS-6738266 FED FS-FIRESTAT USWYMRF Medicine Bow-Routt National Forests, Thunder B... KL7H BROADWAY BROADWAY NaN 2016 ... NaN 2121.0 F 41.041389 -106.762778 USFS WY 007 56007.0 Carbon County 400006154 FS-6824721 FED FS-FIRESTAT USMTFNF Flathead National Forest EKS8 DOLLY VARDEN NaN NaN 2017 ... NaN 424.0 E 48.012222 -113.178333 USFS MT 029 30029.0 Flathead County 400300244 FS-6885846 FED FS-FIRESTAT USWYMRF Medicine Bow-Routt National Forests, Thunder B... L24P GLENDO FIRE NaN NaN 2018 ... 223.0 137.1 D 42.446667 -105.241667 PRIVATE WY 031 56031.0 Platte County 400500130 FS-6954521 FED FS-FIRESTAT USNMGNF Gila National Forest MDB3 BLACK BLACK NaN 2019 ... 227.0 1235.0 F 33.163611 -107.905278 USFS NM 017 35017.0 Grant County 400510712 W-736893 FED DOI-WFMI USOKANA Anadarko Agency MZY5 WEST CACHE NaN NaN 2020 ... 8.0 170.0 D 34.612610 -98.650800 PRIVATE OK NaN NaN NaN <p>29 rows \u00d7 27 columns</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[74]: Copied! <pre>df = pd.read_csv('../Assignments/Assignment_3/Millbrook_NY_daily_weather.csv')\n</pre> df = pd.read_csv('../Assignments/Assignment_3/Millbrook_NY_daily_weather.csv') In\u00a0[75]: Copied! <pre>df.head()\n</pre> df.head() Out[75]: LST_DATE WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 0 2016-01-01 64756 2.422 -73.74 41.79 3.4 -0.5 1.5 1.3 0.0 ... 0.233 0.204 0.155 0.147 4.2 4.4 5.1 6.0 7.6 NaN 1 2016-01-02 64756 2.422 -73.74 41.79 2.9 -3.6 -0.4 -0.3 0.0 ... 0.227 0.199 0.152 0.144 2.8 3.1 4.2 5.7 7.4 NaN 2 2016-01-03 64756 2.422 -73.74 41.79 5.1 -1.8 1.6 1.1 0.0 ... 0.223 0.196 0.151 0.141 2.6 2.8 3.8 5.2 7.2 NaN 3 2016-01-04 64756 2.422 -73.74 41.79 0.5 -14.4 -6.9 -7.5 0.0 ... 0.220 0.194 0.148 0.139 1.7 2.1 3.4 4.9 6.9 NaN 4 2016-01-05 64756 2.422 -73.74 41.79 -5.2 -15.5 -10.3 -11.7 0.0 ... 0.213 0.191 0.148 0.138 0.4 0.9 2.4 4.3 6.6 NaN <p>5 rows \u00d7 29 columns</p> In\u00a0[76]: Copied! <pre>df.describe()\n</pre> df.describe() Out[76]: WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC SOLARAD_DAILY ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 count 2557.0 2557.000000 2557.00 2.557000e+03 2545.000000 2545.000000 2545.000000 2545.000000 2547.000000 2545.000000 ... 2173.000000 2281.000000 2545.000000 2427.000000 2545.000000 2545.000000 2542.000000 2545.000000 2544.000000 0.0 mean 64756.0 2.549059 -73.74 4.179000e+01 15.973635 4.037642 10.003733 10.130059 3.216726 12.908444 ... 0.189869 0.192877 0.150320 0.156753 12.382318 12.353084 12.155901 12.077367 11.994340 NaN std 0.0 0.518602 0.00 7.106817e-15 10.648586 9.568906 9.865217 9.721821 8.131592 8.015033 ... 0.071355 0.073708 0.029615 0.022373 9.531614 9.468884 8.960360 8.190971 7.334731 NaN min 64756.0 -9.000000 -73.74 4.179000e+01 -12.300000 -26.000000 -18.400000 -19.200000 0.000000 0.030000 ... 0.029000 0.030000 0.070000 0.023000 -1.800000 -1.600000 -0.500000 0.500000 1.400000 NaN 25% 64756.0 2.422000 -73.74 4.179000e+01 6.900000 -3.100000 2.000000 2.100000 0.000000 6.070000 ... 0.141000 0.144000 0.133000 0.145000 2.700000 2.800000 3.125000 3.800000 4.700000 NaN 50% 64756.0 2.622000 -73.74 4.179000e+01 16.900000 3.900000 10.200000 10.500000 0.000000 11.820000 ... 0.216000 0.211000 0.157000 0.160000 12.300000 12.200000 12.000000 12.000000 11.900000 NaN 75% 64756.0 2.622000 -73.74 4.179000e+01 25.200000 11.900000 18.700000 18.800000 2.150000 19.450000 ... 0.244000 0.246000 0.170000 0.170000 21.800000 21.700000 21.075000 20.100000 19.100000 NaN max 64756.0 2.622000 -73.74 4.179000e+01 36.500000 23.400000 28.900000 28.400000 133.400000 31.250000 ... 0.359000 0.335000 0.223000 0.218000 28.600000 28.500000 27.100000 25.400000 23.500000 NaN <p>8 rows \u00d7 27 columns</p> In\u00a0[77]: Copied! <pre>df.columns\n</pre> df.columns Out[77]: <pre>Index(['LST_DATE', 'WBANNO', 'CRX_VN', 'LONGITUDE', 'LATITUDE', 'T_DAILY_MAX',\n       'T_DAILY_MIN', 'T_DAILY_MEAN', 'T_DAILY_AVG', 'P_DAILY_CALC',\n       'SOLARAD_DAILY', 'SUR_TEMP_DAILY_TYPE', 'SUR_TEMP_DAILY_MAX',\n       'SUR_TEMP_DAILY_MIN', 'SUR_TEMP_DAILY_AVG', 'RH_DAILY_MAX',\n       'RH_DAILY_MIN', 'RH_DAILY_AVG', 'SOIL_MOISTURE_5_DAILY',\n       'SOIL_MOISTURE_10_DAILY', 'SOIL_MOISTURE_20_DAILY',\n       'SOIL_MOISTURE_50_DAILY', 'SOIL_MOISTURE_100_DAILY',\n       'SOIL_TEMP_5_DAILY', 'SOIL_TEMP_10_DAILY', 'SOIL_TEMP_20_DAILY',\n       'SOIL_TEMP_50_DAILY', 'SOIL_TEMP_100_DAILY', 'Unnamed: 28'],\n      dtype='object')</pre> In\u00a0[78]: Copied! <pre>df['LST_DATE'] = pd.to_datetime(df['LST_DATE'])\n</pre> df['LST_DATE'] = pd.to_datetime(df['LST_DATE']) In\u00a0[79]: Copied! <pre>df = df.drop(columns = ['SUR_TEMP_DAILY_TYPE'])\n</pre> df = df.drop(columns = ['SUR_TEMP_DAILY_TYPE']) In\u00a0[80]: Copied! <pre>df = df.set_index('LST_DATE')\n</pre> df = df.set_index('LST_DATE') In\u00a0[81]: Copied! <pre>df\n</pre> df Out[81]: WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC SOLARAD_DAILY ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 LST_DATE 2016-01-01 64756 2.422 -73.74 41.79 3.4 -0.5 1.5 1.3 0.0 1.69 ... 0.233 0.204 0.155 0.147 4.2 4.4 5.1 6.0 7.6 NaN 2016-01-02 64756 2.422 -73.74 41.79 2.9 -3.6 -0.4 -0.3 0.0 6.25 ... 0.227 0.199 0.152 0.144 2.8 3.1 4.2 5.7 7.4 NaN 2016-01-03 64756 2.422 -73.74 41.79 5.1 -1.8 1.6 1.1 0.0 5.69 ... 0.223 0.196 0.151 0.141 2.6 2.8 3.8 5.2 7.2 NaN 2016-01-04 64756 2.422 -73.74 41.79 0.5 -14.4 -6.9 -7.5 0.0 9.17 ... 0.220 0.194 0.148 0.139 1.7 2.1 3.4 4.9 6.9 NaN 2016-01-05 64756 2.422 -73.74 41.79 -5.2 -15.5 -10.3 -11.7 0.0 9.34 ... 0.213 0.191 0.148 0.138 0.4 0.9 2.4 4.3 6.6 NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2022-12-27 64756 2.622 -73.74 41.79 -0.8 -8.0 -4.4 -3.8 0.0 4.00 ... NaN NaN 0.164 0.157 -0.4 -0.2 0.5 2.2 4.0 NaN 2022-12-28 64756 2.622 -73.74 41.79 7.4 -6.1 0.7 1.3 0.0 7.73 ... NaN NaN 0.162 0.156 -0.4 -0.3 0.4 2.1 3.8 NaN 2022-12-29 64756 2.622 -73.74 41.79 10.7 -1.8 4.4 5.0 0.0 6.66 ... NaN NaN 0.159 0.155 -0.3 -0.3 0.3 1.9 3.7 NaN 2022-12-30 64756 2.622 -73.74 41.79 16.6 4.9 10.7 10.3 0.0 5.39 ... NaN NaN 0.159 0.154 -0.2 -0.2 0.3 1.8 3.6 NaN 2022-12-31 64756 2.622 -73.74 41.79 13.2 2.7 7.9 10.2 5.0 1.25 ... NaN NaN 0.160 0.153 -0.1 -0.2 0.3 1.8 3.4 NaN <p>2557 rows \u00d7 27 columns</p> <p>This timeseries has daily resolution, and the daily plots are somewhat noisy.</p> In\u00a0[82]: Copied! <pre>df.T_DAILY_MEAN.plot()\n</pre> df.T_DAILY_MEAN.plot() Out[82]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> <p>A common way to analyze such data in climate science is to create a \"climatology,\" which contains the average values in each month or day of the year. We can do this easily with groupby. Recall that df.index is a pandas DateTimeIndex object.</p> In\u00a0[83]: Copied! <pre>monthly_climatology = df.groupby(df.index.month).mean(numeric_only=True)\nmonthly_climatology\n</pre> monthly_climatology = df.groupby(df.index.month).mean(numeric_only=True) monthly_climatology Out[83]: WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC SOLARAD_DAILY ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 LST_DATE 1 64756.0 2.564857 -73.74 41.79 2.200000 -7.550463 -2.678241 -2.442130 2.217130 5.655139 ... 0.256699 0.230433 0.164727 0.166809 0.393056 0.472685 0.963889 1.983796 3.371759 NaN 2 64756.0 2.564424 -73.74 41.79 4.866162 -5.901010 -0.519192 -0.220202 3.449495 8.359444 ... 0.250415 0.219145 0.163520 0.165425 0.557071 0.518687 0.707071 1.256566 2.156061 NaN 3 64756.0 2.564857 -73.74 41.79 9.015668 -2.634101 3.184793 3.370046 2.426728 12.813917 ... 0.240800 0.224130 0.168618 0.166180 3.385714 3.270507 3.157604 3.182488 3.337788 NaN 4 64756.0 2.564857 -73.74 41.79 14.930476 1.948095 8.438095 8.733810 3.217143 14.977381 ... 0.229071 0.235376 0.165395 0.165433 9.685714 9.460476 8.950000 8.119524 7.199524 NaN 5 64756.0 2.564857 -73.74 41.79 20.996774 8.094009 14.548848 14.772811 3.182949 17.912673 ... 0.208286 0.221042 0.156848 0.161083 16.721198 16.471889 15.606019 14.184793 12.509217 NaN 6 64756.0 2.564857 -73.74 41.79 25.874286 12.033810 18.952857 19.285714 2.290000 21.610095 ... 0.138119 0.162114 0.136386 0.153190 22.399524 22.177619 21.112381 19.530476 17.661429 NaN 7 64756.0 2.564857 -73.74 41.79 29.040092 15.894470 22.465899 22.322120 3.880184 20.864101 ... 0.106465 0.113350 0.118535 0.139206 25.527189 25.406912 24.316204 22.814286 21.129630 NaN 8 64756.0 2.297069 -73.74 41.79 28.139048 15.543810 21.838571 21.615714 3.969048 18.131429 ... 0.134129 0.128214 0.122652 0.136627 24.912857 24.918095 24.244762 23.358571 22.286190 NaN 9 64756.0 2.564857 -73.74 41.79 23.720096 11.202871 17.460287 17.344019 3.948804 14.033301 ... 0.144627 0.151890 0.127550 0.141841 20.640191 20.736364 20.594258 20.666986 20.554067 NaN 10 64756.0 2.590664 -73.74 41.79 17.773488 5.738140 11.753023 11.740465 3.637963 9.193674 ... 0.190428 0.187633 0.141386 0.144750 14.663256 14.799070 15.074419 15.892558 16.645581 NaN 11 64756.0 2.593429 -73.74 41.79 10.377512 -1.277990 4.547368 4.720574 3.301905 6.567895 ... 0.234833 0.237167 0.166742 0.161895 7.128708 7.302392 7.955288 9.385167 10.990909 NaN 12 64756.0 2.593429 -73.74 41.79 4.488018 -4.917972 -0.217512 0.055760 3.121198 4.639954 ... 0.254506 0.242934 0.171530 0.170230 2.254839 2.385714 2.946083 4.288479 5.912442 NaN <p>12 rows \u00d7 27 columns</p> <p>Each row in this new dataframe respresents the average values for the months (1=January, 2=February, etc.)</p> <p>We can apply more customized aggregations, as with any groupby operation. Below we keep the mean of the mean, max of the max, and min of the min for the temperature measurements.</p> In\u00a0[84]: Copied! <pre>monthly_T_climatology = df.groupby(df.index.month).aggregate({'T_DAILY_MEAN': 'mean',\n                                                              'T_DAILY_MAX': 'max',\n                                                              'T_DAILY_MIN': 'min'})\nmonthly_T_climatology\n</pre> monthly_T_climatology = df.groupby(df.index.month).aggregate({'T_DAILY_MEAN': 'mean',                                                               'T_DAILY_MAX': 'max',                                                               'T_DAILY_MIN': 'min'}) monthly_T_climatology Out[84]: T_DAILY_MEAN T_DAILY_MAX T_DAILY_MIN LST_DATE 1 -2.678241 19.8 -26.0 2 -0.519192 24.9 -24.7 3 3.184793 26.8 -17.4 4 8.438095 30.6 -11.3 5 14.548848 33.4 -3.1 6 18.952857 34.5 1.5 7 22.465899 36.2 8.2 8 21.838571 36.5 6.0 9 17.460287 32.7 -1.6 10 11.753023 29.9 -5.9 11 4.547368 24.4 -15.9 12 -0.217512 17.9 -21.8 In\u00a0[85]: Copied! <pre>monthly_T_climatology.plot(marker='o')\n</pre> monthly_T_climatology.plot(marker='o') Out[85]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> <p>If we want to do it on a finer scale, we can group by day of year.</p> In\u00a0[86]: Copied! <pre>daily_T_climatology = df.groupby(df.index.dayofyear).aggregate({'T_DAILY_MEAN': 'mean',\n                                                            'T_DAILY_MAX': 'max',\n                                                            'T_DAILY_MIN': 'min'})\ndaily_T_climatology.plot(marker='.')\n</pre> daily_T_climatology = df.groupby(df.index.dayofyear).aggregate({'T_DAILY_MEAN': 'mean',                                                             'T_DAILY_MAX': 'max',                                                             'T_DAILY_MIN': 'min'}) daily_T_climatology.plot(marker='.') Out[86]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> <p>A common mode of analysis in climate science is to remove the climatology from a signal to focus only on the \"anomaly\" values. This can be accomplished with transformation.</p> In\u00a0[87]: Copied! <pre>def standardize(x):\n    return (x - x.mean())/x.std()\n\nanomaly = df.groupby(df.index.month).transform(standardize)\n</pre> def standardize(x):     return (x - x.mean())/x.std()  anomaly = df.groupby(df.index.month).transform(standardize)  In\u00a0[88]: Copied! <pre>anomaly.plot(y='T_DAILY_MEAN')\n</pre> anomaly.plot(y='T_DAILY_MEAN') Out[88]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> In\u00a0[89]: Copied! <pre>anomaly\n</pre> anomaly Out[89]: WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC SOLARAD_DAILY ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 LST_DATE 2016-01-01 NaN -1.577491 NaN NaN 0.208563 1.082782 0.727292 0.654996 -0.398067 -1.242257 ... -0.992876 -0.857404 -0.607020 -1.209148 3.178622 3.416310 3.829867 3.857182 3.970999 NaN 2016-01-02 NaN -1.577491 NaN NaN 0.121662 0.606696 0.396565 0.374943 -0.398067 0.186367 ... -1.244252 -1.019588 -0.794239 -1.392266 2.009687 2.285460 2.996505 3.569060 3.783166 NaN 2016-01-03 NaN -1.577491 NaN NaN 0.504027 0.883133 0.744698 0.619989 -0.398067 0.010922 ... -1.411835 -1.116899 -0.856646 -1.575385 1.842696 2.024495 2.626121 3.088858 3.595334 NaN 2016-01-04 NaN -1.577491 NaN NaN -0.295464 -1.051924 -0.734867 -0.885294 -0.398067 1.101187 ... -1.537523 -1.181773 -1.043866 -1.697463 1.091238 1.415576 2.255738 2.800736 3.313586 NaN 2016-01-05 NaN -1.577491 NaN NaN -1.286137 -1.220858 -1.326693 -1.620432 -0.398067 1.154447 ... -1.830794 -1.279083 -1.043866 -1.758503 0.005798 0.371715 1.329779 2.224493 3.031838 NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2022-12-27 NaN 0.407307 NaN NaN -0.963856 -0.556494 -0.802459 -0.738821 -0.464967 -0.249287 ... NaN NaN -0.529551 -1.086642 -1.375941 -1.382812 -1.435337 -1.298599 -1.289971 NaN 2022-12-28 NaN 0.407307 NaN NaN 0.530772 -0.213428 0.176035 0.238415 -0.464967 1.203695 ... NaN NaN -0.670202 -1.168774 -1.375941 -1.436291 -1.494016 -1.360778 -1.424874 NaN 2022-12-29 NaN 0.407307 NaN NaN 1.132268 0.562984 0.885923 0.947390 -0.464967 0.786888 ... NaN NaN -0.881180 -1.250906 -1.324114 -1.436291 -1.552695 -1.485136 -1.492325 NaN 2022-12-30 NaN 0.407307 NaN NaN 2.207671 1.772744 2.094651 1.962948 -0.464967 0.292173 ... NaN NaN -0.881180 -1.333038 -1.272286 -1.382812 -1.552695 -1.547315 -1.559777 NaN 2022-12-31 NaN 0.407307 NaN NaN 1.587947 1.375509 1.557438 1.943787 0.279886 -1.320521 ... NaN NaN -0.810854 -1.415170 -1.220458 -1.382812 -1.552695 -1.547315 -1.694679 NaN <p>2557 rows \u00d7 27 columns</p> In\u00a0[90]: Copied! <pre>anomaly\n</pre> anomaly Out[90]: WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC SOLARAD_DAILY ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 LST_DATE 2016-01-01 NaN -1.577491 NaN NaN 0.208563 1.082782 0.727292 0.654996 -0.398067 -1.242257 ... -0.992876 -0.857404 -0.607020 -1.209148 3.178622 3.416310 3.829867 3.857182 3.970999 NaN 2016-01-02 NaN -1.577491 NaN NaN 0.121662 0.606696 0.396565 0.374943 -0.398067 0.186367 ... -1.244252 -1.019588 -0.794239 -1.392266 2.009687 2.285460 2.996505 3.569060 3.783166 NaN 2016-01-03 NaN -1.577491 NaN NaN 0.504027 0.883133 0.744698 0.619989 -0.398067 0.010922 ... -1.411835 -1.116899 -0.856646 -1.575385 1.842696 2.024495 2.626121 3.088858 3.595334 NaN 2016-01-04 NaN -1.577491 NaN NaN -0.295464 -1.051924 -0.734867 -0.885294 -0.398067 1.101187 ... -1.537523 -1.181773 -1.043866 -1.697463 1.091238 1.415576 2.255738 2.800736 3.313586 NaN 2016-01-05 NaN -1.577491 NaN NaN -1.286137 -1.220858 -1.326693 -1.620432 -0.398067 1.154447 ... -1.830794 -1.279083 -1.043866 -1.758503 0.005798 0.371715 1.329779 2.224493 3.031838 NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2022-12-27 NaN 0.407307 NaN NaN -0.963856 -0.556494 -0.802459 -0.738821 -0.464967 -0.249287 ... NaN NaN -0.529551 -1.086642 -1.375941 -1.382812 -1.435337 -1.298599 -1.289971 NaN 2022-12-28 NaN 0.407307 NaN NaN 0.530772 -0.213428 0.176035 0.238415 -0.464967 1.203695 ... NaN NaN -0.670202 -1.168774 -1.375941 -1.436291 -1.494016 -1.360778 -1.424874 NaN 2022-12-29 NaN 0.407307 NaN NaN 1.132268 0.562984 0.885923 0.947390 -0.464967 0.786888 ... NaN NaN -0.881180 -1.250906 -1.324114 -1.436291 -1.552695 -1.485136 -1.492325 NaN 2022-12-30 NaN 0.407307 NaN NaN 2.207671 1.772744 2.094651 1.962948 -0.464967 0.292173 ... NaN NaN -0.881180 -1.333038 -1.272286 -1.382812 -1.552695 -1.547315 -1.559777 NaN 2022-12-31 NaN 0.407307 NaN NaN 1.587947 1.375509 1.557438 1.943787 0.279886 -1.320521 ... NaN NaN -0.810854 -1.415170 -1.220458 -1.382812 -1.552695 -1.547315 -1.694679 NaN <p>2557 rows \u00d7 27 columns</p> <p>Note that the transform is different from apply.</p> In\u00a0[91]: Copied! <pre>df.groupby(df.index.month).transform('mean')\n</pre> df.groupby(df.index.month).transform('mean') Out[91]: WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC SOLARAD_DAILY ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 LST_DATE 2016-01-01 64756.0 2.564857 -73.74 41.79 2.200000 -7.550463 -2.678241 -2.44213 2.217130 5.655139 ... 0.256699 0.230433 0.164727 0.166809 0.393056 0.472685 0.963889 1.983796 3.371759 NaN 2016-01-02 64756.0 2.564857 -73.74 41.79 2.200000 -7.550463 -2.678241 -2.44213 2.217130 5.655139 ... 0.256699 0.230433 0.164727 0.166809 0.393056 0.472685 0.963889 1.983796 3.371759 NaN 2016-01-03 64756.0 2.564857 -73.74 41.79 2.200000 -7.550463 -2.678241 -2.44213 2.217130 5.655139 ... 0.256699 0.230433 0.164727 0.166809 0.393056 0.472685 0.963889 1.983796 3.371759 NaN 2016-01-04 64756.0 2.564857 -73.74 41.79 2.200000 -7.550463 -2.678241 -2.44213 2.217130 5.655139 ... 0.256699 0.230433 0.164727 0.166809 0.393056 0.472685 0.963889 1.983796 3.371759 NaN 2016-01-05 64756.0 2.564857 -73.74 41.79 2.200000 -7.550463 -2.678241 -2.44213 2.217130 5.655139 ... 0.256699 0.230433 0.164727 0.166809 0.393056 0.472685 0.963889 1.983796 3.371759 NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2022-12-27 64756.0 2.593429 -73.74 41.79 4.488018 -4.917972 -0.217512 0.05576 3.121198 4.639954 ... 0.254506 0.242934 0.171530 0.170230 2.254839 2.385714 2.946083 4.288479 5.912442 NaN 2022-12-28 64756.0 2.593429 -73.74 41.79 4.488018 -4.917972 -0.217512 0.05576 3.121198 4.639954 ... 0.254506 0.242934 0.171530 0.170230 2.254839 2.385714 2.946083 4.288479 5.912442 NaN 2022-12-29 64756.0 2.593429 -73.74 41.79 4.488018 -4.917972 -0.217512 0.05576 3.121198 4.639954 ... 0.254506 0.242934 0.171530 0.170230 2.254839 2.385714 2.946083 4.288479 5.912442 NaN 2022-12-30 64756.0 2.593429 -73.74 41.79 4.488018 -4.917972 -0.217512 0.05576 3.121198 4.639954 ... 0.254506 0.242934 0.171530 0.170230 2.254839 2.385714 2.946083 4.288479 5.912442 NaN 2022-12-31 64756.0 2.593429 -73.74 41.79 4.488018 -4.917972 -0.217512 0.05576 3.121198 4.639954 ... 0.254506 0.242934 0.171530 0.170230 2.254839 2.385714 2.946083 4.288479 5.912442 NaN <p>2557 rows \u00d7 27 columns</p> In\u00a0[92]: Copied! <pre>df.groupby(df.index.month).apply('mean')\n</pre> df.groupby(df.index.month).apply('mean') Out[92]: WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC SOLARAD_DAILY ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 LST_DATE 1 64756.0 2.564857 -73.74 41.79 2.200000 -7.550463 -2.678241 -2.442130 2.217130 5.655139 ... 0.256699 0.230433 0.164727 0.166809 0.393056 0.472685 0.963889 1.983796 3.371759 NaN 2 64756.0 2.564424 -73.74 41.79 4.866162 -5.901010 -0.519192 -0.220202 3.449495 8.359444 ... 0.250415 0.219145 0.163520 0.165425 0.557071 0.518687 0.707071 1.256566 2.156061 NaN 3 64756.0 2.564857 -73.74 41.79 9.015668 -2.634101 3.184793 3.370046 2.426728 12.813917 ... 0.240800 0.224130 0.168618 0.166180 3.385714 3.270507 3.157604 3.182488 3.337788 NaN 4 64756.0 2.564857 -73.74 41.79 14.930476 1.948095 8.438095 8.733810 3.217143 14.977381 ... 0.229071 0.235376 0.165395 0.165433 9.685714 9.460476 8.950000 8.119524 7.199524 NaN 5 64756.0 2.564857 -73.74 41.79 20.996774 8.094009 14.548848 14.772811 3.182949 17.912673 ... 0.208286 0.221042 0.156848 0.161083 16.721198 16.471889 15.606019 14.184793 12.509217 NaN 6 64756.0 2.564857 -73.74 41.79 25.874286 12.033810 18.952857 19.285714 2.290000 21.610095 ... 0.138119 0.162114 0.136386 0.153190 22.399524 22.177619 21.112381 19.530476 17.661429 NaN 7 64756.0 2.564857 -73.74 41.79 29.040092 15.894470 22.465899 22.322120 3.880184 20.864101 ... 0.106465 0.113350 0.118535 0.139206 25.527189 25.406912 24.316204 22.814286 21.129630 NaN 8 64756.0 2.297069 -73.74 41.79 28.139048 15.543810 21.838571 21.615714 3.969048 18.131429 ... 0.134129 0.128214 0.122652 0.136627 24.912857 24.918095 24.244762 23.358571 22.286190 NaN 9 64756.0 2.564857 -73.74 41.79 23.720096 11.202871 17.460287 17.344019 3.948804 14.033301 ... 0.144627 0.151890 0.127550 0.141841 20.640191 20.736364 20.594258 20.666986 20.554067 NaN 10 64756.0 2.590664 -73.74 41.79 17.773488 5.738140 11.753023 11.740465 3.637963 9.193674 ... 0.190428 0.187633 0.141386 0.144750 14.663256 14.799070 15.074419 15.892558 16.645581 NaN 11 64756.0 2.593429 -73.74 41.79 10.377512 -1.277990 4.547368 4.720574 3.301905 6.567895 ... 0.234833 0.237167 0.166742 0.161895 7.128708 7.302392 7.955288 9.385167 10.990909 NaN 12 64756.0 2.593429 -73.74 41.79 4.488018 -4.917972 -0.217512 0.055760 3.121198 4.639954 ... 0.254506 0.242934 0.171530 0.170230 2.254839 2.385714 2.946083 4.288479 5.912442 NaN <p>12 rows \u00d7 27 columns</p> In\u00a0[93]: Copied! <pre>df.resample('M').mean().plot(y='T_DAILY_MEAN', marker='o')\n</pre> df.resample('M').mean().plot(y='T_DAILY_MEAN', marker='o')  Out[93]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> <p>Just like with groupby, we can apply any aggregation function to our resample operation.</p> In\u00a0[94]: Copied! <pre>df.resample('M').max().plot(y='T_DAILY_MAX', marker='o')\n</pre> df.resample('M').max().plot(y='T_DAILY_MAX', marker='o')  Out[94]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> In\u00a0[98]: Copied! <pre>df.rolling(30, center=True).T_DAILY_MEAN.mean().plot()\ndf.rolling(30, center=True, win_type='triang').T_DAILY_MEAN.mean().plot()\n</pre> df.rolling(30, center=True).T_DAILY_MEAN.mean().plot() df.rolling(30, center=True, win_type='triang').T_DAILY_MEAN.mean().plot() Out[98]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> In\u00a0[99]: Copied! <pre>df.loc[(df.T_DAILY_MEAN.isnull())]\n</pre> df.loc[(df.T_DAILY_MEAN.isnull())] Out[99]: WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC SOLARAD_DAILY ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 LST_DATE 2017-10-04 64756 2.622 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018-10-13 64756 2.622 -73.74 41.79 NaN NaN NaN NaN 4.6 NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019-08-10 64756 2.622 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019-08-11 64756 -9.000 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019-08-12 64756 -9.000 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019-08-13 64756 -9.000 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019-08-14 64756 -9.000 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019-08-15 64756 -9.000 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019-08-16 64756 2.622 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019-09-25 64756 2.622 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2021-01-04 64756 2.622 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2021-11-16 64756 2.622 -73.74 41.79 NaN NaN NaN NaN 0.0 NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN <p>12 rows \u00d7 27 columns</p> In\u00a0[102]: Copied! <pre>df_roll = df.rolling(30, center=True).T_DAILY_MEAN.mean()\n</pre> df_roll = df.rolling(30, center=True).T_DAILY_MEAN.mean() In\u00a0[103]: Copied! <pre>list(df_roll.loc[df_roll.isnull()].index)\n</pre> list(df_roll.loc[df_roll.isnull()].index) Out[103]: <pre>[Timestamp('2016-01-01 00:00:00'),\n Timestamp('2016-01-02 00:00:00'),\n Timestamp('2016-01-03 00:00:00'),\n Timestamp('2016-01-04 00:00:00'),\n Timestamp('2016-01-05 00:00:00'),\n Timestamp('2016-01-06 00:00:00'),\n Timestamp('2016-01-07 00:00:00'),\n Timestamp('2016-01-08 00:00:00'),\n Timestamp('2016-01-09 00:00:00'),\n Timestamp('2016-01-10 00:00:00'),\n Timestamp('2016-01-11 00:00:00'),\n Timestamp('2016-01-12 00:00:00'),\n Timestamp('2016-01-13 00:00:00'),\n Timestamp('2016-01-14 00:00:00'),\n Timestamp('2016-01-15 00:00:00'),\n Timestamp('2017-09-20 00:00:00'),\n Timestamp('2017-09-21 00:00:00'),\n Timestamp('2017-09-22 00:00:00'),\n Timestamp('2017-09-23 00:00:00'),\n Timestamp('2017-09-24 00:00:00'),\n Timestamp('2017-09-25 00:00:00'),\n Timestamp('2017-09-26 00:00:00'),\n Timestamp('2017-09-27 00:00:00'),\n Timestamp('2017-09-28 00:00:00'),\n Timestamp('2017-09-29 00:00:00'),\n Timestamp('2017-09-30 00:00:00'),\n Timestamp('2017-10-01 00:00:00'),\n Timestamp('2017-10-02 00:00:00'),\n Timestamp('2017-10-03 00:00:00'),\n Timestamp('2017-10-04 00:00:00'),\n Timestamp('2017-10-05 00:00:00'),\n Timestamp('2017-10-06 00:00:00'),\n Timestamp('2017-10-07 00:00:00'),\n Timestamp('2017-10-08 00:00:00'),\n Timestamp('2017-10-09 00:00:00'),\n Timestamp('2017-10-10 00:00:00'),\n Timestamp('2017-10-11 00:00:00'),\n Timestamp('2017-10-12 00:00:00'),\n Timestamp('2017-10-13 00:00:00'),\n Timestamp('2017-10-14 00:00:00'),\n Timestamp('2017-10-15 00:00:00'),\n Timestamp('2017-10-16 00:00:00'),\n Timestamp('2017-10-17 00:00:00'),\n Timestamp('2017-10-18 00:00:00'),\n Timestamp('2017-10-19 00:00:00'),\n Timestamp('2018-09-29 00:00:00'),\n Timestamp('2018-09-30 00:00:00'),\n Timestamp('2018-10-01 00:00:00'),\n Timestamp('2018-10-02 00:00:00'),\n Timestamp('2018-10-03 00:00:00'),\n Timestamp('2018-10-04 00:00:00'),\n Timestamp('2018-10-05 00:00:00'),\n Timestamp('2018-10-06 00:00:00'),\n Timestamp('2018-10-07 00:00:00'),\n Timestamp('2018-10-08 00:00:00'),\n Timestamp('2018-10-09 00:00:00'),\n Timestamp('2018-10-10 00:00:00'),\n Timestamp('2018-10-11 00:00:00'),\n Timestamp('2018-10-12 00:00:00'),\n Timestamp('2018-10-13 00:00:00'),\n Timestamp('2018-10-14 00:00:00'),\n Timestamp('2018-10-15 00:00:00'),\n Timestamp('2018-10-16 00:00:00'),\n Timestamp('2018-10-17 00:00:00'),\n Timestamp('2018-10-18 00:00:00'),\n Timestamp('2018-10-19 00:00:00'),\n Timestamp('2018-10-20 00:00:00'),\n Timestamp('2018-10-21 00:00:00'),\n Timestamp('2018-10-22 00:00:00'),\n Timestamp('2018-10-23 00:00:00'),\n Timestamp('2018-10-24 00:00:00'),\n Timestamp('2018-10-25 00:00:00'),\n Timestamp('2018-10-26 00:00:00'),\n Timestamp('2018-10-27 00:00:00'),\n Timestamp('2018-10-28 00:00:00'),\n Timestamp('2019-07-27 00:00:00'),\n Timestamp('2019-07-28 00:00:00'),\n Timestamp('2019-07-29 00:00:00'),\n Timestamp('2019-07-30 00:00:00'),\n Timestamp('2019-07-31 00:00:00'),\n Timestamp('2019-08-01 00:00:00'),\n Timestamp('2019-08-02 00:00:00'),\n Timestamp('2019-08-03 00:00:00'),\n Timestamp('2019-08-04 00:00:00'),\n Timestamp('2019-08-05 00:00:00'),\n Timestamp('2019-08-06 00:00:00'),\n Timestamp('2019-08-07 00:00:00'),\n Timestamp('2019-08-08 00:00:00'),\n Timestamp('2019-08-09 00:00:00'),\n Timestamp('2019-08-10 00:00:00'),\n Timestamp('2019-08-11 00:00:00'),\n Timestamp('2019-08-12 00:00:00'),\n Timestamp('2019-08-13 00:00:00'),\n Timestamp('2019-08-14 00:00:00'),\n Timestamp('2019-08-15 00:00:00'),\n Timestamp('2019-08-16 00:00:00'),\n Timestamp('2019-08-17 00:00:00'),\n Timestamp('2019-08-18 00:00:00'),\n Timestamp('2019-08-19 00:00:00'),\n Timestamp('2019-08-20 00:00:00'),\n Timestamp('2019-08-21 00:00:00'),\n Timestamp('2019-08-22 00:00:00'),\n Timestamp('2019-08-23 00:00:00'),\n Timestamp('2019-08-24 00:00:00'),\n Timestamp('2019-08-25 00:00:00'),\n Timestamp('2019-08-26 00:00:00'),\n Timestamp('2019-08-27 00:00:00'),\n Timestamp('2019-08-28 00:00:00'),\n Timestamp('2019-08-29 00:00:00'),\n Timestamp('2019-08-30 00:00:00'),\n Timestamp('2019-08-31 00:00:00'),\n Timestamp('2019-09-11 00:00:00'),\n Timestamp('2019-09-12 00:00:00'),\n Timestamp('2019-09-13 00:00:00'),\n Timestamp('2019-09-14 00:00:00'),\n Timestamp('2019-09-15 00:00:00'),\n Timestamp('2019-09-16 00:00:00'),\n Timestamp('2019-09-17 00:00:00'),\n Timestamp('2019-09-18 00:00:00'),\n Timestamp('2019-09-19 00:00:00'),\n Timestamp('2019-09-20 00:00:00'),\n Timestamp('2019-09-21 00:00:00'),\n Timestamp('2019-09-22 00:00:00'),\n Timestamp('2019-09-23 00:00:00'),\n Timestamp('2019-09-24 00:00:00'),\n Timestamp('2019-09-25 00:00:00'),\n Timestamp('2019-09-26 00:00:00'),\n Timestamp('2019-09-27 00:00:00'),\n Timestamp('2019-09-28 00:00:00'),\n Timestamp('2019-09-29 00:00:00'),\n Timestamp('2019-09-30 00:00:00'),\n Timestamp('2019-10-01 00:00:00'),\n Timestamp('2019-10-02 00:00:00'),\n Timestamp('2019-10-03 00:00:00'),\n Timestamp('2019-10-04 00:00:00'),\n Timestamp('2019-10-05 00:00:00'),\n Timestamp('2019-10-06 00:00:00'),\n Timestamp('2019-10-07 00:00:00'),\n Timestamp('2019-10-08 00:00:00'),\n Timestamp('2019-10-09 00:00:00'),\n Timestamp('2019-10-10 00:00:00'),\n Timestamp('2020-12-21 00:00:00'),\n Timestamp('2020-12-22 00:00:00'),\n Timestamp('2020-12-23 00:00:00'),\n Timestamp('2020-12-24 00:00:00'),\n Timestamp('2020-12-25 00:00:00'),\n Timestamp('2020-12-26 00:00:00'),\n Timestamp('2020-12-27 00:00:00'),\n Timestamp('2020-12-28 00:00:00'),\n Timestamp('2020-12-29 00:00:00'),\n Timestamp('2020-12-30 00:00:00'),\n Timestamp('2020-12-31 00:00:00'),\n Timestamp('2021-01-01 00:00:00'),\n Timestamp('2021-01-02 00:00:00'),\n Timestamp('2021-01-03 00:00:00'),\n Timestamp('2021-01-04 00:00:00'),\n Timestamp('2021-01-05 00:00:00'),\n Timestamp('2021-01-06 00:00:00'),\n Timestamp('2021-01-07 00:00:00'),\n Timestamp('2021-01-08 00:00:00'),\n Timestamp('2021-01-09 00:00:00'),\n Timestamp('2021-01-10 00:00:00'),\n Timestamp('2021-01-11 00:00:00'),\n Timestamp('2021-01-12 00:00:00'),\n Timestamp('2021-01-13 00:00:00'),\n Timestamp('2021-01-14 00:00:00'),\n Timestamp('2021-01-15 00:00:00'),\n Timestamp('2021-01-16 00:00:00'),\n Timestamp('2021-01-17 00:00:00'),\n Timestamp('2021-01-18 00:00:00'),\n Timestamp('2021-01-19 00:00:00'),\n Timestamp('2021-11-02 00:00:00'),\n Timestamp('2021-11-03 00:00:00'),\n Timestamp('2021-11-04 00:00:00'),\n Timestamp('2021-11-05 00:00:00'),\n Timestamp('2021-11-06 00:00:00'),\n Timestamp('2021-11-07 00:00:00'),\n Timestamp('2021-11-08 00:00:00'),\n Timestamp('2021-11-09 00:00:00'),\n Timestamp('2021-11-10 00:00:00'),\n Timestamp('2021-11-11 00:00:00'),\n Timestamp('2021-11-12 00:00:00'),\n Timestamp('2021-11-13 00:00:00'),\n Timestamp('2021-11-14 00:00:00'),\n Timestamp('2021-11-15 00:00:00'),\n Timestamp('2021-11-16 00:00:00'),\n Timestamp('2021-11-17 00:00:00'),\n Timestamp('2021-11-18 00:00:00'),\n Timestamp('2021-11-19 00:00:00'),\n Timestamp('2021-11-20 00:00:00'),\n Timestamp('2021-11-21 00:00:00'),\n Timestamp('2021-11-22 00:00:00'),\n Timestamp('2021-11-23 00:00:00'),\n Timestamp('2021-11-24 00:00:00'),\n Timestamp('2021-11-25 00:00:00'),\n Timestamp('2021-11-26 00:00:00'),\n Timestamp('2021-11-27 00:00:00'),\n Timestamp('2021-11-28 00:00:00'),\n Timestamp('2021-11-29 00:00:00'),\n Timestamp('2021-11-30 00:00:00'),\n Timestamp('2021-12-01 00:00:00'),\n Timestamp('2022-12-18 00:00:00'),\n Timestamp('2022-12-19 00:00:00'),\n Timestamp('2022-12-20 00:00:00'),\n Timestamp('2022-12-21 00:00:00'),\n Timestamp('2022-12-22 00:00:00'),\n Timestamp('2022-12-23 00:00:00'),\n Timestamp('2022-12-24 00:00:00'),\n Timestamp('2022-12-25 00:00:00'),\n Timestamp('2022-12-26 00:00:00'),\n Timestamp('2022-12-27 00:00:00'),\n Timestamp('2022-12-28 00:00:00'),\n Timestamp('2022-12-29 00:00:00'),\n Timestamp('2022-12-30 00:00:00'),\n Timestamp('2022-12-31 00:00:00')]</pre> In\u00a0[104]: Copied! <pre>df.rolling('30D', center=True).T_DAILY_MEAN.mean().plot()\n</pre> df.rolling('30D', center=True).T_DAILY_MEAN.mean().plot()  Out[104]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> In\u00a0[105]: Copied! <pre>df\n</pre> df Out[105]: WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC SOLARAD_DAILY ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 LST_DATE 2016-01-01 64756 2.422 -73.74 41.79 3.4 -0.5 1.5 1.3 0.0 1.69 ... 0.233 0.204 0.155 0.147 4.2 4.4 5.1 6.0 7.6 NaN 2016-01-02 64756 2.422 -73.74 41.79 2.9 -3.6 -0.4 -0.3 0.0 6.25 ... 0.227 0.199 0.152 0.144 2.8 3.1 4.2 5.7 7.4 NaN 2016-01-03 64756 2.422 -73.74 41.79 5.1 -1.8 1.6 1.1 0.0 5.69 ... 0.223 0.196 0.151 0.141 2.6 2.8 3.8 5.2 7.2 NaN 2016-01-04 64756 2.422 -73.74 41.79 0.5 -14.4 -6.9 -7.5 0.0 9.17 ... 0.220 0.194 0.148 0.139 1.7 2.1 3.4 4.9 6.9 NaN 2016-01-05 64756 2.422 -73.74 41.79 -5.2 -15.5 -10.3 -11.7 0.0 9.34 ... 0.213 0.191 0.148 0.138 0.4 0.9 2.4 4.3 6.6 NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2022-12-27 64756 2.622 -73.74 41.79 -0.8 -8.0 -4.4 -3.8 0.0 4.00 ... NaN NaN 0.164 0.157 -0.4 -0.2 0.5 2.2 4.0 NaN 2022-12-28 64756 2.622 -73.74 41.79 7.4 -6.1 0.7 1.3 0.0 7.73 ... NaN NaN 0.162 0.156 -0.4 -0.3 0.4 2.1 3.8 NaN 2022-12-29 64756 2.622 -73.74 41.79 10.7 -1.8 4.4 5.0 0.0 6.66 ... NaN NaN 0.159 0.155 -0.3 -0.3 0.3 1.9 3.7 NaN 2022-12-30 64756 2.622 -73.74 41.79 16.6 4.9 10.7 10.3 0.0 5.39 ... NaN NaN 0.159 0.154 -0.2 -0.2 0.3 1.8 3.6 NaN 2022-12-31 64756 2.622 -73.74 41.79 13.2 2.7 7.9 10.2 5.0 1.25 ... NaN NaN 0.160 0.153 -0.1 -0.2 0.3 1.8 3.4 NaN <p>2557 rows \u00d7 27 columns</p> In\u00a0[106]: Copied! <pre>df_sub = df.loc['2017-01-01':'2017-12-31']\n</pre> df_sub = df.loc['2017-01-01':'2017-12-31'] In\u00a0[107]: Copied! <pre>df_sub['SOIL_MOISTURE_10_DAILY'].plot()\n</pre> df_sub['SOIL_MOISTURE_10_DAILY'].plot() Out[107]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> In\u00a0[110]: Copied! <pre># Fill values forward\n\ndf_sub.ffill()['SOIL_MOISTURE_10_DAILY'].plot()\n\n# Fill values backward\n\ndf_sub.bfill()['SOIL_MOISTURE_10_DAILY'].plot()\n</pre> # Fill values forward  df_sub.ffill()['SOIL_MOISTURE_10_DAILY'].plot()  # Fill values backward  df_sub.bfill()['SOIL_MOISTURE_10_DAILY'].plot() Out[110]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> In\u00a0[114]: Copied! <pre>df_sub.fillna(method = 'ffill', limit = 3)['SOIL_MOISTURE_10_DAILY'].plot()\n</pre> df_sub.fillna(method = 'ffill', limit = 3)['SOIL_MOISTURE_10_DAILY'].plot() Out[114]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Lecture_7_Pandas_Advanced/#lecture-7-advaned-pandas","title":"Lecture 7: Advaned Pandas\u00b6","text":""},{"location":"Lecture_7_Pandas_Advanced/#groupby-methods","title":"Groupby methods\u00b6","text":""},{"location":"Lecture_7_Pandas_Advanced/#an-example","title":"An Example:\u00b6","text":"<p>Question: Find out the top 10 states with largest number of wildfires.</p> <p>This is an example of a \"one-liner\" that you can accomplish with groupby.</p>"},{"location":"Lecture_7_Pandas_Advanced/#what-happened","title":"What Happened?\u00b6","text":"<p>Let's break apart this operation a bit. The workflow with <code>groubpy</code> can be divided into three general steps:</p> <ol> <li><p>Split: Partition the data into different groups based on some criterion.</p> </li> <li><p>Apply: Do some caclulation within each group. Different types of \"apply\" steps might be</p> <ul> <li> Aggregation: Get the mean or max within the group. </li> <li> Transformation: Normalize all the values within a group. </li> <li> Filtration: Eliminate some groups based on a criterion. </li> </ul> </li> <li><p>Combine: Put the results back together into a single object.</p> </li> </ol>"},{"location":"Lecture_7_Pandas_Advanced/#the-groupby-method","title":"The <code>groupby</code> method\u00b6","text":"<p>Both <code>Series</code> and <code>DataFrame</code> objects have a groupby method. It accepts a variety of arguments, but the simplest way to think about it is that you pass another series, whose unique values are used to split the original object into different groups.</p>"},{"location":"Lecture_7_Pandas_Advanced/#the-groubby-object","title":"The <code>GroubBy</code> object\u00b6","text":"<p>When we call, <code>groupby</code> we get back a <code>GroupBy</code> object:</p>"},{"location":"Lecture_7_Pandas_Advanced/#iterating-and-selecting-groups","title":"Iterating and selecting groups\u00b6","text":"<p>You can loop through the groups if you want.</p>"},{"location":"Lecture_7_Pandas_Advanced/#aggregation","title":"Aggregation\u00b6","text":"<p>Now that we know how to create a <code>GroupBy</code> object, let's learn how to do aggregation on it.</p> <p>One way us to use the <code>.aggregate</code> method, which accepts another function as its argument. The result is automatically combined into a new dataframe with the group key as the index.</p>"},{"location":"Lecture_7_Pandas_Advanced/#groupby-multiple-index","title":"Groupby multiple index\u00b6","text":""},{"location":"Lecture_7_Pandas_Advanced/#named-aggregation","title":"Named aggregation\u00b6","text":"<ul> <li>The keywords are the output column names</li> <li>The values are tuples whose first element is the column to select and the second element is the aggregation to apply to that column.</li> </ul>"},{"location":"Lecture_7_Pandas_Advanced/#filtration","title":"Filtration\u00b6","text":""},{"location":"Lecture_7_Pandas_Advanced/#time-grouping-with-the-weather-data","title":"Time grouping with the weather data\u00b6","text":""},{"location":"Lecture_7_Pandas_Advanced/#calculating-anomalies","title":"Calculating anomalies\u00b6","text":""},{"location":"Lecture_7_Pandas_Advanced/#resampling","title":"Resampling\u00b6","text":"<p>Another common operation is to change the resolution of a dataset by resampling in time. Pandas exposes this through the resample function. The resample periods are specified using pandas offset index syntax.</p> <p>Below we resample the dataset by taking the mean over each month.</p>"},{"location":"Lecture_7_Pandas_Advanced/#rolling-operations","title":"Rolling Operations\u00b6","text":""},{"location":"Lecture_7_Pandas_Advanced/#filling-missing-data","title":"Filling missing data\u00b6","text":""},{"location":"Lecture_8_Xarray/","title":"Lecture 8: Xarray for multidimensional gridded data","text":"<p>Here is an example of how we might structure a dataset for a weather forecast:</p> <p>You'll notice multiple data variables (temperature, precipitation), coordinate variables (latitude, longitude), and dimensions (x, y, t). We'll cover how these fit into Xarray's data structures below.</p> <p>Xarray doesn\u2019t just keep track of labels on arrays \u2013 it uses them to provide a powerful and concise interface. For example:</p> <ul> <li><p>Apply operations over dimensions by name: <code>x.sum('time')</code>.</p> </li> <li><p>Select values by label (or logical location) instead of integer location: <code>x.loc['2014-01-01']</code> or <code>x.sel(time='2014-01-01')</code>.</p> </li> <li><p>Mathematical operations (e.g., <code>x - y</code>) vectorize across multiple dimensions (array broadcasting) based on dimension names, not shape.</p> </li> <li><p>Easily use the split-apply-combine paradigm with groupby: <code>x.groupby('time.dayofyear').mean()</code>.</p> </li> <li><p>Database-like alignment based on coordinate labels that smoothly handles missing values: <code>x, y = xr.align(x, y, join='outer')</code>.</p> </li> <li><p>Keep track of arbitrary metadata in the form of a Python dictionary: <code>x.attrs</code>.</p> </li> </ul> <p>The N-dimensional nature of xarray\u2019s data structures makes it suitable for dealing with multi-dimensional scientific data, and its use of dimension names instead of axis labels (<code>dim='time'</code> instead of <code>axis=0</code>) makes such arrays much more manageable than the raw numpy ndarray: with xarray, you don\u2019t need to keep track of the order of an array\u2019s dimensions or insert dummy dimensions of size 1 to align arrays (e.g., using np.newaxis).</p> <p>The immediate payoff of using xarray is that you\u2019ll write less code. The long-term payoff is that you\u2019ll understand what you were thinking when you come back to look at it weeks or months later.</p> In\u00a0[1]: Copied! <pre># First import xarray \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport xarray as xr\n</pre> # First import xarray  import matplotlib.pyplot as plt import numpy as np import xarray as xr In\u00a0[2]: Copied! <pre>ds = xr.tutorial.load_dataset(\"air_temperature\")\nds\n</pre> ds = xr.tutorial.load_dataset(\"air_temperature\") ds Out[2]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>time: 2920</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> <p>We can access \"layers\" of the Dataset (individual DataArrays) with dictionary syntax</p> In\u00a0[3]: Copied! <pre>ds[\"air\"]\n</pre> ds[\"air\"] Out[3]: <pre>&lt;xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)&gt;\narray([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]</pre>xarray.DataArray'air'<ul><li>time: 2920</li><li>lat: 25</li><li>lon: 53</li></ul><ul><li>241.2 242.5 243.5 244.0 244.1 243.9 ... 297.4 297.2 296.5 296.2 295.7<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]</li></ul> <p>We can save some typing by using the \"attribute\" or \"dot\" notation. This won't work for variable names that clash with built-in method names (for example, <code>mean</code>).</p> In\u00a0[4]: Copied! <pre>ds.air\n</pre> ds.air Out[4]: <pre>&lt;xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)&gt;\narray([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]</pre>xarray.DataArray'air'<ul><li>time: 2920</li><li>lat: 25</li><li>lon: 53</li></ul><ul><li>241.2 242.5 243.5 244.0 244.1 243.9 ... 297.4 297.2 296.5 296.2 295.7<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]</li></ul> In\u00a0[5]: Copied! <pre>da = ds[\"air\"]\nda\n</pre> da = ds[\"air\"] da Out[5]: <pre>&lt;xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)&gt;\narray([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]</pre>xarray.DataArray'air'<ul><li>time: 2920</li><li>lat: 25</li><li>lon: 53</li></ul><ul><li>241.2 242.5 243.5 244.0 244.1 243.9 ... 297.4 297.2 296.5 296.2 295.7<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]</li></ul> <p>We can also access the data array directly:</p> In\u00a0[6]: Copied! <pre>ds.air.data\n</pre> ds.air.data   Out[6]: <pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n        ...,\n        [296.4    , 295.9    , 296.19998, ..., 295.4    , 295.1    ,\n         294.79   ],\n        [296.19998, 296.69998, 296.79   , ..., 295.6    , 295.5    ,\n         295.1    ],\n        [296.29   , 297.19998, 297.4    , ..., 296.4    , 296.4    ,\n         296.6    ]],\n\n       [[242.29999, 242.2    , 242.29999, ..., 234.29999, 236.09999,\n         238.7    ],\n        [244.59999, 244.39   , 244.     , ..., 230.29999, 232.     ,\n         235.7    ],\n        [256.19998, 255.5    , 254.2    , ..., 231.2    , 233.2    ,\n         238.2    ],\n        ...,\n        [295.6    , 295.4    , 295.4    , ..., 296.29   , 295.29   ,\n         295.     ],\n        [296.19998, 296.5    , 296.29   , ..., 296.4    , 296.     ,\n         295.6    ],\n        [296.4    , 296.29   , 296.4    , ..., 297.     , 297.     ,\n         296.79   ]],\n\n       ...,\n\n       [[243.48999, 242.98999, 242.09   , ..., 244.18999, 244.48999,\n         244.89   ],\n        [249.09   , 248.98999, 248.59   , ..., 240.59   , 241.29   ,\n         242.68999],\n        [262.69   , 262.19   , 261.69   , ..., 239.39   , 241.68999,\n         245.18999],\n        ...,\n        [294.79   , 295.29   , 297.49   , ..., 295.49   , 295.38998,\n         294.69   ],\n        [296.79   , 297.88998, 298.29   , ..., 295.49   , 295.49   ,\n         294.79   ],\n        [298.19   , 299.19   , 298.79   , ..., 296.09   , 295.79   ,\n         295.79   ]],\n\n       [[245.79   , 244.79   , 243.48999, ..., 243.29   , 243.98999,\n         244.79   ],\n        [249.89   , 249.29   , 248.48999, ..., 241.29   , 242.48999,\n         244.29   ],\n        [262.38998, 261.79   , 261.29   , ..., 240.48999, 243.09   ,\n         246.89   ],\n        ...,\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre> In\u00a0[7]: Copied! <pre>ds.air.dims\n</pre> ds.air.dims Out[7]: <pre>('time', 'lat', 'lon')</pre> In\u00a0[8]: Copied! <pre>ds.air.coords\n</pre> ds.air.coords Out[8]: <pre>Coordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00</pre> In\u00a0[9]: Copied! <pre>ds.air.attrs\n</pre> ds.air.attrs Out[9]: <pre>{'long_name': '4xDaily Air temperature at sigma level 995',\n 'units': 'degK',\n 'precision': 2,\n 'GRIB_id': 11,\n 'GRIB_name': 'TMP',\n 'var_desc': 'Air temperature',\n 'dataset': 'NMC Reanalysis',\n 'level_desc': 'Surface',\n 'statistic': 'Individual Obs',\n 'parent_stat': 'Other',\n 'actual_range': array([185.16, 322.1 ], dtype='&gt;f4')}</pre> In\u00a0[10]: Copied! <pre># assign your own attributes!\nds.air.attrs[\"new_attr\"] = \"xarray\"\nds.air.attrs\n</pre> # assign your own attributes! ds.air.attrs[\"new_attr\"] = \"xarray\" ds.air.attrs Out[10]: <pre>{'long_name': '4xDaily Air temperature at sigma level 995',\n 'units': 'degK',\n 'precision': 2,\n 'GRIB_id': 11,\n 'GRIB_name': 'TMP',\n 'var_desc': 'Air temperature',\n 'dataset': 'NMC Reanalysis',\n 'level_desc': 'Surface',\n 'statistic': 'Individual Obs',\n 'parent_stat': 'Other',\n 'actual_range': array([185.16, 322.1 ], dtype='&gt;f4'),\n 'new_attr': 'xarray'}</pre> In\u00a0[11]: Copied! <pre>## Without Xarray, let's try to use matplotlib to plot the data\n\ntemp = ds.air.data\nplt.pcolormesh(temp[0,:,:])\n</pre> ## Without Xarray, let's try to use matplotlib to plot the data  temp = ds.air.data plt.pcolormesh(temp[0,:,:]) Out[11]: <pre>&lt;matplotlib.collections.QuadMesh at 0x1945afee0&gt;</pre> In\u00a0[12]: Copied! <pre>### Add Lat and Lon\n\ntemp = ds.air.data\nlat = ds.air.lat.data\nlon = ds.air.lon.data\nplt.pcolormesh(lon, lat, temp[0,:,:])\n</pre> ### Add Lat and Lon  temp = ds.air.data lat = ds.air.lat.data lon = ds.air.lon.data plt.pcolormesh(lon, lat, temp[0,:,:]) Out[12]: <pre>&lt;matplotlib.collections.QuadMesh at 0x1947868b0&gt;</pre> In\u00a0[13]: Copied! <pre>temp.mean(axis=1)  ## what did I just do? I can't tell by looking at this line.\n</pre> temp.mean(axis=1)  ## what did I just do? I can't tell by looking at this line.  Out[13]: <pre>array([[279.39798, 279.6664 , 279.66122, ..., 279.9508 , 280.31522,\n        280.6624 ],\n       [279.05722, 279.538  , 279.7296 , ..., 279.77563, 280.27002,\n        280.79764],\n       [279.0104 , 279.2808 , 279.5508 , ..., 279.682  , 280.19763,\n        280.81403],\n       ...,\n       [279.63   , 279.934  , 280.534  , ..., 279.802  , 280.346  ,\n        280.77798],\n       [279.398  , 279.66602, 280.31796, ..., 279.766  , 280.34198,\n        280.834  ],\n       [279.27   , 279.354  , 279.88202, ..., 279.42596, 279.96997,\n        280.48196]], dtype=float32)</pre> In\u00a0[14]: Copied! <pre>ds.air.isel(time=0).plot()\n</pre> ds.air.isel(time=0).plot()  Out[14]: <pre>&lt;matplotlib.collections.QuadMesh at 0x194849940&gt;</pre> In\u00a0[15]: Copied! <pre>ds.air.mean(dim=\"time\").plot()\n</pre> ds.air.mean(dim=\"time\").plot()  Out[15]: <pre>&lt;matplotlib.collections.QuadMesh at 0x194942490&gt;</pre> In\u00a0[16]: Copied! <pre>da[:, 20, 40]\n</pre> da[:, 20, 40] Out[16]: <pre>&lt;xarray.DataArray 'air' (time: 2920)&gt;\narray([295.  , 294.4 , 294.5 , ..., 297.29, 297.79, 297.99], dtype=float32)\nCoordinates:\n    lat      float32 25.0\n    lon      float32 300.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]\n    new_attr:      xarray</pre>xarray.DataArray'air'<ul><li>time: 2920</li></ul><ul><li>295.0 294.4 294.5 295.4 295.2 294.4 ... 297.7 297.3 297.3 297.8 298.0<pre>array([295.  , 294.4 , 294.5 , ..., 297.29, 297.79, 297.99], dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat()float3225.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array(25., dtype=float32)</pre></li><li>lon()float32300.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array(300., dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (12)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray</li></ul> In\u00a0[17]: Copied! <pre>da[0,0,0]\n</pre> da[0,0,0] Out[17]: <pre>&lt;xarray.DataArray 'air' ()&gt;\narray(241.2, dtype=float32)\nCoordinates:\n    lat      float32 75.0\n    lon      float32 200.0\n    time     datetime64[ns] 2013-01-01\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]\n    new_attr:      xarray</pre>xarray.DataArray'air'<ul><li>241.2<pre>array(241.2, dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat()float3275.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array(75., dtype=float32)</pre></li><li>lon()float32200.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array(200., dtype=float32)</pre></li><li>time()datetime64[ns]2013-01-01standard_name :timelong_name :Time<pre>array('2013-01-01T00:00:00.000000000', dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (0)<ul></ul></li><li>Attributes: (12)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray</li></ul> <p>Remembering the axis order can be challenging even with 2D arrays:</p> <ul> <li>is <code>np_array[0,3]</code> the first row and third column or first column and third row?</li> <li>or did I store these samples by row or by column when I saved the data?!.</li> </ul> <p>The difficulty is compounded with added dimensions.</p> <p>Xarray objects eliminate much of the mental overhead by allowing indexing using dimension names instead of axes numbers:</p> In\u00a0[18]: Copied! <pre>da.isel(lat=0, lon=0).plot();\n</pre> da.isel(lat=0, lon=0).plot(); <p>Slicing is also possible similarly:</p> In\u00a0[19]: Copied! <pre>da.isel(time=slice(0, 20), lat=0, lon=0).plot();\n</pre> da.isel(time=slice(0, 20), lat=0, lon=0).plot(); <pre><code>{note}\nUsing the `isel` method, the user can choose/slice the specific elements from a Dataset or DataArray.\n</code></pre> <p>So far, we have explored positional indexing, which relies on knowing the exact indices. But, what if you wanted to select data specifically for a particular latitude? It becomes challenging to determine the corresponding indices in such cases. Xarray reduce this complexity by introducing label-based indexing.</p> <p>For example, let's select all data for Lat 25 \u00b0N and Lon 210 \u00b0E using <code>sel</code> :</p> In\u00a0[20]: hide-output Copied! <pre>da.sel(lat=25, lon=210).plot();\n</pre> da.sel(lat=25, lon=210).plot(); <p>Similarly we can do slicing or filter a range using the <code>.slice</code> function:</p> In\u00a0[21]: Copied! <pre># demonstrate slicing\nda.sel(lon=slice(210, 215))\n</pre> # demonstrate slicing da.sel(lon=slice(210, 215)) Out[21]: <pre>&lt;xarray.DataArray 'air' (time: 2920, lat: 25, lon: 3)&gt;\narray([[[244.09999, 243.89   , 243.59999],\n        [243.39   , 242.39   , 241.7    ],\n        [246.     , 244.39   , 243.09999],\n        ...,\n        [295.5    , 294.     , 293.6    ],\n        [296.1    , 295.1    , 294.6    ],\n        [296.9    , 296.4    , 296.     ]],\n\n       [[243.59999, 243.79999, 244.     ],\n        [243.7    , 243.29999, 242.79999],\n        [249.29999, 247.5    , 245.5    ],\n        ...,\n        [295.1    , 294.1    , 293.79   ],\n        [296.1    , 295.6    , 295.1    ],\n        [297.1    , 296.69998, 296.4    ]],\n\n       [[242.89   , 243.59999, 244.5    ],\n        [242.79999, 242.39   , 242.29999],\n        [250.2    , 248.09999, 246.29999],\n        ...,\n...\n        ...,\n        [297.99   , 297.49   , 297.29   ],\n        [298.59   , 298.49   , 298.19   ],\n        [299.29   , 298.99   , 298.59   ]],\n\n       [[240.29   , 238.89   , 237.79   ],\n        [245.68999, 243.98999, 242.29   ],\n        [259.38998, 257.69   , 255.48999],\n        ...,\n        [297.79   , 297.49   , 297.49   ],\n        [298.29   , 298.09   , 297.79   ],\n        [298.79   , 298.69   , 298.49   ]],\n\n       [[241.09   , 240.09   , 239.39   ],\n        [245.09   , 243.09   , 241.09   ],\n        [257.79   , 254.98999, 251.98999],\n        ...,\n        [297.29   , 297.29   , 297.38998],\n        [298.19   , 298.09   , 297.79   ],\n        [298.88998, 298.69   , 298.38998]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 210.0 212.5 215.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]\n    new_attr:      xarray</pre>xarray.DataArray'air'<ul><li>time: 2920</li><li>lat: 25</li><li>lon: 3</li></ul><ul><li>244.1 243.9 243.6 243.4 242.4 241.7 ... 298.1 297.8 298.9 298.7 298.4<pre>array([[[244.09999, 243.89   , 243.59999],\n        [243.39   , 242.39   , 241.7    ],\n        [246.     , 244.39   , 243.09999],\n        ...,\n        [295.5    , 294.     , 293.6    ],\n        [296.1    , 295.1    , 294.6    ],\n        [296.9    , 296.4    , 296.     ]],\n\n       [[243.59999, 243.79999, 244.     ],\n        [243.7    , 243.29999, 242.79999],\n        [249.29999, 247.5    , 245.5    ],\n        ...,\n        [295.1    , 294.1    , 293.79   ],\n        [296.1    , 295.6    , 295.1    ],\n        [297.1    , 296.69998, 296.4    ]],\n\n       [[242.89   , 243.59999, 244.5    ],\n        [242.79999, 242.39   , 242.29999],\n        [250.2    , 248.09999, 246.29999],\n        ...,\n...\n        ...,\n        [297.99   , 297.49   , 297.29   ],\n        [298.59   , 298.49   , 298.19   ],\n        [299.29   , 298.99   , 298.59   ]],\n\n       [[240.29   , 238.89   , 237.79   ],\n        [245.68999, 243.98999, 242.29   ],\n        [259.38998, 257.69   , 255.48999],\n        ...,\n        [297.79   , 297.49   , 297.49   ],\n        [298.29   , 298.09   , 297.79   ],\n        [298.79   , 298.69   , 298.49   ]],\n\n       [[241.09   , 240.09   , 239.39   ],\n        [245.09   , 243.09   , 241.09   ],\n        [257.79   , 254.98999, 251.98999],\n        ...,\n        [297.29   , 297.29   , 297.38998],\n        [298.19   , 298.09   , 297.79   ],\n        [298.88998, 298.69   , 298.38998]]], dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32210.0 212.5 215.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([210. , 212.5, 215. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([210.0, 212.5, 215.0], dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (12)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray</li></ul> In\u00a0[22]: Copied! <pre># demonstrate slicing\nda.sel(lat=slice(25, 50))\n</pre> # demonstrate slicing da.sel(lat=slice(25, 50)) Out[22]: <pre>&lt;xarray.DataArray 'air' (time: 2920, lat: 0, lon: 53)&gt;\narray([], shape=(2920, 0, 53), dtype=float32)\nCoordinates:\n  * lat      (lat) float32 \n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]\n    new_attr:      xarray</pre>xarray.DataArray'air'<ul><li>time: 2920</li><li>lat: 0</li><li>lon: 53</li></ul><ul><li><pre>array([], shape=(2920, 0, 53), dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat(lat)float32standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([], dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (12)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray</li></ul> In\u00a0[23]: Copied! <pre># demonstrate slicing\nda.sel(lat=slice(50, 25), lon=slice(210, 215))\n</pre> # demonstrate slicing da.sel(lat=slice(50, 25), lon=slice(210, 215)) Out[23]: <pre>&lt;xarray.DataArray 'air' (time: 2920, lat: 11, lon: 3)&gt;\narray([[[279.5    , 280.1    , 280.6    ],\n        [279.4    , 280.29   , 281.29   ],\n        [280.29   , 282.     , 283.29   ],\n        ...,\n        [294.5    , 294.9    , 293.5    ],\n        [295.4    , 294.69998, 293.19998],\n        [295.4    , 294.     , 292.9    ]],\n\n       [[278.4    , 279.19998, 280.6    ],\n        [279.1    , 279.69998, 280.79   ],\n        [280.1    , 280.6    , 281.69998],\n        ...,\n        [294.     , 294.5    , 294.19998],\n        [295.29   , 294.79   , 293.6    ],\n        [295.6    , 294.29   , 292.9    ]],\n\n       [[277.5    , 277.6    , 278.79   ],\n        [278.79   , 278.29   , 279.1    ],\n        [280.5    , 279.6    , 279.69998],\n        ...,\n...\n        ...,\n        [292.99   , 292.79   , 293.19   ],\n        [294.29   , 294.09   , 294.29   ],\n        [295.49   , 295.19   , 294.69   ]],\n\n       [[279.38998, 280.09   , 281.99   ],\n        [280.49   , 282.59   , 284.29   ],\n        [283.38998, 285.38998, 285.99   ],\n        ...,\n        [292.59   , 292.88998, 293.29   ],\n        [293.88998, 293.88998, 293.99   ],\n        [295.19   , 295.29   , 294.49   ]],\n\n       [[279.88998, 281.79   , 283.69   ],\n        [281.69   , 283.09   , 283.79   ],\n        [283.59   , 284.79   , 284.69   ],\n        ...,\n        [293.59   , 293.59   , 293.49   ],\n        [294.99   , 294.59   , 294.19   ],\n        [295.49   , 295.59   , 295.09   ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 50.0 47.5 45.0 42.5 40.0 ... 35.0 32.5 30.0 27.5 25.0\n  * lon      (lon) float32 210.0 212.5 215.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]\n    new_attr:      xarray</pre>xarray.DataArray'air'<ul><li>time: 2920</li><li>lat: 11</li><li>lon: 3</li></ul><ul><li>279.5 280.1 280.6 279.4 280.3 281.3 ... 294.6 294.2 295.5 295.6 295.1<pre>array([[[279.5    , 280.1    , 280.6    ],\n        [279.4    , 280.29   , 281.29   ],\n        [280.29   , 282.     , 283.29   ],\n        ...,\n        [294.5    , 294.9    , 293.5    ],\n        [295.4    , 294.69998, 293.19998],\n        [295.4    , 294.     , 292.9    ]],\n\n       [[278.4    , 279.19998, 280.6    ],\n        [279.1    , 279.69998, 280.79   ],\n        [280.1    , 280.6    , 281.69998],\n        ...,\n        [294.     , 294.5    , 294.19998],\n        [295.29   , 294.79   , 293.6    ],\n        [295.6    , 294.29   , 292.9    ]],\n\n       [[277.5    , 277.6    , 278.79   ],\n        [278.79   , 278.29   , 279.1    ],\n        [280.5    , 279.6    , 279.69998],\n        ...,\n...\n        ...,\n        [292.99   , 292.79   , 293.19   ],\n        [294.29   , 294.09   , 294.29   ],\n        [295.49   , 295.19   , 294.69   ]],\n\n       [[279.38998, 280.09   , 281.99   ],\n        [280.49   , 282.59   , 284.29   ],\n        [283.38998, 285.38998, 285.99   ],\n        ...,\n        [292.59   , 292.88998, 293.29   ],\n        [293.88998, 293.88998, 293.99   ],\n        [295.19   , 295.29   , 294.49   ]],\n\n       [[279.88998, 281.79   , 283.69   ],\n        [281.69   , 283.09   , 283.79   ],\n        [283.59   , 284.79   , 284.69   ],\n        ...,\n        [293.59   , 293.59   , 293.49   ],\n        [294.99   , 294.59   , 294.19   ],\n        [295.49   , 295.59   , 295.09   ]]], dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat(lat)float3250.0 47.5 45.0 ... 30.0 27.5 25.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([50. , 47.5, 45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. ],\n      dtype=float32)</pre></li><li>lon(lon)float32210.0 212.5 215.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([210. , 212.5, 215. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([50.0, 47.5, 45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0], dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([210.0, 212.5, 215.0], dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (12)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray</li></ul> In\u00a0[24]: Copied! <pre>da\n</pre> da Out[24]: <pre>&lt;xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)&gt;\narray([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]\n    new_attr:      xarray</pre>xarray.DataArray'air'<ul><li>time: 2920</li><li>lat: 25</li><li>lon: 53</li></ul><ul><li>241.2 242.5 243.5 244.0 244.1 243.9 ... 297.4 297.2 296.5 296.2 295.7<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (12)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray</li></ul> In\u00a0[25]: Copied! <pre>da.drop_sel(lat=75.0, lon=200.0)\n</pre> da.drop_sel(lat=75.0, lon=200.0) Out[25]: <pre>&lt;xarray.DataArray 'air' (time: 2920, lat: 24, lon: 52)&gt;\narray([[[244.5    , 244.7    , 244.2    , ..., 232.79999, 235.29999,\n         239.29999],\n        [249.79999, 248.89   , 247.5    , ..., 233.2    , 236.39   ,\n         241.7    ],\n        [267.1    , 267.1    , 266.69998, ..., 249.2    , 253.09999,\n         256.9    ],\n        ...,\n        [296.19998, 296.4    , 296.5    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [296.19998, 296.79   , 296.5    , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.79   , 297.1    , 297.     , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[244.09999, 244.2    , 244.09999, ..., 231.     , 232.5    ,\n         235.7    ],\n        [252.89   , 252.09999, 250.79999, ..., 230.79999, 233.39   ,\n         238.5    ],\n        [269.4    , 268.6    , 267.4    , ..., 247.2    , 250.5    ,\n         254.39   ],\n...\n        [293.88998, 295.38998, 297.19   , ..., 295.09   , 294.69   ,\n         294.29   ],\n        [297.19   , 297.59   , 297.88998, ..., 295.29   , 295.09   ,\n         294.38998],\n        [298.38998, 298.49   , 298.59   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[249.29   , 248.39   , 246.98999, ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.19   , 261.38998, 259.99   , ..., 239.89   , 242.59   ,\n         246.29   ],\n        [272.09   , 271.99   , 271.59   , ..., 255.39   , 258.99   ,\n         262.49   ],\n        ...,\n        [293.69   , 295.09   , 296.69   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.88998, 297.19   , 297.49   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [298.09   , 298.09   , 298.49   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 72.5 70.0 67.5 65.0 62.5 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 202.5 205.0 207.5 210.0 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]\n    new_attr:      xarray</pre>xarray.DataArray'air'<ul><li>time: 2920</li><li>lat: 24</li><li>lon: 52</li></ul><ul><li>244.5 244.7 244.2 243.4 242.4 241.7 ... 297.4 297.2 296.5 296.2 295.7<pre>array([[[244.5    , 244.7    , 244.2    , ..., 232.79999, 235.29999,\n         239.29999],\n        [249.79999, 248.89   , 247.5    , ..., 233.2    , 236.39   ,\n         241.7    ],\n        [267.1    , 267.1    , 266.69998, ..., 249.2    , 253.09999,\n         256.9    ],\n        ...,\n        [296.19998, 296.4    , 296.5    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [296.19998, 296.79   , 296.5    , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.79   , 297.1    , 297.     , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[244.09999, 244.2    , 244.09999, ..., 231.     , 232.5    ,\n         235.7    ],\n        [252.89   , 252.09999, 250.79999, ..., 230.79999, 233.39   ,\n         238.5    ],\n        [269.4    , 268.6    , 267.4    , ..., 247.2    , 250.5    ,\n         254.39   ],\n...\n        [293.88998, 295.38998, 297.19   , ..., 295.09   , 294.69   ,\n         294.29   ],\n        [297.19   , 297.59   , 297.88998, ..., 295.29   , 295.09   ,\n         294.38998],\n        [298.38998, 298.49   , 298.59   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[249.29   , 248.39   , 246.98999, ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.19   , 261.38998, 259.99   , ..., 239.89   , 242.59   ,\n         246.29   ],\n        [272.09   , 271.99   , 271.59   , ..., 255.39   , 258.99   ,\n         262.49   ],\n        ...,\n        [293.69   , 295.09   , 296.69   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.88998, 297.19   , 297.49   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [298.09   , 298.09   , 298.49   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat(lat)float3272.5 70.0 67.5 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5, 45. ,\n       42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5, 15. ],\n      dtype=float32)</pre></li><li>lon(lon)float32202.5 205.0 207.5 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5, 225. ,\n       227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5, 250. ,\n       252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5, 275. ,\n       277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5, 300. ,\n       302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5, 325. ,\n       327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5, 45.0,\n       42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5, 15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5, 225.0,\n       227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5, 250.0,\n       252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5, 275.0,\n       277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5, 300.0,\n       302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5, 325.0,\n       327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (12)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray</li></ul> <p>So far, all the above will require us to specify exact coordinate values, but what if we don't have the exact values? We can use nearest neighbor lookups to address this issue:</p> In\u00a0[26]: Copied! <pre>da.sel(lat=52.25, lon=251.8998, method=\"nearest\")\n</pre> da.sel(lat=52.25, lon=251.8998, method=\"nearest\") Out[26]: <pre>&lt;xarray.DataArray 'air' (time: 2920)&gt;\narray([262.69998, 263.19998, 270.9    , ..., 264.19   , 265.19   ,\n       266.99   ], dtype=float32)\nCoordinates:\n    lat      float32 52.5\n    lon      float32 252.5\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]\n    new_attr:      xarray</pre>xarray.DataArray'air'<ul><li>time: 2920</li></ul><ul><li>262.7 263.2 270.9 274.1 273.3 270.6 ... 253.4 261.6 264.2 265.2 267.0<pre>array([262.69998, 263.19998, 270.9    , ..., 264.19   , 265.19   ,\n       266.99   ], dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat()float3252.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array(52.5, dtype=float32)</pre></li><li>lon()float32252.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array(252.5, dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (12)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray</li></ul> In\u00a0[27]: Copied! <pre>da.sel(lat=52.25, lon=251.8998, method=\"ffill\")\n</pre> da.sel(lat=52.25, lon=251.8998, method=\"ffill\") Out[27]: <pre>&lt;xarray.DataArray 'air' (time: 2920)&gt;\narray([269.5    , 269.29   , 273.69998, ..., 267.49   , 269.29   ,\n       268.69   ], dtype=float32)\nCoordinates:\n    lat      float32 52.5\n    lon      float32 250.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]\n    new_attr:      xarray</pre>xarray.DataArray'air'<ul><li>time: 2920</li></ul><ul><li>269.5 269.3 273.7 273.5 272.4 270.3 ... 257.4 266.5 267.5 269.3 268.7<pre>array([269.5    , 269.29   , 273.69998, ..., 267.49   , 269.29   ,\n       268.69   ], dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat()float3252.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array(52.5, dtype=float32)</pre></li><li>lon()float32250.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array(250., dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (12)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray</li></ul> <p><code>tolerance</code> argument limits the maximum distance for valid matches with an inexact lookup:</p> In\u00a0[28]: Copied! <pre>da.sel(lat=52.25, lon=251.8998, method=\"nearest\", tolerance=2)\n</pre> da.sel(lat=52.25, lon=251.8998, method=\"nearest\", tolerance=2) Out[28]: <pre>&lt;xarray.DataArray 'air' (time: 2920)&gt;\narray([262.69998, 263.19998, 270.9    , ..., 264.19   , 265.19   ,\n       266.99   ], dtype=float32)\nCoordinates:\n    lat      float32 52.5\n    lon      float32 252.5\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]\n    new_attr:      xarray</pre>xarray.DataArray'air'<ul><li>time: 2920</li></ul><ul><li>262.7 263.2 270.9 274.1 273.3 270.6 ... 253.4 261.6 264.2 265.2 267.0<pre>array([262.69998, 263.19998, 270.9    , ..., 264.19   , 265.19   ,\n       266.99   ], dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat()float3252.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array(52.5, dtype=float32)</pre></li><li>lon()float32252.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array(252.5, dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (12)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray</li></ul> In\u00a0[29]: Copied! <pre>da.sel(lat=52.25, lon=198, method=\"nearest\", tolerance=2)\n</pre> da.sel(lat=52.25, lon=198, method=\"nearest\", tolerance=2) Out[29]: <pre>&lt;xarray.DataArray 'air' (time: 2920)&gt;\narray([276.69998, 275.79   , 275.29   , ..., 277.49   , 276.79   ,\n       276.88998], dtype=float32)\nCoordinates:\n    lat      float32 52.5\n    lon      float32 200.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]\n    new_attr:      xarray</pre>xarray.DataArray'air'<ul><li>time: 2920</li></ul><ul><li>276.7 275.8 275.3 275.6 275.7 276.2 ... 277.7 277.2 277.5 276.8 276.9<pre>array([276.69998, 275.79   , 275.29   , ..., 277.49   , 276.79   ,\n       276.88998], dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat()float3252.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array(52.5, dtype=float32)</pre></li><li>lon()float32200.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array(200., dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (12)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray</li></ul> <pre><code>{tip}\nAll of these indexing methods work on the dataset too!\n</code></pre> <p>We can also use these methods to index all variables in a dataset simultaneously, returning a new dataset:</p> In\u00a0[30]: Copied! <pre>ds.sel(lat=52.25, lon=251.8998, method=\"nearest\")\n</pre> ds.sel(lat=52.25, lon=251.8998, method=\"nearest\") Out[30]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (time: 2920)\nCoordinates:\n    lat      float32 52.5\n    lon      float32 252.5\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time) float32 262.7 263.2 270.9 274.1 ... 261.6 264.2 265.2 267.0\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 2920</li></ul></li><li>Coordinates: (3)<ul><li>lat()float3252.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array(52.5, dtype=float32)</pre></li><li>lon()float32252.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array(252.5, dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air(time)float32262.7 263.2 270.9 ... 265.2 267.0long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray<pre>array([262.69998, 263.19998, 270.9    , ..., 264.19   , 265.19   ,\n       266.99   ], dtype=float32)</pre></li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> In\u00a0[31]: Copied! <pre>ds.sel(time='2013-01-01 06:00')\n</pre> ds.sel(time='2013-01-01 06:00') Out[31]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n    time     datetime64[ns] 2013-01-01T06:00:00\nData variables:\n    air      (lat, lon) float32 242.1 242.7 243.1 243.4 ... 296.4 296.4 296.6\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time()datetime64[ns]2013-01-01T06:00:00standard_name :timelong_name :Time<pre>array('2013-01-01T06:00:00.000000000', dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air(lat, lon)float32242.1 242.7 243.1 ... 296.4 296.6long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray<pre>array([[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n        235.79999],\n       [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n        235.7    ],\n       [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n        238.5    ],\n       ...,\n       [296.4    , 295.9    , 296.19998, ..., 295.4    , 295.1    ,\n        294.79   ],\n       [296.19998, 296.69998, 296.79   , ..., 295.6    , 295.5    ,\n        295.1    ],\n       [296.29   , 297.19998, 297.4    , ..., 296.4    , 296.4    ,\n        296.6    ]], dtype=float32)</pre></li></ul></li><li>Indexes: (2)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> <p>By default, datetime selection will return a range of values that match the provided string. For e.g. <code>time=\"2013-01-01\"</code> will return all timestamps for that day (4 of them here):</p> In\u00a0[32]: Copied! <pre>ds.sel(time='2013-01-01')\n</pre> ds.sel(time='2013-01-01') Out[32]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 4, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-01-01T18:00:00\nData variables:\n    air      (time, lat, lon) float32 241.2 242.5 243.5 ... 297.8 298.0 297.9\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>time: 4</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2013-01-01T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air(time, lat, lon)float32241.2 242.5 243.5 ... 298.0 297.9long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [295.6    , 295.4    , 295.4    , ..., 296.29   , 295.29   ,\n         295.     ],\n        [296.19998, 296.5    , 296.29   , ..., 296.4    , 296.     ,\n         295.6    ],\n        [296.4    , 296.29   , 296.4    , ..., 297.     , 297.     ,\n         296.79   ]],\n\n       [[241.89   , 241.79999, 241.79999, ..., 234.39   , 235.5    ,\n         237.59999],\n        [246.29999, 245.29999, 244.2    , ..., 230.89   , 231.5    ,\n         234.5    ],\n        [256.6    , 254.7    , 252.09999, ..., 230.7    , 231.79999,\n         236.09999],\n        ...,\n        [296.6    , 296.4    , 296.     , ..., 296.5    , 295.79   ,\n         295.29   ],\n        [297.     , 297.5    , 297.1    , ..., 296.79   , 296.6    ,\n         296.29   ],\n        [297.5    , 297.69998, 297.5    , ..., 297.79   , 298.     ,\n         297.9    ]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> <p>We can use this feature to select all points in a year:</p> In\u00a0[33]: Copied! <pre>ds.sel(time=\"2014\")\n</pre> ds.sel(time=\"2014\") Out[33]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 1460, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2014-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 252.3 251.2 250.0 ... 296.5 296.2 295.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>time: 1460</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2014-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2014-01-01T00:00:00.000000000', '2014-01-01T06:00:00.000000000',\n       '2014-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air(time, lat, lon)float32252.3 251.2 250.0 ... 296.2 295.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray<pre>array([[[252.29999, 251.2    , 250.     , ..., 240.29999, 241.59999,\n         243.2    ],\n        [252.39   , 252.79999, 253.     , ..., 235.59999, 238.29999,\n         242.29999],\n        [246.5    , 247.59999, 249.2    , ..., 241.5    , 245.2    ,\n         250.39   ],\n        ...,\n        [297.5    , 297.19998, 296.9    , ..., 295.9    , 295.19998,\n         294.6    ],\n        [298.19998, 298.1    , 297.6    , ..., 295.9    , 295.6    ,\n         294.69998],\n        [298.29   , 298.6    , 298.4    , ..., 296.     , 295.29   ,\n         294.69998]],\n\n       [[252.5    , 251.2    , 249.7    , ..., 240.2    , 241.5    ,\n         243.2    ],\n        [253.5    , 253.59999, 253.39   , ..., 237.39   , 239.29999,\n         242.59999],\n        [250.     , 250.7    , 251.7    , ..., 241.5    , 244.59999,\n         249.     ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2014-01-01 00:00:00', '2014-01-01 06:00:00',\n               '2014-01-01 12:00:00', '2014-01-01 18:00:00',\n               '2014-01-02 00:00:00', '2014-01-02 06:00:00',\n               '2014-01-02 12:00:00', '2014-01-02 18:00:00',\n               '2014-01-03 00:00:00', '2014-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=1460, freq=None))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> <p>or a month:</p> In\u00a0[34]: Copied! <pre>ds.sel(time=\"2014-May\")\n\n# ds.sel(time=\"2014-05\")\n</pre> ds.sel(time=\"2014-May\")  # ds.sel(time=\"2014-05\") Out[34]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 124, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2014-05-01 ... 2014-05-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 264.9 265.0 265.0 ... 296.5 296.2 296.2\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>time: 124</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2014-05-01 ... 2014-05-31T18:00:00standard_name :timelong_name :Time<pre>array(['2014-05-01T00:00:00.000000000', '2014-05-01T06:00:00.000000000',\n       '2014-05-01T12:00:00.000000000', '2014-05-01T18:00:00.000000000',\n       '2014-05-02T00:00:00.000000000', '2014-05-02T06:00:00.000000000',\n       '2014-05-02T12:00:00.000000000', '2014-05-02T18:00:00.000000000',\n       '2014-05-03T00:00:00.000000000', '2014-05-03T06:00:00.000000000',\n       '2014-05-03T12:00:00.000000000', '2014-05-03T18:00:00.000000000',\n       '2014-05-04T00:00:00.000000000', '2014-05-04T06:00:00.000000000',\n       '2014-05-04T12:00:00.000000000', '2014-05-04T18:00:00.000000000',\n       '2014-05-05T00:00:00.000000000', '2014-05-05T06:00:00.000000000',\n       '2014-05-05T12:00:00.000000000', '2014-05-05T18:00:00.000000000',\n       '2014-05-06T00:00:00.000000000', '2014-05-06T06:00:00.000000000',\n       '2014-05-06T12:00:00.000000000', '2014-05-06T18:00:00.000000000',\n       '2014-05-07T00:00:00.000000000', '2014-05-07T06:00:00.000000000',\n       '2014-05-07T12:00:00.000000000', '2014-05-07T18:00:00.000000000',\n       '2014-05-08T00:00:00.000000000', '2014-05-08T06:00:00.000000000',\n       '2014-05-08T12:00:00.000000000', '2014-05-08T18:00:00.000000000',\n       '2014-05-09T00:00:00.000000000', '2014-05-09T06:00:00.000000000',\n       '2014-05-09T12:00:00.000000000', '2014-05-09T18:00:00.000000000',\n       '2014-05-10T00:00:00.000000000', '2014-05-10T06:00:00.000000000',\n       '2014-05-10T12:00:00.000000000', '2014-05-10T18:00:00.000000000',\n       '2014-05-11T00:00:00.000000000', '2014-05-11T06:00:00.000000000',\n       '2014-05-11T12:00:00.000000000', '2014-05-11T18:00:00.000000000',\n       '2014-05-12T00:00:00.000000000', '2014-05-12T06:00:00.000000000',\n       '2014-05-12T12:00:00.000000000', '2014-05-12T18:00:00.000000000',\n       '2014-05-13T00:00:00.000000000', '2014-05-13T06:00:00.000000000',\n       '2014-05-13T12:00:00.000000000', '2014-05-13T18:00:00.000000000',\n       '2014-05-14T00:00:00.000000000', '2014-05-14T06:00:00.000000000',\n       '2014-05-14T12:00:00.000000000', '2014-05-14T18:00:00.000000000',\n       '2014-05-15T00:00:00.000000000', '2014-05-15T06:00:00.000000000',\n       '2014-05-15T12:00:00.000000000', '2014-05-15T18:00:00.000000000',\n       '2014-05-16T00:00:00.000000000', '2014-05-16T06:00:00.000000000',\n       '2014-05-16T12:00:00.000000000', '2014-05-16T18:00:00.000000000',\n       '2014-05-17T00:00:00.000000000', '2014-05-17T06:00:00.000000000',\n       '2014-05-17T12:00:00.000000000', '2014-05-17T18:00:00.000000000',\n       '2014-05-18T00:00:00.000000000', '2014-05-18T06:00:00.000000000',\n       '2014-05-18T12:00:00.000000000', '2014-05-18T18:00:00.000000000',\n       '2014-05-19T00:00:00.000000000', '2014-05-19T06:00:00.000000000',\n       '2014-05-19T12:00:00.000000000', '2014-05-19T18:00:00.000000000',\n       '2014-05-20T00:00:00.000000000', '2014-05-20T06:00:00.000000000',\n       '2014-05-20T12:00:00.000000000', '2014-05-20T18:00:00.000000000',\n       '2014-05-21T00:00:00.000000000', '2014-05-21T06:00:00.000000000',\n       '2014-05-21T12:00:00.000000000', '2014-05-21T18:00:00.000000000',\n       '2014-05-22T00:00:00.000000000', '2014-05-22T06:00:00.000000000',\n       '2014-05-22T12:00:00.000000000', '2014-05-22T18:00:00.000000000',\n       '2014-05-23T00:00:00.000000000', '2014-05-23T06:00:00.000000000',\n       '2014-05-23T12:00:00.000000000', '2014-05-23T18:00:00.000000000',\n       '2014-05-24T00:00:00.000000000', '2014-05-24T06:00:00.000000000',\n       '2014-05-24T12:00:00.000000000', '2014-05-24T18:00:00.000000000',\n       '2014-05-25T00:00:00.000000000', '2014-05-25T06:00:00.000000000',\n       '2014-05-25T12:00:00.000000000', '2014-05-25T18:00:00.000000000',\n       '2014-05-26T00:00:00.000000000', '2014-05-26T06:00:00.000000000',\n       '2014-05-26T12:00:00.000000000', '2014-05-26T18:00:00.000000000',\n       '2014-05-27T00:00:00.000000000', '2014-05-27T06:00:00.000000000',\n       '2014-05-27T12:00:00.000000000', '2014-05-27T18:00:00.000000000',\n       '2014-05-28T00:00:00.000000000', '2014-05-28T06:00:00.000000000',\n       '2014-05-28T12:00:00.000000000', '2014-05-28T18:00:00.000000000',\n       '2014-05-29T00:00:00.000000000', '2014-05-29T06:00:00.000000000',\n       '2014-05-29T12:00:00.000000000', '2014-05-29T18:00:00.000000000',\n       '2014-05-30T00:00:00.000000000', '2014-05-30T06:00:00.000000000',\n       '2014-05-30T12:00:00.000000000', '2014-05-30T18:00:00.000000000',\n       '2014-05-31T00:00:00.000000000', '2014-05-31T06:00:00.000000000',\n       '2014-05-31T12:00:00.000000000', '2014-05-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air(time, lat, lon)float32264.9 265.0 265.0 ... 296.2 296.2long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray<pre>array([[[264.9    , 265.     , 265.     , ..., 252.39   , 252.89   ,\n         253.7    ],\n        [272.     , 272.29   , 272.6    , ..., 249.29999, 250.2    ,\n         252.39   ],\n        [274.1    , 274.     , 274.     , ..., 252.39   , 254.89   ,\n         258.69998],\n        ...,\n        [296.6    , 295.69998, 295.79   , ..., 294.5    , 294.19998,\n         293.9    ],\n        [297.79   , 297.4    , 296.9    , ..., 294.79   , 294.69998,\n         294.4    ],\n        [298.1    , 298.4    , 298.     , ..., 295.29   , 295.5    ,\n         295.1    ]],\n\n       [[265.79   , 265.4    , 265.1    , ..., 248.89   , 250.5    ,\n         252.     ],\n        [272.6    , 272.6    , 272.6    , ..., 247.09999, 249.     ,\n         252.09999],\n        [274.     , 274.1    , 274.29   , ..., 247.59999, 250.7    ,\n         255.7    ],\n...\n        [297.4    , 296.79   , 296.4    , ..., 295.79   , 295.     ,\n         294.5    ],\n        [297.4    , 297.19998, 296.29   , ..., 295.9    , 295.69998,\n         295.1    ],\n        [297.79   , 297.6    , 297.29   , ..., 296.1    , 295.9    ,\n         295.79   ]],\n\n       [[267.     , 266.79   , 266.5    , ..., 263.     , 265.     ,\n         267.1    ],\n        [268.9    , 269.29   , 269.4    , ..., 262.6    , 265.1    ,\n         267.9    ],\n        [272.19998, 272.19998, 272.19998, ..., 265.1    , 267.19998,\n         269.79   ],\n        ...,\n        [298.     , 297.69998, 297.19998, ..., 295.9    , 295.19998,\n         294.9    ],\n        [297.9    , 297.9    , 297.4    , ..., 296.29   , 295.9    ,\n         295.4    ],\n        [297.9    , 297.6    , 297.5    , ..., 296.5    , 296.19998,\n         296.19998]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2014-05-01 00:00:00', '2014-05-01 06:00:00',\n               '2014-05-01 12:00:00', '2014-05-01 18:00:00',\n               '2014-05-02 00:00:00', '2014-05-02 06:00:00',\n               '2014-05-02 12:00:00', '2014-05-02 18:00:00',\n               '2014-05-03 00:00:00', '2014-05-03 06:00:00',\n               ...\n               '2014-05-29 12:00:00', '2014-05-29 18:00:00',\n               '2014-05-30 00:00:00', '2014-05-30 06:00:00',\n               '2014-05-30 12:00:00', '2014-05-30 18:00:00',\n               '2014-05-31 00:00:00', '2014-05-31 06:00:00',\n               '2014-05-31 12:00:00', '2014-05-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=124, freq=None))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> In\u00a0[35]: Copied! <pre># This will return a subset of the dataset corresponding to the entire year of 2013.\nds.sel(time=slice('2013-01-01', '2013-12-31'))\n</pre> # This will return a subset of the dataset corresponding to the entire year of 2013. ds.sel(time=slice('2013-01-01', '2013-12-31')) Out[35]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 1460, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 241.2 242.5 243.5 ... 296.1 295.1 294.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>time: 1460</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2013-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2013-12-31T06:00:00.000000000',\n       '2013-12-31T12:00:00.000000000', '2013-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air(time, lat, lon)float32241.2 242.5 243.5 ... 295.1 294.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [296.1    , 295.1    , 295.1    , ..., 296.     , 294.69998,\n         293.79   ],\n        [297.     , 296.6    , 296.29   , ..., 295.6    , 294.6    ,\n         293.79   ],\n        [297.4    , 297.4    , 297.19998, ..., 296.1    , 295.     ,\n         294.4    ]],\n\n       [[251.89   , 251.2    , 250.29999, ..., 239.7    , 239.89   ,\n         240.7    ],\n        [252.29999, 253.     , 253.5    , ..., 237.29999, 239.5    ,\n         242.89   ],\n        [249.09999, 250.2    , 251.59999, ..., 243.     , 247.     ,\n         251.89   ],\n        ...,\n        [296.4    , 295.6    , 295.19998, ..., 295.9    , 295.1    ,\n         294.     ],\n        [297.1    , 296.79   , 296.5    , ..., 295.6    , 295.1    ,\n         294.5    ],\n        [297.29   , 297.69998, 297.69998, ..., 296.1    , 295.1    ,\n         294.69998]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2013-12-29 12:00:00', '2013-12-29 18:00:00',\n               '2013-12-30 00:00:00', '2013-12-30 06:00:00',\n               '2013-12-30 12:00:00', '2013-12-30 18:00:00',\n               '2013-12-31 00:00:00', '2013-12-31 06:00:00',\n               '2013-12-31 12:00:00', '2013-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=1460, freq=None))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> <p>The slice function takes two arguments, start and stop, to make a slice that includes these endpoints. When we use <code>slice</code> with the <code>sel</code> method, it provides an efficient way to select a range of dates. The above example shows the usage of slice for datetime indexing.</p> In\u00a0[36]: Copied! <pre>dates = ['2013-07-09', '2013-10-11', '2013-12-24']\nds.sel(time=dates)\n</pre> dates = ['2013-07-09', '2013-10-11', '2013-12-24'] ds.sel(time=dates) Out[36]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 3, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-07-09 2013-10-11 2013-12-24\nData variables:\n    air      (time, lat, lon) float32 279.0 278.6 278.1 ... 296.8 296.6 296.5\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>time: 3</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-07-09 2013-10-11 2013-12-24standard_name :timelong_name :Time<pre>array(['2013-07-09T00:00:00.000000000', '2013-10-11T00:00:00.000000000',\n       '2013-12-24T00:00:00.000000000'], dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air(time, lat, lon)float32279.0 278.6 278.1 ... 296.6 296.5long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray<pre>array([[[279.     , 278.6    , 278.1    , ..., 269.6    , 272.     ,\n         274.5    ],\n        [277.29   , 277.     , 276.5    , ..., 267.4    , 269.29   ,\n         271.69998],\n        [280.5    , 280.69998, 280.5    , ..., 266.69998, 268.19998,\n         270.29   ],\n        ...,\n        [299.6    , 299.1    , 298.4    , ..., 297.1    , 296.79   ,\n         296.     ],\n        [299.19998, 298.9    , 298.5    , ..., 296.69998, 296.79   ,\n         296.5    ],\n        [299.4    , 299.29   , 298.79   , ..., 297.29   , 297.29   ,\n         297.4    ]],\n\n       [[268.4    , 268.19998, 268.1    , ..., 247.09999, 247.09999,\n         247.7    ],\n        [272.69998, 272.6    , 272.6    , ..., 250.     , 250.79999,\n         252.7    ],\n        [270.69998, 269.29   , 268.4    , ..., 255.5    , 257.79   ,\n         261.4    ],\n...\n        [299.1    , 298.5    , 298.1    , ..., 298.4    , 298.1    ,\n         297.4    ],\n        [298.9    , 299.1    , 298.9    , ..., 299.19998, 298.9    ,\n         298.1    ],\n        [298.9    , 299.19998, 299.19998, ..., 299.29   , 299.29   ,\n         299.5    ]],\n\n       [[249.09999, 249.09999, 249.2    , ..., 244.59999, 246.79999,\n         249.5    ],\n        [249.79999, 250.5    , 251.     , ..., 243.89   , 246.     ,\n         249.2    ],\n        [246.29999, 246.39   , 246.79999, ..., 241.09999, 244.5    ,\n         249.09999],\n        ...,\n        [297.5    , 296.69998, 296.19998, ..., 295.     , 294.69998,\n         294.1    ],\n        [298.1    , 297.6    , 297.     , ..., 295.6    , 295.29   ,\n         294.69998],\n        [298.4    , 298.1    , 297.6    , ..., 296.79   , 296.6    ,\n         296.5    ]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-07-09', '2013-10-11', '2013-12-24'], dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> In\u00a0[37]: Copied! <pre>ds.sel(time=ds.time.dt.month == 7)\n</pre> ds.sel(time=ds.time.dt.month == 7) Out[37]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 248, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-07-01 ... 2014-07-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 273.7 273.0 272.5 ... 297.5 297.6 297.8\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>time: 248</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-07-01 ... 2014-07-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-07-01T00:00:00.000000000', '2013-07-01T06:00:00.000000000',\n       '2013-07-01T12:00:00.000000000', ..., '2014-07-31T06:00:00.000000000',\n       '2014-07-31T12:00:00.000000000', '2014-07-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air(time, lat, lon)float32273.7 273.0 272.5 ... 297.6 297.8long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray<pre>array([[[273.69998, 273.     , 272.5    , ..., 265.29   , 266.6    ,\n         268.19998],\n        [275.5    , 275.19998, 274.9    , ..., 265.4    , 267.29   ,\n         269.69998],\n        [285.1    , 286.     , 286.6    , ..., 265.     , 267.4    ,\n         270.6    ],\n        ...,\n        [299.19998, 298.69998, 297.69998, ..., 296.6    , 296.5    ,\n         296.4    ],\n        [298.5    , 298.6    , 298.     , ..., 296.     , 296.1    ,\n         295.9    ],\n        [298.29   , 298.69998, 298.4    , ..., 297.19998, 297.29   ,\n         297.1    ]],\n\n       [[274.     , 273.69998, 273.29   , ..., 261.6    , 263.29   ,\n         265.29   ],\n        [278.4    , 278.6    , 278.5    , ..., 260.     , 262.9    ,\n         266.19998],\n        [286.6    , 287.69998, 288.4    , ..., 260.69998, 264.9    ,\n         269.1    ],\n...\n        [298.79   , 297.79   , 297.29   , ..., 297.9    , 297.     ,\n         296.79   ],\n        [299.4    , 298.79   , 298.4    , ..., 297.4    , 297.19998,\n         297.     ],\n        [299.5    , 299.1    , 298.69998, ..., 297.4    , 297.5    ,\n         297.69998]],\n\n       [[274.     , 273.6    , 273.     , ..., 265.29   , 266.5    ,\n         268.19998],\n        [276.     , 276.     , 275.79   , ..., 265.79   , 267.5    ,\n         269.79   ],\n        [281.6    , 283.1    , 284.4    , ..., 269.4    , 271.5    ,\n         274.1    ],\n        ...,\n        [299.4    , 298.79   , 298.29   , ..., 298.1    , 297.4    ,\n         297.19998],\n        [299.79   , 299.6    , 299.1    , ..., 297.4    , 297.29   ,\n         297.19998],\n        [299.69998, 299.5    , 299.1    , ..., 297.5    , 297.6    ,\n         297.79   ]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-07-01 00:00:00', '2013-07-01 06:00:00',\n               '2013-07-01 12:00:00', '2013-07-01 18:00:00',\n               '2013-07-02 00:00:00', '2013-07-02 06:00:00',\n               '2013-07-02 12:00:00', '2013-07-02 18:00:00',\n               '2013-07-03 00:00:00', '2013-07-03 06:00:00',\n               ...\n               '2014-07-29 12:00:00', '2014-07-29 18:00:00',\n               '2014-07-30 00:00:00', '2014-07-30 06:00:00',\n               '2014-07-30 12:00:00', '2014-07-30 18:00:00',\n               '2014-07-31 00:00:00', '2014-07-31 06:00:00',\n               '2014-07-31 12:00:00', '2014-07-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=248, freq=None))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> <p>Or, if you wanted to select data from a specific day of each month, you could use:</p> In\u00a0[38]: Copied! <pre>ds.sel(time=ds.time.dt.day == 15)\n</pre> ds.sel(time=ds.time.dt.day == 15) Out[38]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 96, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-15 ... 2014-12-15T18:00:00\nData variables:\n    air      (time, lat, lon) float32 243.8 243.4 242.8 ... 297.1 296.9 296.9\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>time: 96</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-15 ... 2014-12-15T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-15T00:00:00.000000000', '2013-01-15T06:00:00.000000000',\n       '2013-01-15T12:00:00.000000000', '2013-01-15T18:00:00.000000000',\n       '2013-02-15T00:00:00.000000000', '2013-02-15T06:00:00.000000000',\n       '2013-02-15T12:00:00.000000000', '2013-02-15T18:00:00.000000000',\n       '2013-03-15T00:00:00.000000000', '2013-03-15T06:00:00.000000000',\n       '2013-03-15T12:00:00.000000000', '2013-03-15T18:00:00.000000000',\n       '2013-04-15T00:00:00.000000000', '2013-04-15T06:00:00.000000000',\n       '2013-04-15T12:00:00.000000000', '2013-04-15T18:00:00.000000000',\n       '2013-05-15T00:00:00.000000000', '2013-05-15T06:00:00.000000000',\n       '2013-05-15T12:00:00.000000000', '2013-05-15T18:00:00.000000000',\n       '2013-06-15T00:00:00.000000000', '2013-06-15T06:00:00.000000000',\n       '2013-06-15T12:00:00.000000000', '2013-06-15T18:00:00.000000000',\n       '2013-07-15T00:00:00.000000000', '2013-07-15T06:00:00.000000000',\n       '2013-07-15T12:00:00.000000000', '2013-07-15T18:00:00.000000000',\n       '2013-08-15T00:00:00.000000000', '2013-08-15T06:00:00.000000000',\n       '2013-08-15T12:00:00.000000000', '2013-08-15T18:00:00.000000000',\n       '2013-09-15T00:00:00.000000000', '2013-09-15T06:00:00.000000000',\n       '2013-09-15T12:00:00.000000000', '2013-09-15T18:00:00.000000000',\n       '2013-10-15T00:00:00.000000000', '2013-10-15T06:00:00.000000000',\n       '2013-10-15T12:00:00.000000000', '2013-10-15T18:00:00.000000000',\n       '2013-11-15T00:00:00.000000000', '2013-11-15T06:00:00.000000000',\n       '2013-11-15T12:00:00.000000000', '2013-11-15T18:00:00.000000000',\n       '2013-12-15T00:00:00.000000000', '2013-12-15T06:00:00.000000000',\n       '2013-12-15T12:00:00.000000000', '2013-12-15T18:00:00.000000000',\n       '2014-01-15T00:00:00.000000000', '2014-01-15T06:00:00.000000000',\n       '2014-01-15T12:00:00.000000000', '2014-01-15T18:00:00.000000000',\n       '2014-02-15T00:00:00.000000000', '2014-02-15T06:00:00.000000000',\n       '2014-02-15T12:00:00.000000000', '2014-02-15T18:00:00.000000000',\n       '2014-03-15T00:00:00.000000000', '2014-03-15T06:00:00.000000000',\n       '2014-03-15T12:00:00.000000000', '2014-03-15T18:00:00.000000000',\n       '2014-04-15T00:00:00.000000000', '2014-04-15T06:00:00.000000000',\n       '2014-04-15T12:00:00.000000000', '2014-04-15T18:00:00.000000000',\n       '2014-05-15T00:00:00.000000000', '2014-05-15T06:00:00.000000000',\n       '2014-05-15T12:00:00.000000000', '2014-05-15T18:00:00.000000000',\n       '2014-06-15T00:00:00.000000000', '2014-06-15T06:00:00.000000000',\n       '2014-06-15T12:00:00.000000000', '2014-06-15T18:00:00.000000000',\n       '2014-07-15T00:00:00.000000000', '2014-07-15T06:00:00.000000000',\n       '2014-07-15T12:00:00.000000000', '2014-07-15T18:00:00.000000000',\n       '2014-08-15T00:00:00.000000000', '2014-08-15T06:00:00.000000000',\n       '2014-08-15T12:00:00.000000000', '2014-08-15T18:00:00.000000000',\n       '2014-09-15T00:00:00.000000000', '2014-09-15T06:00:00.000000000',\n       '2014-09-15T12:00:00.000000000', '2014-09-15T18:00:00.000000000',\n       '2014-10-15T00:00:00.000000000', '2014-10-15T06:00:00.000000000',\n       '2014-10-15T12:00:00.000000000', '2014-10-15T18:00:00.000000000',\n       '2014-11-15T00:00:00.000000000', '2014-11-15T06:00:00.000000000',\n       '2014-11-15T12:00:00.000000000', '2014-11-15T18:00:00.000000000',\n       '2014-12-15T00:00:00.000000000', '2014-12-15T06:00:00.000000000',\n       '2014-12-15T12:00:00.000000000', '2014-12-15T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air(time, lat, lon)float32243.8 243.4 242.8 ... 296.9 296.9long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray<pre>array([[[243.79999, 243.39   , 242.79999, ..., 232.59999, 233.2    ,\n         234.7    ],\n        [247.     , 246.79999, 246.2    , ..., 233.7    , 233.89   ,\n         236.     ],\n        [261.5    , 260.9    , 259.6    , ..., 236.39   , 238.89   ,\n         243.39   ],\n        ...,\n        [296.5    , 297.29   , 297.6    , ..., 297.1    , 296.6    ,\n         295.4    ],\n        [297.5    , 297.9    , 297.4    , ..., 297.1    , 297.     ,\n         295.79   ],\n        [297.79   , 298.19998, 298.     , ..., 297.5    , 297.19998,\n         296.6    ]],\n\n       [[244.09999, 243.59999, 242.79999, ..., 234.29999, 234.79999,\n         235.79999],\n        [246.7    , 246.5    , 246.09999, ..., 236.5    , 236.29999,\n         237.59999],\n        [258.1    , 258.5    , 258.6    , ..., 239.09999, 240.89   ,\n         244.2    ],\n...\n        [296.79   , 295.79   , 295.38998, ..., 295.79   , 295.19998,\n         295.     ],\n        [297.79   , 297.5    , 297.     , ..., 295.79   , 295.6    ,\n         295.29   ],\n        [298.29   , 298.19998, 298.     , ..., 296.6    , 296.19998,\n         296.1    ]],\n\n       [[244.5    , 243.79999, 243.     , ..., 236.39   , 237.79999,\n         240.     ],\n        [243.89   , 244.68999, 245.29999, ..., 230.89   , 231.69   ,\n         234.79999],\n        [243.29999, 243.29999, 243.5    , ..., 232.29999, 233.09999,\n         237.09999],\n        ...,\n        [296.79   , 295.69998, 295.29   , ..., 296.29   , 295.79   ,\n         295.29   ],\n        [298.1    , 297.38998, 296.88998, ..., 296.29   , 296.1    ,\n         295.88998],\n        [298.38998, 298.38998, 298.19998, ..., 297.1    , 296.88998,\n         296.88998]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-15 00:00:00', '2013-01-15 06:00:00',\n               '2013-01-15 12:00:00', '2013-01-15 18:00:00',\n               '2013-02-15 00:00:00', '2013-02-15 06:00:00',\n               '2013-02-15 12:00:00', '2013-02-15 18:00:00',\n               '2013-03-15 00:00:00', '2013-03-15 06:00:00',\n               '2013-03-15 12:00:00', '2013-03-15 18:00:00',\n               '2013-04-15 00:00:00', '2013-04-15 06:00:00',\n               '2013-04-15 12:00:00', '2013-04-15 18:00:00',\n               '2013-05-15 00:00:00', '2013-05-15 06:00:00',\n               '2013-05-15 12:00:00', '2013-05-15 18:00:00',\n               '2013-06-15 00:00:00', '2013-06-15 06:00:00',\n               '2013-06-15 12:00:00', '2013-06-15 18:00:00',\n               '2013-07-15 00:00:00', '2013-07-15 06:00:00',\n               '2013-07-15 12:00:00', '2013-07-15 18:00:00',\n               '2013-08-15 00:00:00', '2013-08-15 06:00:00',\n               '2013-08-15 12:00:00', '2013-08-15 18:00:00',\n               '2013-09-15 00:00:00', '2013-09-15 06:00:00',\n               '2013-09-15 12:00:00', '2013-09-15 18:00:00',\n               '2013-10-15 00:00:00', '2013-10-15 06:00:00',\n               '2013-10-15 12:00:00', '2013-10-15 18:00:00',\n               '2013-11-15 00:00:00', '2013-11-15 06:00:00',\n               '2013-11-15 12:00:00', '2013-11-15 18:00:00',\n               '2013-12-15 00:00:00', '2013-12-15 06:00:00',\n               '2013-12-15 12:00:00', '2013-12-15 18:00:00',\n               '2014-01-15 00:00:00', '2014-01-15 06:00:00',\n               '2014-01-15 12:00:00', '2014-01-15 18:00:00',\n               '2014-02-15 00:00:00', '2014-02-15 06:00:00',\n               '2014-02-15 12:00:00', '2014-02-15 18:00:00',\n               '2014-03-15 00:00:00', '2014-03-15 06:00:00',\n               '2014-03-15 12:00:00', '2014-03-15 18:00:00',\n               '2014-04-15 00:00:00', '2014-04-15 06:00:00',\n               '2014-04-15 12:00:00', '2014-04-15 18:00:00',\n               '2014-05-15 00:00:00', '2014-05-15 06:00:00',\n               '2014-05-15 12:00:00', '2014-05-15 18:00:00',\n               '2014-06-15 00:00:00', '2014-06-15 06:00:00',\n               '2014-06-15 12:00:00', '2014-06-15 18:00:00',\n               '2014-07-15 00:00:00', '2014-07-15 06:00:00',\n               '2014-07-15 12:00:00', '2014-07-15 18:00:00',\n               '2014-08-15 00:00:00', '2014-08-15 06:00:00',\n               '2014-08-15 12:00:00', '2014-08-15 18:00:00',\n               '2014-09-15 00:00:00', '2014-09-15 06:00:00',\n               '2014-09-15 12:00:00', '2014-09-15 18:00:00',\n               '2014-10-15 00:00:00', '2014-10-15 06:00:00',\n               '2014-10-15 12:00:00', '2014-10-15 18:00:00',\n               '2014-11-15 00:00:00', '2014-11-15 06:00:00',\n               '2014-11-15 12:00:00', '2014-11-15 18:00:00',\n               '2014-12-15 00:00:00', '2014-12-15 06:00:00',\n               '2014-12-15 12:00:00', '2014-12-15 18:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> <p>Similar to Pandas, xarray supports different kinds of arithmetic operations</p> In\u00a0[39]: Copied! <pre>ds['air_C'] = ds['air'] - 273.15\n</pre> ds['air_C'] = ds['air'] - 273.15 In\u00a0[40]: Copied! <pre>ds['air_C']\n</pre> ds['air_C'] Out[40]: <pre>&lt;xarray.DataArray 'air_C' (time: 2920, lat: 25, lon: 53)&gt;\narray([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 20.540009,  20.73999 ,  22.23999 , ...,  21.940002,\n          21.540009,  21.140015],\n        [ 23.140015,  24.040009,  24.440002, ...,  22.140015,\n          21.940002,  21.23999 ],\n        [ 24.640015,  25.23999 ,  25.339996, ...,  22.540009,\n          22.339996,  22.040009]],\n\n       [[-28.059998, -28.86    , -29.86    , ..., -31.460007,\n         -31.660004, -31.36    ],\n        [-23.259995, -23.86    , -24.759995, ..., -33.559998,\n         -32.86    , -31.460007],\n        [-10.160004, -10.959991, -11.76001 , ..., -33.259995,\n         -30.559998, -26.86    ],\n        ...,\n        [ 20.640015,  20.540009,  21.940002, ...,  22.140015,\n          21.940002,  21.540009],\n        [ 22.940002,  23.73999 ,  24.040009, ...,  22.540009,\n          22.540009,  22.040009],\n        [ 24.540009,  24.940002,  24.940002, ...,  23.339996,\n          23.040009,  22.540009]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00</pre>xarray.DataArray'air_C'<ul><li>time: 2920</li><li>lat: 25</li><li>lon: 53</li></ul><ul><li>-31.95 -30.65 -29.65 -29.15 -29.05 ... 24.24 24.04 23.34 23.04 22.54<pre>array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 20.540009,  20.73999 ,  22.23999 , ...,  21.940002,\n          21.540009,  21.140015],\n        [ 23.140015,  24.040009,  24.440002, ...,  22.140015,\n          21.940002,  21.23999 ],\n        [ 24.640015,  25.23999 ,  25.339996, ...,  22.540009,\n          22.339996,  22.040009]],\n\n       [[-28.059998, -28.86    , -29.86    , ..., -31.460007,\n         -31.660004, -31.36    ],\n        [-23.259995, -23.86    , -24.759995, ..., -33.559998,\n         -32.86    , -31.460007],\n        [-10.160004, -10.959991, -11.76001 , ..., -33.259995,\n         -30.559998, -26.86    ],\n        ...,\n        [ 20.640015,  20.540009,  21.940002, ...,  22.140015,\n          21.940002,  21.540009],\n        [ 22.940002,  23.73999 ,  24.040009, ...,  22.540009,\n          22.540009,  22.040009],\n        [ 24.540009,  24.940002,  24.940002, ...,  23.339996,\n          23.040009,  22.540009]]], dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[41]: Copied! <pre># Generate a boolean mask where temperature is higher than 0\u02daC\nds['air_C'] &gt;  0\n</pre> # Generate a boolean mask where temperature is higher than 0\u02daC ds['air_C'] &gt;  0  Out[41]: <pre>&lt;xarray.DataArray 'air_C' (time: 2920, lat: 25, lon: 53)&gt;\narray([[[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]],\n\n       [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]],\n\n       [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n...\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]],\n\n       [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]],\n\n       [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]]])\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00</pre>xarray.DataArray'air_C'<ul><li>time: 2920</li><li>lat: 25</li><li>lon: 53</li></ul><ul><li>False False False False False False ... True True True True True True<pre>array([[[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]],\n\n       [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]],\n\n       [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n...\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]],\n\n       [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]],\n\n       [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]]])</pre></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[42]: Copied! <pre># Generate a boolean mask where long-term mean temperature is higher than 0\u02daC\nds['air_C'].mean(dim='time') &gt;  0\n</pre> # Generate a boolean mask where long-term mean temperature is higher than 0\u02daC ds['air_C'].mean(dim='time') &gt;  0  Out[42]: <pre>&lt;xarray.DataArray 'air_C' (lat: 25, lon: 53)&gt;\narray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       ...,\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True]])\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0</pre>xarray.DataArray'air_C'<ul><li>lat: 25</li><li>lon: 53</li></ul><ul><li>False False False False False False ... True True True True True True<pre>array([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       ...,\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True]])</pre></li><li>Coordinates: (2)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li></ul></li><li>Indexes: (2)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[43]: Copied! <pre>(ds['air_C'].mean(dim='time') &gt;  0).plot()\n</pre> (ds['air_C'].mean(dim='time') &gt;  0).plot() Out[43]: <pre>&lt;matplotlib.collections.QuadMesh at 0x194bbaa30&gt;</pre> In\u00a0[44]: Copied! <pre># Apply the mask:\nds.where(ds['air_C'].mean(dim='time') &gt;  0).mean(dim = 'time').air.plot()\n</pre> # Apply the mask: ds.where(ds['air_C'].mean(dim='time') &gt;  0).mean(dim = 'time').air.plot() Out[44]: <pre>&lt;matplotlib.collections.QuadMesh at 0x194c9b100&gt;</pre> In\u00a0[45]: Copied! <pre>ds['air_C'].mean(dim='time').plot()\n</pre> ds['air_C'].mean(dim='time').plot() Out[45]: <pre>&lt;matplotlib.collections.QuadMesh at 0x194dd1130&gt;</pre> In\u00a0[46]: Copied! <pre>(ds['air_C'].mean(dim='time')*np.cos(np.deg2rad(ds.lat))).plot()\n</pre> (ds['air_C'].mean(dim='time')*np.cos(np.deg2rad(ds.lat))).plot() Out[46]: <pre>&lt;matplotlib.collections.QuadMesh at 0x194e8ccd0&gt;</pre> In\u00a0[47]: Copied! <pre># Make cos(lat) two dimensional\n(xr.ones_like(ds['air_C'].mean(dim='time'))*np.cos(np.deg2rad(ds.lat))).plot()\n</pre> # Make cos(lat) two dimensional (xr.ones_like(ds['air_C'].mean(dim='time'))*np.cos(np.deg2rad(ds.lat))).plot() Out[47]: <pre>&lt;matplotlib.collections.QuadMesh at 0x194f7a3d0&gt;</pre> In\u00a0[48]: Copied! <pre># Make cos(lat) two dimensional\n(np.cos(np.deg2rad(ds.lat))*xr.ones_like(ds.lon)).plot()\n</pre> # Make cos(lat) two dimensional (np.cos(np.deg2rad(ds.lat))*xr.ones_like(ds.lon)).plot() Out[48]: <pre>&lt;matplotlib.collections.QuadMesh at 0x194aa2f10&gt;</pre> In\u00a0[49]: Copied! <pre># here's ds\nds\n</pre> # here's ds ds Out[49]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\n    air_C    (time, lat, lon) float32 -31.95 -30.65 -29.65 ... 23.34 23.04 22.54\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>time: 2920</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (2)<ul><li>air(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li><li>air_C(time, lat, lon)float32-31.95 -30.65 ... 23.04 22.54<pre>array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 20.540009,  20.73999 ,  22.23999 , ...,  21.940002,\n          21.540009,  21.140015],\n        [ 23.140015,  24.040009,  24.440002, ...,  22.140015,\n          21.940002,  21.23999 ],\n        [ 24.640015,  25.23999 ,  25.339996, ...,  22.540009,\n          22.339996,  22.040009]],\n\n       [[-28.059998, -28.86    , -29.86    , ..., -31.460007,\n         -31.660004, -31.36    ],\n        [-23.259995, -23.86    , -24.759995, ..., -33.559998,\n         -32.86    , -31.460007],\n        [-10.160004, -10.959991, -11.76001 , ..., -33.259995,\n         -30.559998, -26.86    ],\n        ...,\n        [ 20.640015,  20.540009,  21.940002, ...,  22.140015,\n          21.940002,  21.540009],\n        [ 22.940002,  23.73999 ,  24.040009, ...,  22.540009,\n          22.540009,  22.040009],\n        [ 24.540009,  24.940002,  24.940002, ...,  23.339996,\n          23.040009,  22.540009]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> In\u00a0[50]: Copied! <pre># seasonal groups\nds.groupby(\"time.season\")\n</pre> # seasonal groups ds.groupby(\"time.season\") Out[50]: <pre>DatasetGroupBy, grouped over 'season'\n4 groups with labels 'DJF', 'JJA', 'MAM', 'SON'.</pre> In\u00a0[51]: Copied! <pre># make a seasonal mean\nseasonal_mean = ds.groupby(\"time.season\").mean()\nseasonal_mean\n</pre> # make a seasonal mean seasonal_mean = ds.groupby(\"time.season\").mean() seasonal_mean Out[51]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, season: 4, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * season   (season) object 'DJF' 'JJA' 'MAM' 'SON'\nData variables:\n    air      (season, lat, lon) float32 247.0 247.0 246.7 ... 299.4 299.4 299.5\n    air_C    (season, lat, lon) float32 -26.14 -26.19 -26.43 ... 26.22 26.32\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>season: 4</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>season(season)object'DJF' 'JJA' 'MAM' 'SON'<pre>array(['DJF', 'JJA', 'MAM', 'SON'], dtype=object)</pre></li></ul></li><li>Data variables: (2)<ul><li>air(season, lat, lon)float32247.0 247.0 246.7 ... 299.4 299.5long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray<pre>array([[[247.01007, 246.95503, 246.71684, ..., 241.55386, 242.69449,\n         244.23262],\n        [248.83022, 248.98196, 248.93813, ..., 240.31064, 242.24562,\n         245.32053],\n        [250.95328, 250.49544, 250.07674, ..., 241.26309, 244.65495,\n         249.44342],\n        ...,\n        [296.19055, 295.5512 , 295.43677, ..., 295.39368, 294.8794 ,\n         294.33405],\n        [296.939  , 296.83542, 296.46448, ..., 295.6107 , 295.42157,\n         294.959  ],\n        [297.4473 , 297.49   , 297.24048, ..., 296.29694, 296.13766,\n         296.05698]],\n\n       [[273.39032, 273.2029 , 273.011  , ..., 264.78116, 266.19952,\n         267.94702],\n        [274.33145, 274.48965, 274.63034, ..., 263.89758, 265.93015,\n         268.56693],\n        [278.7285 , 279.0914 , 279.4369 , ..., 264.95203, 267.38788,\n         270.48755],\n...\n        [296.76483, 296.2117 , 295.9462 , ..., 295.5077 , 294.8774 ,\n         294.3801 ],\n        [297.43817, 297.3017 , 296.8633 , ..., 295.5603 , 295.31448,\n         294.91473],\n        [297.8931 , 297.9006 , 297.60345, ..., 295.88574, 295.74725,\n         295.72275]],\n\n       [[261.96924, 261.61057, 261.15253, ..., 248.58957, 249.53653,\n         250.96701],\n        [267.2477 , 267.07742, 266.83246, ..., 247.14966, 248.85371,\n         251.75676],\n        [268.2268 , 266.99548, 266.33118, ..., 247.50015, 250.78438,\n         255.50696],\n        ...,\n        [298.80624, 298.0542 , 297.75958, ..., 298.73898, 298.367  ,\n         297.99283],\n        [299.2114 , 299.02353, 298.54553, ..., 298.85175, 298.93045,\n         298.72104],\n        [299.21207, 299.26212, 299.0019 , ..., 299.36758, 299.37134,\n         299.47427]]], dtype=float32)</pre></li><li>air_C(season, lat, lon)float32-26.14 -26.19 ... 26.22 26.32<pre>array([[[-26.13999   , -26.194775  , -26.43317   , ..., -31.596172  ,\n         -30.455513  , -28.917418  ],\n        [-24.319826  , -24.168097  , -24.211771  , ..., -32.839237  ,\n         -30.904285  , -27.829481  ],\n        [-22.196844  , -22.654673  , -23.073437  , ..., -31.886757  ,\n         -28.495083  , -23.706408  ],\n        ...,\n        [ 23.039928  ,  22.400595  ,  22.286192  , ...,  22.242882  ,\n          21.72881   ,  21.183443  ],\n        [ 23.788368  ,  23.684774  ,  23.313797  , ...,  22.459927  ,\n          22.27082   ,  21.808601  ],\n        [ 24.29653   ,  24.339394  ,  24.089901  , ...,  23.14633   ,\n          22.987143  ,  22.906038  ]],\n\n       [[  0.23956957,   0.05219144,  -0.13998248, ...,  -8.369579  ,\n          -6.951163  ,  -5.2037587 ],\n        [  1.180723  ,   1.3389578 ,   1.4795684 , ...,  -9.2531    ,\n          -7.22022   ,  -4.583697  ],\n        [  5.577762  ,   5.9407234 ,   6.286212  , ...,  -8.1986685 ,\n          -5.7628355 ,  -2.6633291 ],\n...\n        [ 23.614012  ,  23.060812  ,  22.795355  , ...,  22.356813  ,\n          21.726547  ,  21.229237  ],\n        [ 24.287266  ,  24.150602  ,  23.712402  , ...,  22.409357  ,\n          22.16376   ,  21.763842  ],\n        [ 24.742495  ,  24.749857  ,  24.452438  , ...,  22.734667  ,\n          22.596312  ,  22.571692  ]],\n\n       [[-11.181002  , -11.5395    , -11.997769  , ..., -24.560495  ,\n         -23.613464  , -22.18285   ],\n        [ -5.9028273 ,  -6.072874  ,  -6.3179007 , ..., -26.00037   ,\n         -24.296288  , -21.393293  ],\n        [ -4.923568  ,  -6.1548376 ,  -6.819064  , ..., -25.649801  ,\n         -22.365923  , -17.643177  ],\n        ...,\n        [ 25.655682  ,  24.903734  ,  24.609102  , ...,  25.588436  ,\n          25.216568  ,  24.842041  ],\n        [ 26.060581  ,  25.872908  ,  25.394646  , ...,  25.701126  ,\n          25.779886  ,  25.570583  ],\n        [ 26.061464  ,  26.111534  ,  25.851294  , ...,  26.217041  ,\n          26.220764  ,  26.323593  ]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>seasonPandasIndex<pre>PandasIndex(Index(['DJF', 'JJA', 'MAM', 'SON'], dtype='object', name='season'))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> <p>The seasons are out of order (they are alphabetically sorted). This is a common annoyance. The solution is to use <code>.sel</code> to change the order of labels</p> In\u00a0[52]: Copied! <pre>seasonal_mean = seasonal_mean.sel(season=[\"DJF\", \"MAM\", \"JJA\", \"SON\"])\nseasonal_mean\n</pre> seasonal_mean = seasonal_mean.sel(season=[\"DJF\", \"MAM\", \"JJA\", \"SON\"]) seasonal_mean Out[52]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, season: 4, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * season   (season) object 'DJF' 'MAM' 'JJA' 'SON'\nData variables:\n    air      (season, lat, lon) float32 247.0 247.0 246.7 ... 299.4 299.4 299.5\n    air_C    (season, lat, lon) float32 -26.14 -26.19 -26.43 ... 26.22 26.32\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>season: 4</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>season(season)object'DJF' 'MAM' 'JJA' 'SON'<pre>array(['DJF', 'MAM', 'JJA', 'SON'], dtype=object)</pre></li></ul></li><li>Data variables: (2)<ul><li>air(season, lat, lon)float32247.0 247.0 246.7 ... 299.4 299.5long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray<pre>array([[[247.01007, 246.95503, 246.71684, ..., 241.55386, 242.69449,\n         244.23262],\n        [248.83022, 248.98196, 248.93813, ..., 240.31064, 242.24562,\n         245.32053],\n        [250.95328, 250.49544, 250.07674, ..., 241.26309, 244.65495,\n         249.44342],\n        ...,\n        [296.19055, 295.5512 , 295.43677, ..., 295.39368, 294.8794 ,\n         294.33405],\n        [296.939  , 296.83542, 296.46448, ..., 295.6107 , 295.42157,\n         294.959  ],\n        [297.4473 , 297.49   , 297.24048, ..., 296.29694, 296.13766,\n         296.05698]],\n\n       [[258.86465, 258.69296, 258.3957 , ..., 248.1146 , 249.09572,\n         250.37955],\n        [260.2769 , 260.37476, 260.3422 , ..., 247.43277, 249.08131,\n         251.56924],\n        [260.90564, 260.45697, 260.12427, ..., 248.48018, 251.28386,\n         255.22194],\n...\n        [298.82135, 297.98038, 297.3638 , ..., 297.59656, 297.0226 ,\n         296.5534 ],\n        [298.91702, 298.57843, 298.001  , ..., 297.41306, 297.43808,\n         297.17545],\n        [298.90414, 298.88306, 298.6041 , ..., 297.80478, 297.87   ,\n         297.96603]],\n\n       [[261.96924, 261.61057, 261.15253, ..., 248.58957, 249.53653,\n         250.96701],\n        [267.2477 , 267.07742, 266.83246, ..., 247.14966, 248.85371,\n         251.75676],\n        [268.2268 , 266.99548, 266.33118, ..., 247.50015, 250.78438,\n         255.50696],\n        ...,\n        [298.80624, 298.0542 , 297.75958, ..., 298.73898, 298.367  ,\n         297.99283],\n        [299.2114 , 299.02353, 298.54553, ..., 298.85175, 298.93045,\n         298.72104],\n        [299.21207, 299.26212, 299.0019 , ..., 299.36758, 299.37134,\n         299.47427]]], dtype=float32)</pre></li><li>air_C(season, lat, lon)float32-26.14 -26.19 ... 26.22 26.32<pre>array([[[-26.13999   , -26.194775  , -26.43317   , ..., -31.596172  ,\n         -30.455513  , -28.917418  ],\n        [-24.319826  , -24.168097  , -24.211771  , ..., -32.839237  ,\n         -30.904285  , -27.829481  ],\n        [-22.196844  , -22.654673  , -23.073437  , ..., -31.886757  ,\n         -28.495083  , -23.706408  ],\n        ...,\n        [ 23.039928  ,  22.400595  ,  22.286192  , ...,  22.242882  ,\n          21.72881   ,  21.183443  ],\n        [ 23.788368  ,  23.684774  ,  23.313797  , ...,  22.459927  ,\n          22.27082   ,  21.808601  ],\n        [ 24.29653   ,  24.339394  ,  24.089901  , ...,  23.14633   ,\n          22.987143  ,  22.906038  ]],\n\n       [[-14.286038  , -14.457757  , -14.755094  , ..., -25.035625  ,\n         -24.054403  , -22.770765  ],\n        [-12.873715  , -12.775794  , -12.80842   , ..., -25.717266  ,\n         -24.068861  , -21.581238  ],\n        [-12.245043  , -12.693659  , -13.026268  , ..., -24.670092  ,\n         -21.866636  , -17.9285    ],\n...\n        [ 25.670477  ,  24.829489  ,  24.212788  , ...,  24.445515  ,\n          23.871685  ,  23.40229   ],\n        [ 25.766272  ,  25.427404  ,  24.850374  , ...,  24.262072  ,\n          24.287355  ,  24.0246    ],\n        [ 25.753113  ,  25.732016  ,  25.453396  , ...,  24.653965  ,\n          24.719198  ,  24.815287  ]],\n\n       [[-11.181002  , -11.5395    , -11.997769  , ..., -24.560495  ,\n         -23.613464  , -22.18285   ],\n        [ -5.9028273 ,  -6.072874  ,  -6.3179007 , ..., -26.00037   ,\n         -24.296288  , -21.393293  ],\n        [ -4.923568  ,  -6.1548376 ,  -6.819064  , ..., -25.649801  ,\n         -22.365923  , -17.643177  ],\n        ...,\n        [ 25.655682  ,  24.903734  ,  24.609102  , ...,  25.588436  ,\n          25.216568  ,  24.842041  ],\n        [ 26.060581  ,  25.872908  ,  25.394646  , ...,  25.701126  ,\n          25.779886  ,  25.570583  ],\n        [ 26.061464  ,  26.111534  ,  25.851294  , ...,  26.217041  ,\n          26.220764  ,  26.323593  ]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>seasonPandasIndex<pre>PandasIndex(Index(['DJF', 'MAM', 'JJA', 'SON'], dtype='object', name='season'))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> In\u00a0[53]: Copied! <pre>seasonal_mean.air.plot(col=\"season\")\n</pre> seasonal_mean.air.plot(col=\"season\") Out[53]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x1950926a0&gt;</pre> In\u00a0[54]: Copied! <pre># Make the figure to 2x2 plots:\n\n# facet the seasonal_mean\nseasonal_mean.air.plot(col=\"season\", col_wrap=2);\n</pre> # Make the figure to 2x2 plots:  # facet the seasonal_mean seasonal_mean.air.plot(col=\"season\", col_wrap=2); In\u00a0[55]: Copied! <pre># Calculate zonal average\nseasonal_mean.air.mean(\"lon\").plot.line(hue=\"season\", y=\"lat\");\n</pre> # Calculate zonal average seasonal_mean.air.mean(\"lon\").plot.line(hue=\"season\", y=\"lat\"); In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[56]: Copied! <pre># resample to monthly frequency\nds.resample(time=\"M\").mean()\n</pre> # resample to monthly frequency ds.resample(time=\"M\").mean() Out[56]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 24, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-31 2013-02-28 ... 2014-12-31\nData variables:\n    air      (time, lat, lon) float32 244.5 244.7 244.7 ... 297.7 297.7 297.7\n    air_C    (time, lat, lon) float32 -28.68 -28.49 -28.48 ... 24.55 24.57 24.56\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>time: 24</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-31 ... 2014-12-31standard_name :timelong_name :Time<pre>array(['2013-01-31T00:00:00.000000000', '2013-02-28T00:00:00.000000000',\n       '2013-03-31T00:00:00.000000000', '2013-04-30T00:00:00.000000000',\n       '2013-05-31T00:00:00.000000000', '2013-06-30T00:00:00.000000000',\n       '2013-07-31T00:00:00.000000000', '2013-08-31T00:00:00.000000000',\n       '2013-09-30T00:00:00.000000000', '2013-10-31T00:00:00.000000000',\n       '2013-11-30T00:00:00.000000000', '2013-12-31T00:00:00.000000000',\n       '2014-01-31T00:00:00.000000000', '2014-02-28T00:00:00.000000000',\n       '2014-03-31T00:00:00.000000000', '2014-04-30T00:00:00.000000000',\n       '2014-05-31T00:00:00.000000000', '2014-06-30T00:00:00.000000000',\n       '2014-07-31T00:00:00.000000000', '2014-08-31T00:00:00.000000000',\n       '2014-09-30T00:00:00.000000000', '2014-10-31T00:00:00.000000000',\n       '2014-11-30T00:00:00.000000000', '2014-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (2)<ul><li>air(time, lat, lon)float32244.5 244.7 244.7 ... 297.7 297.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]new_attr :xarray<pre>array([[[244.4667 , 244.66354, 244.67027, ..., 242.49142, 243.40633,\n         244.67577],\n        [247.07323, 247.02248, 246.7275 , ..., 240.58205, 242.04489,\n         244.70726],\n        [250.37941, 249.83484, 249.10748, ..., 241.98434, 244.76712,\n         249.00505],\n        ...,\n        [295.83795, 295.15085, 294.9229 , ..., 295.36826, 294.88437,\n         294.26828],\n        [296.46942, 296.31686, 295.84802, ..., 295.5876 , 295.34058,\n         294.86536],\n        [297.05316, 297.0418 , 296.73517, ..., 296.30438, 296.09732,\n         296.0389 ]],\n\n       [[240.73384, 240.7013 , 240.4115 , ..., 241.60518, 242.71988,\n         243.94455],\n        [241.93309, 242.06935, 241.913  , ..., 241.01428, 242.32481,\n         244.72758],\n        [245.32361, 245.0261 , 244.36955, ..., 243.41588, 245.7661 ,\n         249.65858],\n...\n        [298.04895, 297.35007, 297.22195, ..., 298.01172, 297.66013,\n         297.14554],\n        [298.96484, 298.81186, 298.27136, ..., 298.10403, 298.22104,\n         297.88547],\n        [299.17334, 299.2175 , 298.89566, ..., 298.71625, 298.74167,\n         298.7802 ]],\n\n       [[246.80156, 246.88907, 246.76907, ..., 240.07089, 241.08206,\n         242.2817 ],\n        [247.72998, 248.30064, 248.74443, ..., 238.61859, 240.3222 ,\n         242.97026],\n        [249.96893, 249.58516, 249.57521, ..., 237.70308, 241.23743,\n         246.22667],\n        ...,\n        [296.4491 , 295.6914 , 295.75824, ..., 296.52817, 296.21747,\n         295.8128 ],\n        [297.44586, 297.43613, 297.1817 , ..., 296.95242, 297.05823,\n         296.72897],\n        [298.0472 , 298.22598, 298.0595 , ..., 297.6975 , 297.72318,\n         297.71024]]], dtype=float32)</pre></li><li>air_C(time, lat, lon)float32-28.68 -28.49 ... 24.57 24.56<pre>array([[[-28.68323  , -28.486452 , -28.479755 , ..., -30.658554 ,\n         -29.743628 , -28.474194 ],\n        [-26.076784 , -26.127504 , -26.4225   , ..., -32.5679   ,\n         -31.105167 , -28.442825 ],\n        [-22.770565 , -23.31516  , -24.042498 , ..., -31.165657 ,\n         -28.38291  , -24.144924 ],\n        ...,\n        [ 22.688152 ,  22.00097  ,  21.773153 , ...,  22.218397 ,\n          21.734531 ,  21.118395 ],\n        [ 23.31952  ,  23.16702  ,  22.698233 , ...,  22.43775  ,\n          22.190727 ,  21.715578 ],\n        [ 23.903486 ,  23.89203  ,  23.585333 , ...,  23.154608 ,\n          22.947426 ,  22.889124 ]],\n\n       [[-32.41607  , -32.44866  , -32.738483 , ..., -31.54482  ,\n         -30.430185 , -29.205448 ],\n        [-31.216885 , -31.08063  , -31.236965 , ..., -32.135708 ,\n         -30.825186 , -28.42241  ],\n        [-27.826433 , -28.123934 , -28.78045  , ..., -29.734114 ,\n         -27.383936 , -23.491434 ],\n...\n        [ 24.899088 ,  24.200085 ,  24.072004 , ...,  24.861843 ,\n          24.510258 ,  23.995668 ],\n        [ 25.815008 ,  25.661922 ,  25.121607 , ...,  24.954088 ,\n          25.071083 ,  24.735588 ],\n        [ 26.023424 ,  26.06767  ,  25.74576  , ...,  25.566338 ,\n          25.591848 ,  25.630259 ]],\n\n       [[-26.348473 , -26.260897 , -26.380894 , ..., -33.07903  ,\n         -32.067986 , -30.868315 ],\n        [-25.419994 , -24.849277 , -24.405483 , ..., -34.531376 ,\n         -32.82783  , -30.179682 ],\n        [-23.181051 , -23.56476  , -23.574757 , ..., -35.446938 ,\n         -31.91259  , -26.923311 ],\n        ...,\n        [ 23.299198 ,  22.541454 ,  22.60839  , ...,  23.378307 ,\n          23.067505 ,  22.662996 ],\n        [ 24.295895 ,  24.286139 ,  24.031782 , ...,  23.80259  ,\n          23.908312 ,  23.579037 ],\n        [ 24.897346 ,  25.076134 ,  24.909689 , ...,  24.547583 ,\n          24.573233 ,  24.560413 ]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-31', '2013-02-28', '2013-03-31', '2013-04-30',\n               '2013-05-31', '2013-06-30', '2013-07-31', '2013-08-31',\n               '2013-09-30', '2013-10-31', '2013-11-30', '2013-12-31',\n               '2014-01-31', '2014-02-28', '2014-03-31', '2014-04-30',\n               '2014-05-31', '2014-06-30', '2014-07-31', '2014-08-31',\n               '2014-09-30', '2014-10-31', '2014-11-30', '2014-12-31'],\n              dtype='datetime64[ns]', name='time', freq='M'))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> In\u00a0[57]: Copied! <pre>ds.resample(time=\"M\").mean().mean(dim =['lat','lon']).air.plot()\n</pre> ds.resample(time=\"M\").mean().mean(dim =['lat','lon']).air.plot() Out[57]: <pre>[&lt;matplotlib.lines.Line2D at 0x1954c0730&gt;]</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Lecture_8_Xarray/#lecture-8-xarray-for-multidimensional-gridded-data","title":"Lecture 8: Xarray for multidimensional gridded data\u00b6","text":"<p>In last week's lecture, we saw how Pandas provided a way to keep track of additional \"metadata\" surrounding tabular datasets, including \"indexes\" for each row and labels for each column. These features, together with Pandas' many useful routines for all kinds of data munging and analysis, have made Pandas one of the most popular python packages in the world.</p> <p>However, not all earth and environmental science datasets easily fit into the \"tabular\" model (i.e. rows and columns) imposed by Pandas. In particular, we often deal with multidimensional data. By multidimensional data (also often called N-dimensional), I mean data with many independent dimensions or axes. For example, we might represent Earth's surface temperature $T$ as a three dimensional variable</p> <p>$$ T(x, y, t) $$</p> <p>where $x$ is longitude, $y$ is latitude, and $t$ is time.</p> <p>The point of xarray is to provide pandas-level convenience for working with this type of data.</p>"},{"location":"Lecture_8_Xarray/#xarray-data-model","title":"Xarray Data Model\u00b6","text":""},{"location":"Lecture_8_Xarray/#xarray-functionality","title":"Xarray functionality\u00b6","text":""},{"location":"Lecture_8_Xarray/#xarray-data-structures","title":"Xarray data structures\u00b6","text":"<p>Like Pandas, xarray has two fundamental data structures:</p> <ul> <li>a <code>DataArray</code>, which holds a single multi-dimensional variable and its coordinates</li> <li>a <code>Dataset</code>, which holds multiple variables that potentially share the same coordinates</li> </ul>"},{"location":"Lecture_8_Xarray/#dataarray","title":"DataArray\u00b6","text":"<p>A <code>DataArray</code> has four essential attributes:</p> <ul> <li><code>values</code>: a <code>numpy.ndarray</code> holding the array\u2019s values</li> <li><code>dims</code>: dimension names for each axis (e.g., <code>('x', 'y', 'z')</code>)</li> <li><code>coords</code>: a dict-like container of arrays (coordinates) that label each point (e.g., 1-dimensional arrays of numbers, datetime objects or strings)</li> <li><code>attrs</code>: an <code>OrderedDict</code> to hold arbitrary metadata (attributes)</li> </ul> <p>Let's start by constructing some DataArrays manually</p>"},{"location":"Lecture_8_Xarray/#install-xarray","title":"Install Xarray:\u00b6","text":"<p><code>$ conda install -c conda-forge xarray dask netCDF4 bottleneck python-graphviz </code></p>"},{"location":"Lecture_8_Xarray/#xarray-data-structure","title":"Xarray Data Structure\u00b6","text":"<p>Xarray has a few small real-world tutorial datasets hosted in this GitHub repository https://github.com/pydata/xarray-data. We'll use the xarray.tutorial.load_dataset convenience function to download and open the <code>air_temperature</code> (National Centers for Environmental Prediction) Dataset by name.</p>"},{"location":"Lecture_8_Xarray/#what-is-included-in-xarray-dataset","title":"What is included in xarray <code>Dataset</code>?\u00b6","text":"<p>The output consists of:</p> <ul> <li>a summary of all dimensions of the <code>Dataset</code> <code>(lat: 25, time: 2920, lon: 53)</code>: this tells us that the first dimension is named <code>lat</code> and has a size of <code>25</code>, the second dimension is named <code>time</code> and has a size of <code>2920</code>, and the third dimension is named <code>lon</code> and has a size of <code>53</code>. Because we will access the dimensions by name, the order doesn't matter.</li> <li>an unordered list of coordinates or dimensions with coordinates with one item per line. Each item has a name, one or more dimensions in parentheses, a dtype and a preview of the values.</li> <li>an alphabetically sorted list of dimensions without coordinates (if there are any)</li> <li>an unordered list of attributes, or metadata</li> </ul>"},{"location":"Lecture_8_Xarray/#dataarray","title":"DataArray\u00b6","text":"<p>The <code>DataArray</code> class consists of an array (data) and its associated dimension names, labels, and attributes (metadata).</p>"},{"location":"Lecture_8_Xarray/#named-dimensions","title":"Named dimensions\u00b6","text":"<p><code>.dims</code> are the named axes of your data. They may (dimension coordinates) or may not (dimensions without coordinates) have associated values.</p> <p>In this case we have 2 spatial dimensions (<code>latitude</code> and <code>longitude</code> are stored with shorthand names <code>lat</code> and <code>lon</code>) and one temporal dimension (<code>time</code>).</p>"},{"location":"Lecture_8_Xarray/#coordinates","title":"Coordinates\u00b6","text":"<p><code>.coords</code> is a simple dict-like data container for mapping coordinate names to values.</p> <p>Here we see the actual timestamps and spatial positions of our air temperature data:</p>"},{"location":"Lecture_8_Xarray/#attributes","title":"Attributes\u00b6","text":"<p><code>.attrs</code> is a dictionary that can contain arbitrary Python objects (strings, lists, integers, dictionaries, etc.) containing information about your data. Your only limitation is that some attributes may not be writeable to certain file formats.</p>"},{"location":"Lecture_8_Xarray/#working-with-labelled-data","title":"Working with Labelled Data\u00b6","text":"<p>Xarray's labels make working with multidimensional data much easier. Metadata provides context and provides code that is more legible. This reduces the likelihood of errors from typos and makes analysis more intuitive and fun!</p>"},{"location":"Lecture_8_Xarray/#selecting-data-indexing","title":"Selecting Data (Indexing)\u00b6","text":"<p>We can always use regular numpy indexing and slicing on DataArrays</p>"},{"location":"Lecture_8_Xarray/#position-based-indexing","title":"Position-based Indexing\u00b6","text":"<p>Indexing a <code>DataArray</code> directly works (mostly) just like it does for numpy <code>ndarrays</code>, except that the returned object is always another <code>DataArray</code>:</p> <p>This approach however does not take advantage of the dimension names and coordinate location information that is present in a Xarray object.</p>"},{"location":"Lecture_8_Xarray/#positional-indexing-using-dimension-names","title":"Positional Indexing Using Dimension Names\u00b6","text":""},{"location":"Lecture_8_Xarray/#label-based-indexing","title":"Label-based Indexing\u00b6","text":"<p>To select data by coordinate labels instead of integer indices we can use <code>sel</code> instead of <code>isel</code>:</p>"},{"location":"Lecture_8_Xarray/#dropping-using-drop_sel","title":"Dropping using <code>drop_sel</code>\u00b6","text":"<p>If instead of selecting data we want to drop it, we can use <code>drop_sel</code> method with syntax similar to <code>sel</code>:</p>"},{"location":"Lecture_8_Xarray/#nearest-neighbor-lookups","title":"Nearest Neighbor Lookups\u00b6","text":"<p>The label based selection methods <code>sel()</code> support <code>method</code> and <code>tolerance</code> keyword argument. The <code>method</code> parameter allows for enabling nearest neighbor (inexact) lookups by use of the methods <code>ffill</code> (propagate last valid index forward), <code>backfill</code> or <code>nearest</code>:</p>"},{"location":"Lecture_8_Xarray/#datetime-indexing","title":"Datetime Indexing\u00b6","text":"<p>Datetime indexing is a critical feature when working with time series data, which is a common occurrence in atmospheric and environmental sciences. Essentially, datetime indexing allows you to select data points or a series of data points that correspond to certain date or time criteria. This becomes essential for time-series analysis where the date or time information associated with each data point can be as critical as the data point itself.</p> <p>Let's see some of the techniques to perform datetime indexing in Xarray:</p>"},{"location":"Lecture_8_Xarray/#selecting-data-based-on-single-datetime","title":"Selecting data based on single datetime\u00b6","text":"<p>Let's say we have a Dataset ds and we want to select data at a particular date and time, for instance, '2013-01-01' at 6AM. We can do this by using the <code>sel</code> (select) method, like so:</p>"},{"location":"Lecture_8_Xarray/#selecting-data-for-a-range-of-dates","title":"Selecting data for a range of dates\u00b6","text":"<p>Now, let's say we want to select data between a certain range of dates. We can still use the <code>sel</code> method, but this time we will combine it with slice:</p>"},{"location":"Lecture_8_Xarray/#indexing-with-a-datetimeindex-or-date-string-list","title":"Indexing with a DatetimeIndex or date string list\u00b6","text":"<p>Another technique is to use a list of datetime objects or date strings for indexing. For example, you could select data for specific, non-contiguous dates like this:</p>"},{"location":"Lecture_8_Xarray/#fancy-indexing-based-on-year-month-day-or-other-datetime-components","title":"Fancy indexing based on year, month, day, or other datetime components\u00b6","text":"<p>In addition to the basic datetime indexing techniques, Xarray also supports \"fancy\" indexing options, which can provide more flexibility and efficiency in your data analysis tasks. You can directly access datetime components such as year, month, day, hour, etc. using the <code>.dt</code> accessor. Here is an example of selecting all data points from July across all years:</p>"},{"location":"Lecture_8_Xarray/#xarray-computation","title":"Xarray Computation\u00b6","text":""},{"location":"Lecture_8_Xarray/#broadcasting-expanding-data","title":"Broadcasting: expanding data\u00b6","text":""},{"location":"Lecture_8_Xarray/#xarray-groupby-and-resample","title":"Xarray Groupby and Resample\u00b6","text":""},{"location":"Lecture_8_Xarray/#groupby","title":"groupby\u00b6","text":""},{"location":"Lecture_8_Xarray/#resample","title":"resample\u00b6","text":""},{"location":"Lecture_9_Dask_Parallel_Computing/","title":"Lecture 9 Dask for Parellel Computing and Big Data","text":"<p>In past lectures, we learned how to use numpy, pandas, and xarray to analyze various types of geoscience data. In this lecture, we address an incresingly common problem: what happens if the data we wish to analyze is \"big data\"</p> In\u00a0[14]: Copied! <pre>import numpy as np\nshape = (1000, 4000)\nones_np = np.ones(shape)\nones_np\n</pre> import numpy as np shape = (1000, 4000) ones_np = np.ones(shape) ones_np Out[14]: <pre>array([[1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       ...,\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.]])</pre> <p>This array contains exactly 32 MB of data:</p> In\u00a0[2]: Copied! <pre>ones_np.nbytes / 1e6\n</pre> ones_np.nbytes / 1e6 Out[2]: <pre>32.0</pre> <p>Now let's create the same array using dask's array interface.</p> In\u00a0[3]: Copied! <pre>import dask\nimport dask.array as da\nones = da.ones(shape)\n</pre> import dask import dask.array as da ones = da.ones(shape) In\u00a0[4]: Copied! <pre>ones\n</pre> ones Out[4]:  Array   Chunk   Bytes   30.52 MiB   30.52 MiB   Shape   (1000, 4000)   (1000, 4000)   Dask graph   1 chunks in 1 graph layer   Data type   float64 numpy.ndarray  4000 1000 In\u00a0[5]: Copied! <pre>ones = da.ones((100,1000,1000))\n</pre> ones = da.ones((100,1000,1000)) In\u00a0[6]: Copied! <pre># Dask will determine an optimal chunk size. \nones\n</pre> # Dask will determine an optimal chunk size.  ones Out[6]:  Array   Chunk   Bytes   762.94 MiB   127.63 MiB   Shape   (100, 1000, 1000)   (100, 409, 409)   Dask graph   9 chunks in 1 graph layer   Data type   float64 numpy.ndarray  1000 1000 100 <p>By default, dask reads the entire array as a single chuck. If the data size is too big, dask will split the dataset to different chunks.  \"Chunks\" describes how the array is split up over many sub-arrays.</p> <p>There are several ways to specify chunks. In this lecture, we will use a block shape.</p> In\u00a0[7]: Copied! <pre>chunk_shape = (1000, 1000)\nshape = (1000, 4000)\nones = da.ones(shape, chunks=chunk_shape)\nones\n</pre> chunk_shape = (1000, 1000) shape = (1000, 4000) ones = da.ones(shape, chunks=chunk_shape) ones Out[7]:  Array   Chunk   Bytes   30.52 MiB   7.63 MiB   Shape   (1000, 4000)   (1000, 1000)   Dask graph   4 chunks in 1 graph layer   Data type   float64 numpy.ndarray  4000 1000 In\u00a0[8]: Copied! <pre>ones.chunksize\n</pre> ones.chunksize Out[8]: <pre>(1000, 1000)</pre> <p>Notice that we just see a symbolic represetnation of the array, including its shape, dtype, and chunksize. No data has been generated yet. When we call <code>.compute()</code> on a dask array, the computation is trigger and the dask array becomes a numpy array.</p> In\u00a0[9]: Copied! <pre>ones.compute()\n</pre> ones.compute() Out[9]: <pre>array([[1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       ...,\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.]])</pre> <p>In order to understand what happened when we called <code>.compute()</code>, we can visualize the dask graph, the symbolic operations that make up the array</p> In\u00a0[19]: Copied! <pre># Install graphviz: mamba install -c conda-forge python-graphviz graphviz\n</pre> # Install graphviz: mamba install -c conda-forge python-graphviz graphviz In\u00a0[18]: Copied! <pre>ones.visualize()\n</pre> ones.visualize() Out[18]: <p>Our array has four chunks. To generate it, dask calls <code>np.ones</code> four times and then concatenates this together into one array.</p> <p>Rather than immediately loading a dask array (which puts all the data into RAM), it is more common to want to reduce the data somehow. For example</p> In\u00a0[20]: Copied! <pre>sum_of_ones = ones.sum()\nsum_of_ones.visualize()\n</pre> sum_of_ones = ones.sum() sum_of_ones.visualize() Out[20]: <p>Here we see dask's strategy for finding the sum. This simple example illustrates the beauty of dask: it automatically designs an algorithm appropriate for custom operations with big data.</p> <p>If we make our operation more complex, the graph gets more complex.</p> In\u00a0[21]: Copied! <pre>fancy_calculation = (ones * ones[::-1, ::-1]).mean()\nfancy_calculation.visualize()\n</pre> fancy_calculation = (ones * ones[::-1, ::-1]).mean() fancy_calculation.visualize() Out[21]: In\u00a0[22]: Copied! <pre>bigshape = (200000, 4000)\nbig_ones = da.ones(bigshape, chunks=chunk_shape)\nbig_ones\n</pre> bigshape = (200000, 4000) big_ones = da.ones(bigshape, chunks=chunk_shape) big_ones Out[22]:  Array   Chunk   Bytes   5.96 GiB   7.63 MiB   Shape   (200000, 4000)   (1000, 1000)   Dask graph   800 chunks in 1 graph layer   Data type   float64 numpy.ndarray  4000 200000 In\u00a0[23]: Copied! <pre>big_ones.nbytes / 1e6\n</pre> big_ones.nbytes / 1e6 Out[23]: <pre>6400.0</pre> <p>This dataset is 6.4 GB, rather MB! This is probably close to or greater than the amount of available RAM than you have in your computer. Nevertheless, dask has no problem working on it.</p> <p>Do not try to <code>.visualize()</code> this array!</p> <p>When doing a big calculation, dask also has some tools to help us understand what is happening under the hood</p> In\u00a0[24]: Copied! <pre>from dask.diagnostics import ProgressBar\n\nbig_calc = (big_ones * big_ones[::-1, ::-1]).mean()\n\nwith ProgressBar():\n    result = big_calc.compute()\nresult\n</pre> from dask.diagnostics import ProgressBar  big_calc = (big_ones * big_ones[::-1, ::-1]).mean()  with ProgressBar():     result = big_calc.compute() result <pre>[########################################] | 100% Completed | 2.38 sms\n</pre> Out[24]: <pre>1.0</pre> In\u00a0[25]: Copied! <pre>big_ones_reduce = (np.cos(big_ones)**2).mean(axis=0)\nbig_ones_reduce\n</pre> big_ones_reduce = (np.cos(big_ones)**2).mean(axis=0) big_ones_reduce Out[25]:  Array   Chunk   Bytes   31.25 kiB   7.81 kiB   Shape   (4000,)   (1000,)   Dask graph   4 chunks in 8 graph layers   Data type   float64 numpy.ndarray  4000 1 <p>Plotting also triggers computation, since we need the actual values</p> In\u00a0[26]: Copied! <pre>from matplotlib import pyplot as plt\n</pre> from matplotlib import pyplot as plt In\u00a0[27]: Copied! <pre>plt.plot(big_ones_reduce)\n</pre> plt.plot(big_ones_reduce) Out[27]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f50b7bfa820&gt;]</pre> In\u00a0[1]: Copied! <pre>import numpy as np\nimport xarray as xr\n</pre> import numpy as np import xarray as xr  In\u00a0[30]: Copied! <pre>ds = xr.tutorial.open_dataset(\"air_temperature\")\n</pre> ds = xr.tutorial.open_dataset(\"air_temperature\") In\u00a0[31]: Copied! <pre>ds = xr.tutorial.open_dataset(\n    \"air_temperature\",\n    chunks={  # this tells xarray to open the dataset as a dask array\n        \"lat\": \"auto\",\n        \"lon\": \"auto\",\n        \"time\": 1000,\n    },\n)\nds\n</pre> ds = xr.tutorial.open_dataset(     \"air_temperature\",     chunks={  # this tells xarray to open the dataset as a dask array         \"lat\": \"auto\",         \"lon\": \"auto\",         \"time\": 1000,     }, ) ds Out[31]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 dask.array&lt;chunksize=(1000, 25, 53), meta=np.ndarray&gt;\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>time: 2920</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air(time, lat, lon)float32dask.array&lt;chunksize=(1000, 25, 53), meta=np.ndarray&gt;long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]  Array   Chunk   Bytes   14.76 MiB   5.05 MiB   Shape   (2920, 25, 53)   (1000, 25, 53)   Dask graph   3 chunks in 2 graph layers   Data type   float32 numpy.ndarray  53 25 2920 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> In\u00a0[32]: Copied! <pre>ds.air.data  # dask array, not numpy\n</pre> ds.air.data  # dask array, not numpy Out[32]:  Array   Chunk   Bytes   14.76 MiB   5.05 MiB   Shape   (2920, 25, 53)   (1000, 25, 53)   Dask graph   3 chunks in 2 graph layers   Data type   float32 numpy.ndarray  53 25 2920 In\u00a0[33]: Copied! <pre>ds.air.as_numpy().data  ## numpy array\n</pre> ds.air.as_numpy().data  ## numpy array Out[33]: <pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n        ...,\n        [296.4    , 295.9    , 296.19998, ..., 295.4    , 295.1    ,\n         294.79   ],\n        [296.19998, 296.69998, 296.79   , ..., 295.6    , 295.5    ,\n         295.1    ],\n        [296.29   , 297.19998, 297.4    , ..., 296.4    , 296.4    ,\n         296.6    ]],\n\n       [[242.29999, 242.2    , 242.29999, ..., 234.29999, 236.09999,\n         238.7    ],\n        [244.59999, 244.39   , 244.     , ..., 230.29999, 232.     ,\n         235.7    ],\n        [256.19998, 255.5    , 254.2    , ..., 231.2    , 233.2    ,\n         238.2    ],\n        ...,\n        [295.6    , 295.4    , 295.4    , ..., 296.29   , 295.29   ,\n         295.     ],\n        [296.19998, 296.5    , 296.29   , ..., 296.4    , 296.     ,\n         295.6    ],\n        [296.4    , 296.29   , 296.4    , ..., 297.     , 297.     ,\n         296.79   ]],\n\n       ...,\n\n       [[243.48999, 242.98999, 242.09   , ..., 244.18999, 244.48999,\n         244.89   ],\n        [249.09   , 248.98999, 248.59   , ..., 240.59   , 241.29   ,\n         242.68999],\n        [262.69   , 262.19   , 261.69   , ..., 239.39   , 241.68999,\n         245.18999],\n        ...,\n        [294.79   , 295.29   , 297.49   , ..., 295.49   , 295.38998,\n         294.69   ],\n        [296.79   , 297.88998, 298.29   , ..., 295.49   , 295.49   ,\n         294.79   ],\n        [298.19   , 299.19   , 298.79   , ..., 296.09   , 295.79   ,\n         295.79   ]],\n\n       [[245.79   , 244.79   , 243.48999, ..., 243.29   , 243.98999,\n         244.79   ],\n        [249.89   , 249.29   , 248.48999, ..., 241.29   , 242.48999,\n         244.29   ],\n        [262.38998, 261.79   , 261.29   , ..., 240.48999, 243.09   ,\n         246.89   ],\n        ...,\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre> <pre><code>{exercise}\n:label: data-values\nTry calling `ds.air.values` and `ds.air.data`. Do you understand the difference?\n</code></pre> In\u00a0[34]: Copied! <pre>ds.air.to_numpy()\n</pre> ds.air.to_numpy() Out[34]: <pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n        ...,\n        [296.4    , 295.9    , 296.19998, ..., 295.4    , 295.1    ,\n         294.79   ],\n        [296.19998, 296.69998, 296.79   , ..., 295.6    , 295.5    ,\n         295.1    ],\n        [296.29   , 297.19998, 297.4    , ..., 296.4    , 296.4    ,\n         296.6    ]],\n\n       [[242.29999, 242.2    , 242.29999, ..., 234.29999, 236.09999,\n         238.7    ],\n        [244.59999, 244.39   , 244.     , ..., 230.29999, 232.     ,\n         235.7    ],\n        [256.19998, 255.5    , 254.2    , ..., 231.2    , 233.2    ,\n         238.2    ],\n        ...,\n        [295.6    , 295.4    , 295.4    , ..., 296.29   , 295.29   ,\n         295.     ],\n        [296.19998, 296.5    , 296.29   , ..., 296.4    , 296.     ,\n         295.6    ],\n        [296.4    , 296.29   , 296.4    , ..., 297.     , 297.     ,\n         296.79   ]],\n\n       ...,\n\n       [[243.48999, 242.98999, 242.09   , ..., 244.18999, 244.48999,\n         244.89   ],\n        [249.09   , 248.98999, 248.59   , ..., 240.59   , 241.29   ,\n         242.68999],\n        [262.69   , 262.19   , 261.69   , ..., 239.39   , 241.68999,\n         245.18999],\n        ...,\n        [294.79   , 295.29   , 297.49   , ..., 295.49   , 295.38998,\n         294.69   ],\n        [296.79   , 297.88998, 298.29   , ..., 295.49   , 295.49   ,\n         294.79   ],\n        [298.19   , 299.19   , 298.79   , ..., 296.09   , 295.79   ,\n         295.79   ]],\n\n       [[245.79   , 244.79   , 243.48999, ..., 243.29   , 243.98999,\n         244.79   ],\n        [249.89   , 249.29   , 248.48999, ..., 241.29   , 242.48999,\n         244.29   ],\n        [262.38998, 261.79   , 261.29   , ..., 240.48999, 243.09   ,\n         246.89   ],\n        ...,\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre> <p></p> In\u00a0[35]: Copied! <pre>mean = ds.air.mean(\"time\")\nmean\n</pre> mean = ds.air.mean(\"time\") mean Out[35]: <pre>&lt;xarray.DataArray 'air' (lat: 25, lon: 53)&gt;\ndask.array&lt;mean_agg-aggregate, shape=(25, 53), dtype=float32, chunksize=(25, 53), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0</pre>xarray.DataArray'air'<ul><li>lat: 25</li><li>lon: 53</li></ul><ul><li>dask.array&lt;chunksize=(25, 53), meta=np.ndarray&gt;  Array   Chunk   Bytes   5.18 kiB   5.18 kiB   Shape   (25, 53)   (25, 53)   Dask graph   1 chunks in 4 graph layers   Data type   float32 numpy.ndarray  53 25 </li><li>Coordinates: (2)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li></ul></li><li>Indexes: (2)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li></ul></li><li>Attributes: (0)</li></ul> <p>Dask actually constructs a graph of the required computation. Here it's pretty simple: The full array is subdivided into 3 arrays. Dask will load each of these subarrays in a separate thread using the default single-machine scheduling. You can visualize dask 'task graphs' which represent the requested computation:</p> In\u00a0[36]: Copied! <pre>mean.data  # dask array\n</pre> mean.data  # dask array Out[36]:  Array   Chunk   Bytes   5.18 kiB   5.18 kiB   Shape   (25, 53)   (25, 53)   Dask graph   1 chunks in 4 graph layers   Data type   float32 numpy.ndarray  53 25 In\u00a0[37]: Copied! <pre># visualize the graph for the underlying dask array\n# we ask it to visualize the graph from left to right because it looks nicer\ndask.visualize(mean.data, rankdir=\"LR\", optimize_graph =True)\n</pre> # visualize the graph for the underlying dask array # we ask it to visualize the graph from left to right because it looks nicer dask.visualize(mean.data, rankdir=\"LR\", optimize_graph =True) Out[37]: In\u00a0[38]: Copied! <pre>ds.load()\n</pre> ds.load() Out[38]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 25</li><li>time: 2920</li><li>lon: 53</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)</pre></li><li>lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)</pre></li><li>time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Time<pre>array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]<pre>array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([75.0, 72.5, 70.0, 67.5, 65.0, 62.5, 60.0, 57.5, 55.0, 52.5, 50.0, 47.5,\n       45.0, 42.5, 40.0, 37.5, 35.0, 32.5, 30.0, 27.5, 25.0, 22.5, 20.0, 17.5,\n       15.0],\n      dtype='float32', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5,\n       225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5,\n       250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5,\n       275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5,\n       300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5,\n       325.0, 327.5, 330.0],\n      dtype='float32', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 06:00:00',\n               '2013-01-01 12:00:00', '2013-01-01 18:00:00',\n               '2013-01-02 00:00:00', '2013-01-02 06:00:00',\n               '2013-01-02 12:00:00', '2013-01-02 18:00:00',\n               '2013-01-03 00:00:00', '2013-01-03 06:00:00',\n               ...\n               '2014-12-29 12:00:00', '2014-12-29 18:00:00',\n               '2014-12-30 00:00:00', '2014-12-30 06:00:00',\n               '2014-12-30 12:00:00', '2014-12-30 18:00:00',\n               '2014-12-31 00:00:00', '2014-12-31 06:00:00',\n               '2014-12-31 12:00:00', '2014-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=2920, freq=None))</pre></li></ul></li><li>Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis (4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</li></ul> In\u00a0[2]: Copied! <pre>ds_multiple = xr.open_mfdataset('/scratch/xj103/rcaes/GEOS_Chem_Sims/GEOSChem.SpeciesConcHourlyNO2.2020*_2100z.nc4')\n</pre> ds_multiple = xr.open_mfdataset('/scratch/xj103/rcaes/GEOS_Chem_Sims/GEOSChem.SpeciesConcHourlyNO2.2020*_2100z.nc4')   In\u00a0[3]: Copied! <pre>ds_multiple\n</pre> ds_multiple Out[3]: <pre>&lt;xarray.Dataset&gt;\nDimensions:          (time: 30, lev: 47, lat: 81, lon: 65)\nCoordinates:\n  * time             (time) datetime64[ns] 2020-09-01T21:30:00 ... 2020-09-30...\n  * lev              (lev) float64 0.9925 0.9775 0.9625 ... 0.0001387 3.8e-05\n  * lat              (lat) float64 27.0 27.25 27.5 27.75 ... 46.5 46.75 47.0\n  * lon              (lon) float64 -130.0 -129.7 -129.4 ... -110.6 -110.3 -110.0\nData variables:\n    SpeciesConc_NO2  (time, lev, lat, lon) float32 dask.array&lt;chunksize=(1, 47, 81, 65), meta=np.ndarray&gt;</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 30</li><li>lev: 47</li><li>lat: 81</li><li>lon: 65</li></ul></li><li>Coordinates: (4)<ul><li>time(time)datetime64[ns]2020-09-01T21:30:00 ... 2020-09-...long_name :Timeaxis :T<pre>array(['2020-09-01T21:30:00.000000000', '2020-09-02T21:30:00.000000000',\n       '2020-09-03T21:30:00.000000000', '2020-09-04T21:30:00.000000000',\n       '2020-09-05T21:30:00.000000000', '2020-09-06T21:30:00.000000000',\n       '2020-09-07T21:30:00.000000000', '2020-09-08T21:30:00.000000000',\n       '2020-09-09T21:30:00.000000000', '2020-09-10T21:30:00.000000000',\n       '2020-09-11T21:30:00.000000000', '2020-09-12T21:30:00.000000000',\n       '2020-09-13T21:30:00.000000000', '2020-09-14T21:30:00.000000000',\n       '2020-09-15T21:30:00.000000000', '2020-09-16T21:30:00.000000000',\n       '2020-09-17T21:30:00.000000000', '2020-09-18T21:30:00.000000000',\n       '2020-09-19T21:30:00.000000000', '2020-09-20T21:30:00.000000000',\n       '2020-09-21T21:30:00.000000000', '2020-09-22T21:30:00.000000000',\n       '2020-09-23T21:30:00.000000000', '2020-09-24T21:30:00.000000000',\n       '2020-09-25T21:30:00.000000000', '2020-09-26T21:30:00.000000000',\n       '2020-09-27T21:30:00.000000000', '2020-09-28T21:30:00.000000000',\n       '2020-09-29T21:30:00.000000000', '2020-09-30T21:30:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>lev(lev)float640.9925 0.9775 ... 0.0001387 3.8e-05long_name :hybrid level at midpoints ((A/P0)+B)units :levelaxis :Zpositive :upstandard_name :atmosphere_hybrid_sigma_pressure_coordinateformula_terms :a: hyam b: hybm p0: P0 ps: PS<pre>array([9.925000e-01, 9.774999e-01, 9.624998e-01, 9.475000e-01, 9.325001e-01,\n       9.174999e-01, 9.024999e-01, 8.875000e-01, 8.725000e-01, 8.575001e-01,\n       8.425001e-01, 8.275002e-01, 8.100002e-01, 7.875000e-01, 7.625000e-01,\n       7.375001e-01, 7.125001e-01, 6.875001e-01, 6.562501e-01, 6.187502e-01,\n       5.812502e-01, 5.437501e-01, 5.062501e-01, 4.687501e-01, 4.312501e-01,\n       3.937501e-01, 3.562501e-01, 3.127916e-01, 2.664790e-01, 2.265135e-01,\n       1.925410e-01, 1.636615e-01, 1.391150e-01, 1.182500e-01, 1.005144e-01,\n       8.543901e-02, 6.745011e-02, 4.828166e-02, 3.427161e-02, 2.407970e-02,\n       1.454227e-02, 6.684757e-03, 2.863681e-03, 1.133785e-03, 4.140641e-04,\n       1.386745e-04, 3.800000e-05])</pre></li><li>lat(lat)float6427.0 27.25 27.5 ... 46.5 46.75 47.0long_name :Latitudeunits :degrees_northaxis :Y<pre>array([27.  , 27.25, 27.5 , 27.75, 28.  , 28.25, 28.5 , 28.75, 29.  , 29.25,\n       29.5 , 29.75, 30.  , 30.25, 30.5 , 30.75, 31.  , 31.25, 31.5 , 31.75,\n       32.  , 32.25, 32.5 , 32.75, 33.  , 33.25, 33.5 , 33.75, 34.  , 34.25,\n       34.5 , 34.75, 35.  , 35.25, 35.5 , 35.75, 36.  , 36.25, 36.5 , 36.75,\n       37.  , 37.25, 37.5 , 37.75, 38.  , 38.25, 38.5 , 38.75, 39.  , 39.25,\n       39.5 , 39.75, 40.  , 40.25, 40.5 , 40.75, 41.  , 41.25, 41.5 , 41.75,\n       42.  , 42.25, 42.5 , 42.75, 43.  , 43.25, 43.5 , 43.75, 44.  , 44.25,\n       44.5 , 44.75, 45.  , 45.25, 45.5 , 45.75, 46.  , 46.25, 46.5 , 46.75,\n       47.  ])</pre></li><li>lon(lon)float64-130.0 -129.7 ... -110.3 -110.0long_name :Longitudeunits :degrees_eastaxis :X<pre>array([-130.    , -129.6875, -129.375 , -129.0625, -128.75  , -128.4375,\n       -128.125 , -127.8125, -127.5   , -127.1875, -126.875 , -126.5625,\n       -126.25  , -125.9375, -125.625 , -125.3125, -125.    , -124.6875,\n       -124.375 , -124.0625, -123.75  , -123.4375, -123.125 , -122.8125,\n       -122.5   , -122.1875, -121.875 , -121.5625, -121.25  , -120.9375,\n       -120.625 , -120.3125, -120.    , -119.6875, -119.375 , -119.0625,\n       -118.75  , -118.4375, -118.125 , -117.8125, -117.5   , -117.1875,\n       -116.875 , -116.5625, -116.25  , -115.9375, -115.625 , -115.3125,\n       -115.    , -114.6875, -114.375 , -114.0625, -113.75  , -113.4375,\n       -113.125 , -112.8125, -112.5   , -112.1875, -111.875 , -111.5625,\n       -111.25  , -110.9375, -110.625 , -110.3125, -110.    ])</pre></li></ul></li><li>Data variables: (1)<ul><li>SpeciesConc_NO2(time, lev, lat, lon)float32dask.array&lt;chunksize=(1, 47, 81, 65), meta=np.ndarray&gt;long_name :Dry mixing ratio of species NO2units :mol mol-1 dryaveraging_method :time-averaged  Array   Chunk   Bytes   28.32 MiB   0.94 MiB   Shape   (30, 47, 81, 65)   (1, 47, 81, 65)   Dask graph   30 chunks in 61 graph layers   Data type   float32 numpy.ndarray  30 1 65 81 47 </li></ul></li><li>Indexes: (4)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2020-09-01 21:30:00', '2020-09-02 21:30:00',\n               '2020-09-03 21:30:00', '2020-09-04 21:30:00',\n               '2020-09-05 21:30:00', '2020-09-06 21:30:00',\n               '2020-09-07 21:30:00', '2020-09-08 21:30:00',\n               '2020-09-09 21:30:00', '2020-09-10 21:30:00',\n               '2020-09-11 21:30:00', '2020-09-12 21:30:00',\n               '2020-09-13 21:30:00', '2020-09-14 21:30:00',\n               '2020-09-15 21:30:00', '2020-09-16 21:30:00',\n               '2020-09-17 21:30:00', '2020-09-18 21:30:00',\n               '2020-09-19 21:30:00', '2020-09-20 21:30:00',\n               '2020-09-21 21:30:00', '2020-09-22 21:30:00',\n               '2020-09-23 21:30:00', '2020-09-24 21:30:00',\n               '2020-09-25 21:30:00', '2020-09-26 21:30:00',\n               '2020-09-27 21:30:00', '2020-09-28 21:30:00',\n               '2020-09-29 21:30:00', '2020-09-30 21:30:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li><li>levPandasIndex<pre>PandasIndex(Index([         0.99250002413,          0.97749990013,     0.9624997759999999,\n                  0.947499955,             0.93250006,             0.91749991,\n                   0.90249991,             0.88749996,             0.87249996,\n           0.8575000599999999,            0.842500125,             0.82750016,\n                    0.8100002,     0.7875000200000001,            0.762499965,\n           0.7375001050000001,              0.7125001,              0.6875001,\n                   0.65625015,     0.6187501999999999,             0.58125015,\n                    0.5437501,              0.5062501,    0.46875009999999995,\n          0.43125009999999997,              0.3937501,              0.3562501,\n          0.31279157999999996,             0.26647905,           0.2265135325,\n           0.1925410165877065,     0.1636615040877065,    0.13911500000000002,\n                      0.11825,             0.10051436,            0.085439015,\n                   0.06745011,   0.048281660000000004,   0.034271609999999994,\n         0.024079704999999996,   0.014542270999999999,   0.006684756500000001,\n                 0.0028636805,          0.00113378455, 0.00041406405000000003,\n               0.000138674505, 3.8000004999999996e-05],\n      dtype='float64', name='lev'))</pre></li><li>latPandasIndex<pre>PandasIndex(Index([ 27.0, 27.25,  27.5, 27.75,  28.0, 28.25,  28.5, 28.75,  29.0, 29.25,\n        29.5, 29.75,  30.0, 30.25,  30.5, 30.75,  31.0, 31.25,  31.5, 31.75,\n        32.0, 32.25,  32.5, 32.75,  33.0, 33.25,  33.5, 33.75,  34.0, 34.25,\n        34.5, 34.75,  35.0, 35.25,  35.5, 35.75,  36.0, 36.25,  36.5, 36.75,\n        37.0, 37.25,  37.5, 37.75,  38.0, 38.25,  38.5, 38.75,  39.0, 39.25,\n        39.5, 39.75,  40.0, 40.25,  40.5, 40.75,  41.0, 41.25,  41.5, 41.75,\n        42.0, 42.25,  42.5, 42.75,  43.0, 43.25,  43.5, 43.75,  44.0, 44.25,\n        44.5, 44.75,  45.0, 45.25,  45.5, 45.75,  46.0, 46.25,  46.5, 46.75,\n        47.0],\n      dtype='float64', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([   -130.0, -129.6875,  -129.375, -129.0625,   -128.75, -128.4375,\n        -128.125, -127.8125,    -127.5, -127.1875,  -126.875, -126.5625,\n         -126.25, -125.9375,  -125.625, -125.3125,    -125.0, -124.6875,\n        -124.375, -124.0625,   -123.75, -123.4375,  -123.125, -122.8125,\n          -122.5, -122.1875,  -121.875, -121.5625,   -121.25, -120.9375,\n        -120.625, -120.3125,    -120.0, -119.6875,  -119.375, -119.0625,\n         -118.75, -118.4375,  -118.125, -117.8125,    -117.5, -117.1875,\n        -116.875, -116.5625,   -116.25, -115.9375,  -115.625, -115.3125,\n          -115.0, -114.6875,  -114.375, -114.0625,   -113.75, -113.4375,\n        -113.125, -112.8125,    -112.5, -112.1875,  -111.875, -111.5625,\n         -111.25, -110.9375,  -110.625, -110.3125,    -110.0],\n      dtype='float64', name='lon'))</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[5]: Copied! <pre>%time ds_multiple.SpeciesConc_NO2.mean(dim = 'time').compute()\n</pre> %time ds_multiple.SpeciesConc_NO2.mean(dim = 'time').compute() <pre>CPU times: user 462 ms, sys: 89.5 ms, total: 552 ms\nWall time: 2.52 s\n</pre> Out[5]: <pre>&lt;xarray.DataArray 'SpeciesConc_NO2' (lev: 47, lat: 81, lon: 65)&gt;\narray([[[4.9062303e-11, 4.9062081e-11, 4.9061859e-11, ...,\n         4.9071434e-11, 4.9071955e-11, 4.9072468e-11],\n        [1.6923058e-11, 1.6950108e-11, 1.6952899e-11, ...,\n         9.9948189e-11, 9.9900643e-11, 9.9872791e-11],\n        [1.6929434e-11, 1.6926667e-11, 1.6934249e-11, ...,\n         9.9928288e-11, 9.9878307e-11, 9.9870709e-11],\n        ...,\n        [8.5456746e-11, 8.5386170e-11, 8.5320598e-11, ...,\n         4.0959905e-10, 4.0894488e-10, 4.1077211e-10],\n        [8.5364313e-11, 8.5232314e-11, 8.5183277e-11, ...,\n         4.0959866e-10, 4.0940437e-10, 4.1138837e-10],\n        [8.0792273e-10, 8.0792262e-10, 8.0792262e-10, ...,\n         8.0796042e-10, 8.0796181e-10, 8.0796331e-10]],\n\n       [[4.8373788e-11, 4.8373711e-11, 4.8373645e-11, ...,\n         4.8377212e-11, 4.8377392e-11, 4.8377573e-11],\n        [1.6216743e-11, 1.6300355e-11, 1.6300242e-11, ...,\n         9.9929100e-11, 9.9903898e-11, 9.9897722e-11],\n        [1.6222215e-11, 1.6223784e-11, 1.6226818e-11, ...,\n         9.9919122e-11, 9.9897909e-11, 9.9892851e-11],\n...\n        [1.2932498e-09, 1.2932503e-09, 1.2932567e-09, ...,\n         1.2929462e-09, 1.2929313e-09, 1.2928540e-09],\n        [1.2932498e-09, 1.2936645e-09, 1.2936429e-09, ...,\n         1.2944885e-09, 1.2945151e-09, 1.2928540e-09],\n        [1.2952455e-09, 1.2952455e-09, 1.2952455e-09, ...,\n         1.2952455e-09, 1.2952455e-09, 1.2952455e-09]],\n\n       [[1.3088649e-09, 1.3088649e-09, 1.3088649e-09, ...,\n         1.3088649e-09, 1.3088649e-09, 1.3088649e-09],\n        [1.3110013e-09, 1.3099452e-09, 1.3099236e-09, ...,\n         1.3121275e-09, 1.3121271e-09, 1.3131806e-09],\n        [1.3110013e-09, 1.3109217e-09, 1.3109236e-09, ...,\n         1.3131171e-09, 1.3131758e-09, 1.3131806e-09],\n        ...,\n        [1.3325602e-09, 1.3325523e-09, 1.3325528e-09, ...,\n         1.3367689e-09, 1.3354362e-09, 1.3367826e-09],\n        [1.3325604e-09, 1.3321834e-09, 1.3321892e-09, ...,\n         1.3367548e-09, 1.3367537e-09, 1.3367827e-09],\n        [1.3349080e-09, 1.3349080e-09, 1.3349080e-09, ...,\n         1.3349080e-09, 1.3349080e-09, 1.3349080e-09]]], dtype=float32)\nCoordinates:\n  * lev      (lev) float64 0.9925 0.9775 0.9625 ... 0.0004141 0.0001387 3.8e-05\n  * lat      (lat) float64 27.0 27.25 27.5 27.75 28.0 ... 46.25 46.5 46.75 47.0\n  * lon      (lon) float64 -130.0 -129.7 -129.4 -129.1 ... -110.6 -110.3 -110.0</pre>xarray.DataArray'SpeciesConc_NO2'<ul><li>lev: 47</li><li>lat: 81</li><li>lon: 65</li></ul><ul><li>4.906e-11 4.906e-11 4.906e-11 ... 1.335e-09 1.335e-09 1.335e-09<pre>array([[[4.9062303e-11, 4.9062081e-11, 4.9061859e-11, ...,\n         4.9071434e-11, 4.9071955e-11, 4.9072468e-11],\n        [1.6923058e-11, 1.6950108e-11, 1.6952899e-11, ...,\n         9.9948189e-11, 9.9900643e-11, 9.9872791e-11],\n        [1.6929434e-11, 1.6926667e-11, 1.6934249e-11, ...,\n         9.9928288e-11, 9.9878307e-11, 9.9870709e-11],\n        ...,\n        [8.5456746e-11, 8.5386170e-11, 8.5320598e-11, ...,\n         4.0959905e-10, 4.0894488e-10, 4.1077211e-10],\n        [8.5364313e-11, 8.5232314e-11, 8.5183277e-11, ...,\n         4.0959866e-10, 4.0940437e-10, 4.1138837e-10],\n        [8.0792273e-10, 8.0792262e-10, 8.0792262e-10, ...,\n         8.0796042e-10, 8.0796181e-10, 8.0796331e-10]],\n\n       [[4.8373788e-11, 4.8373711e-11, 4.8373645e-11, ...,\n         4.8377212e-11, 4.8377392e-11, 4.8377573e-11],\n        [1.6216743e-11, 1.6300355e-11, 1.6300242e-11, ...,\n         9.9929100e-11, 9.9903898e-11, 9.9897722e-11],\n        [1.6222215e-11, 1.6223784e-11, 1.6226818e-11, ...,\n         9.9919122e-11, 9.9897909e-11, 9.9892851e-11],\n...\n        [1.2932498e-09, 1.2932503e-09, 1.2932567e-09, ...,\n         1.2929462e-09, 1.2929313e-09, 1.2928540e-09],\n        [1.2932498e-09, 1.2936645e-09, 1.2936429e-09, ...,\n         1.2944885e-09, 1.2945151e-09, 1.2928540e-09],\n        [1.2952455e-09, 1.2952455e-09, 1.2952455e-09, ...,\n         1.2952455e-09, 1.2952455e-09, 1.2952455e-09]],\n\n       [[1.3088649e-09, 1.3088649e-09, 1.3088649e-09, ...,\n         1.3088649e-09, 1.3088649e-09, 1.3088649e-09],\n        [1.3110013e-09, 1.3099452e-09, 1.3099236e-09, ...,\n         1.3121275e-09, 1.3121271e-09, 1.3131806e-09],\n        [1.3110013e-09, 1.3109217e-09, 1.3109236e-09, ...,\n         1.3131171e-09, 1.3131758e-09, 1.3131806e-09],\n        ...,\n        [1.3325602e-09, 1.3325523e-09, 1.3325528e-09, ...,\n         1.3367689e-09, 1.3354362e-09, 1.3367826e-09],\n        [1.3325604e-09, 1.3321834e-09, 1.3321892e-09, ...,\n         1.3367548e-09, 1.3367537e-09, 1.3367827e-09],\n        [1.3349080e-09, 1.3349080e-09, 1.3349080e-09, ...,\n         1.3349080e-09, 1.3349080e-09, 1.3349080e-09]]], dtype=float32)</pre></li><li>Coordinates: (3)<ul><li>lev(lev)float640.9925 0.9775 ... 0.0001387 3.8e-05long_name :hybrid level at midpoints ((A/P0)+B)units :levelaxis :Zpositive :upstandard_name :atmosphere_hybrid_sigma_pressure_coordinateformula_terms :a: hyam b: hybm p0: P0 ps: PS<pre>array([9.925000e-01, 9.774999e-01, 9.624998e-01, 9.475000e-01, 9.325001e-01,\n       9.174999e-01, 9.024999e-01, 8.875000e-01, 8.725000e-01, 8.575001e-01,\n       8.425001e-01, 8.275002e-01, 8.100002e-01, 7.875000e-01, 7.625000e-01,\n       7.375001e-01, 7.125001e-01, 6.875001e-01, 6.562501e-01, 6.187502e-01,\n       5.812502e-01, 5.437501e-01, 5.062501e-01, 4.687501e-01, 4.312501e-01,\n       3.937501e-01, 3.562501e-01, 3.127916e-01, 2.664790e-01, 2.265135e-01,\n       1.925410e-01, 1.636615e-01, 1.391150e-01, 1.182500e-01, 1.005144e-01,\n       8.543901e-02, 6.745011e-02, 4.828166e-02, 3.427161e-02, 2.407970e-02,\n       1.454227e-02, 6.684757e-03, 2.863681e-03, 1.133785e-03, 4.140641e-04,\n       1.386745e-04, 3.800000e-05])</pre></li><li>lat(lat)float6427.0 27.25 27.5 ... 46.5 46.75 47.0long_name :Latitudeunits :degrees_northaxis :Y<pre>array([27.  , 27.25, 27.5 , 27.75, 28.  , 28.25, 28.5 , 28.75, 29.  , 29.25,\n       29.5 , 29.75, 30.  , 30.25, 30.5 , 30.75, 31.  , 31.25, 31.5 , 31.75,\n       32.  , 32.25, 32.5 , 32.75, 33.  , 33.25, 33.5 , 33.75, 34.  , 34.25,\n       34.5 , 34.75, 35.  , 35.25, 35.5 , 35.75, 36.  , 36.25, 36.5 , 36.75,\n       37.  , 37.25, 37.5 , 37.75, 38.  , 38.25, 38.5 , 38.75, 39.  , 39.25,\n       39.5 , 39.75, 40.  , 40.25, 40.5 , 40.75, 41.  , 41.25, 41.5 , 41.75,\n       42.  , 42.25, 42.5 , 42.75, 43.  , 43.25, 43.5 , 43.75, 44.  , 44.25,\n       44.5 , 44.75, 45.  , 45.25, 45.5 , 45.75, 46.  , 46.25, 46.5 , 46.75,\n       47.  ])</pre></li><li>lon(lon)float64-130.0 -129.7 ... -110.3 -110.0long_name :Longitudeunits :degrees_eastaxis :X<pre>array([-130.    , -129.6875, -129.375 , -129.0625, -128.75  , -128.4375,\n       -128.125 , -127.8125, -127.5   , -127.1875, -126.875 , -126.5625,\n       -126.25  , -125.9375, -125.625 , -125.3125, -125.    , -124.6875,\n       -124.375 , -124.0625, -123.75  , -123.4375, -123.125 , -122.8125,\n       -122.5   , -122.1875, -121.875 , -121.5625, -121.25  , -120.9375,\n       -120.625 , -120.3125, -120.    , -119.6875, -119.375 , -119.0625,\n       -118.75  , -118.4375, -118.125 , -117.8125, -117.5   , -117.1875,\n       -116.875 , -116.5625, -116.25  , -115.9375, -115.625 , -115.3125,\n       -115.    , -114.6875, -114.375 , -114.0625, -113.75  , -113.4375,\n       -113.125 , -112.8125, -112.5   , -112.1875, -111.875 , -111.5625,\n       -111.25  , -110.9375, -110.625 , -110.3125, -110.    ])</pre></li></ul></li><li>Indexes: (3)<ul><li>levPandasIndex<pre>PandasIndex(Index([         0.99250002413,          0.97749990013,     0.9624997759999999,\n                  0.947499955,             0.93250006,             0.91749991,\n                   0.90249991,             0.88749996,             0.87249996,\n           0.8575000599999999,            0.842500125,             0.82750016,\n                    0.8100002,     0.7875000200000001,            0.762499965,\n           0.7375001050000001,              0.7125001,              0.6875001,\n                   0.65625015,     0.6187501999999999,             0.58125015,\n                    0.5437501,              0.5062501,    0.46875009999999995,\n          0.43125009999999997,              0.3937501,              0.3562501,\n          0.31279157999999996,             0.26647905,           0.2265135325,\n           0.1925410165877065,     0.1636615040877065,    0.13911500000000002,\n                      0.11825,             0.10051436,            0.085439015,\n                   0.06745011,   0.048281660000000004,   0.034271609999999994,\n         0.024079704999999996,   0.014542270999999999,   0.006684756500000001,\n                 0.0028636805,          0.00113378455, 0.00041406405000000003,\n               0.000138674505, 3.8000004999999996e-05],\n      dtype='float64', name='lev'))</pre></li><li>latPandasIndex<pre>PandasIndex(Index([ 27.0, 27.25,  27.5, 27.75,  28.0, 28.25,  28.5, 28.75,  29.0, 29.25,\n        29.5, 29.75,  30.0, 30.25,  30.5, 30.75,  31.0, 31.25,  31.5, 31.75,\n        32.0, 32.25,  32.5, 32.75,  33.0, 33.25,  33.5, 33.75,  34.0, 34.25,\n        34.5, 34.75,  35.0, 35.25,  35.5, 35.75,  36.0, 36.25,  36.5, 36.75,\n        37.0, 37.25,  37.5, 37.75,  38.0, 38.25,  38.5, 38.75,  39.0, 39.25,\n        39.5, 39.75,  40.0, 40.25,  40.5, 40.75,  41.0, 41.25,  41.5, 41.75,\n        42.0, 42.25,  42.5, 42.75,  43.0, 43.25,  43.5, 43.75,  44.0, 44.25,\n        44.5, 44.75,  45.0, 45.25,  45.5, 45.75,  46.0, 46.25,  46.5, 46.75,\n        47.0],\n      dtype='float64', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([   -130.0, -129.6875,  -129.375, -129.0625,   -128.75, -128.4375,\n        -128.125, -127.8125,    -127.5, -127.1875,  -126.875, -126.5625,\n         -126.25, -125.9375,  -125.625, -125.3125,    -125.0, -124.6875,\n        -124.375, -124.0625,   -123.75, -123.4375,  -123.125, -122.8125,\n          -122.5, -122.1875,  -121.875, -121.5625,   -121.25, -120.9375,\n        -120.625, -120.3125,    -120.0, -119.6875,  -119.375, -119.0625,\n         -118.75, -118.4375,  -118.125, -117.8125,    -117.5, -117.1875,\n        -116.875, -116.5625,   -116.25, -115.9375,  -115.625, -115.3125,\n          -115.0, -114.6875,  -114.375, -114.0625,   -113.75, -113.4375,\n        -113.125, -112.8125,    -112.5, -112.1875,  -111.875, -111.5625,\n         -111.25, -110.9375,  -110.625, -110.3125,    -110.0],\n      dtype='float64', name='lon'))</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[8]: Copied! <pre>rolling_mean = ds_multiple.SpeciesConc_NO2.rolling(time=5).mean()  \nrolling_mean  # contains dask array\n</pre> rolling_mean = ds_multiple.SpeciesConc_NO2.rolling(time=5).mean()   rolling_mean  # contains dask array Out[8]: <pre>&lt;xarray.DataArray 'SpeciesConc_NO2' (time: 30, lev: 47, lat: 81, lon: 65)&gt;\ndask.array&lt;truediv, shape=(30, 47, 81, 65), dtype=float32, chunksize=(5, 47, 81, 65), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * time     (time) datetime64[ns] 2020-09-01T21:30:00 ... 2020-09-30T21:30:00\n  * lev      (lev) float64 0.9925 0.9775 0.9625 ... 0.0004141 0.0001387 3.8e-05\n  * lat      (lat) float64 27.0 27.25 27.5 27.75 28.0 ... 46.25 46.5 46.75 47.0\n  * lon      (lon) float64 -130.0 -129.7 -129.4 -129.1 ... -110.6 -110.3 -110.0\nAttributes:\n    long_name:         Dry mixing ratio of species NO2\n    units:             mol mol-1 dry\n    averaging_method:  time-averaged</pre>xarray.DataArray'SpeciesConc_NO2'<ul><li>time: 30</li><li>lev: 47</li><li>lat: 81</li><li>lon: 65</li></ul><ul><li>dask.array&lt;chunksize=(5, 47, 81, 65), meta=np.ndarray&gt;  Array   Chunk   Bytes   28.32 MiB   4.72 MiB   Shape   (30, 47, 81, 65)   (5, 47, 81, 65)   Dask graph   6 chunks in 86 graph layers   Data type   float32 numpy.ndarray  30 1 65 81 47 </li><li>Coordinates: (4)<ul><li>time(time)datetime64[ns]2020-09-01T21:30:00 ... 2020-09-...long_name :Timeaxis :T<pre>array(['2020-09-01T21:30:00.000000000', '2020-09-02T21:30:00.000000000',\n       '2020-09-03T21:30:00.000000000', '2020-09-04T21:30:00.000000000',\n       '2020-09-05T21:30:00.000000000', '2020-09-06T21:30:00.000000000',\n       '2020-09-07T21:30:00.000000000', '2020-09-08T21:30:00.000000000',\n       '2020-09-09T21:30:00.000000000', '2020-09-10T21:30:00.000000000',\n       '2020-09-11T21:30:00.000000000', '2020-09-12T21:30:00.000000000',\n       '2020-09-13T21:30:00.000000000', '2020-09-14T21:30:00.000000000',\n       '2020-09-15T21:30:00.000000000', '2020-09-16T21:30:00.000000000',\n       '2020-09-17T21:30:00.000000000', '2020-09-18T21:30:00.000000000',\n       '2020-09-19T21:30:00.000000000', '2020-09-20T21:30:00.000000000',\n       '2020-09-21T21:30:00.000000000', '2020-09-22T21:30:00.000000000',\n       '2020-09-23T21:30:00.000000000', '2020-09-24T21:30:00.000000000',\n       '2020-09-25T21:30:00.000000000', '2020-09-26T21:30:00.000000000',\n       '2020-09-27T21:30:00.000000000', '2020-09-28T21:30:00.000000000',\n       '2020-09-29T21:30:00.000000000', '2020-09-30T21:30:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>lev(lev)float640.9925 0.9775 ... 0.0001387 3.8e-05long_name :hybrid level at midpoints ((A/P0)+B)units :levelaxis :Zpositive :upstandard_name :atmosphere_hybrid_sigma_pressure_coordinateformula_terms :a: hyam b: hybm p0: P0 ps: PS<pre>array([9.925000e-01, 9.774999e-01, 9.624998e-01, 9.475000e-01, 9.325001e-01,\n       9.174999e-01, 9.024999e-01, 8.875000e-01, 8.725000e-01, 8.575001e-01,\n       8.425001e-01, 8.275002e-01, 8.100002e-01, 7.875000e-01, 7.625000e-01,\n       7.375001e-01, 7.125001e-01, 6.875001e-01, 6.562501e-01, 6.187502e-01,\n       5.812502e-01, 5.437501e-01, 5.062501e-01, 4.687501e-01, 4.312501e-01,\n       3.937501e-01, 3.562501e-01, 3.127916e-01, 2.664790e-01, 2.265135e-01,\n       1.925410e-01, 1.636615e-01, 1.391150e-01, 1.182500e-01, 1.005144e-01,\n       8.543901e-02, 6.745011e-02, 4.828166e-02, 3.427161e-02, 2.407970e-02,\n       1.454227e-02, 6.684757e-03, 2.863681e-03, 1.133785e-03, 4.140641e-04,\n       1.386745e-04, 3.800000e-05])</pre></li><li>lat(lat)float6427.0 27.25 27.5 ... 46.5 46.75 47.0long_name :Latitudeunits :degrees_northaxis :Y<pre>array([27.  , 27.25, 27.5 , 27.75, 28.  , 28.25, 28.5 , 28.75, 29.  , 29.25,\n       29.5 , 29.75, 30.  , 30.25, 30.5 , 30.75, 31.  , 31.25, 31.5 , 31.75,\n       32.  , 32.25, 32.5 , 32.75, 33.  , 33.25, 33.5 , 33.75, 34.  , 34.25,\n       34.5 , 34.75, 35.  , 35.25, 35.5 , 35.75, 36.  , 36.25, 36.5 , 36.75,\n       37.  , 37.25, 37.5 , 37.75, 38.  , 38.25, 38.5 , 38.75, 39.  , 39.25,\n       39.5 , 39.75, 40.  , 40.25, 40.5 , 40.75, 41.  , 41.25, 41.5 , 41.75,\n       42.  , 42.25, 42.5 , 42.75, 43.  , 43.25, 43.5 , 43.75, 44.  , 44.25,\n       44.5 , 44.75, 45.  , 45.25, 45.5 , 45.75, 46.  , 46.25, 46.5 , 46.75,\n       47.  ])</pre></li><li>lon(lon)float64-130.0 -129.7 ... -110.3 -110.0long_name :Longitudeunits :degrees_eastaxis :X<pre>array([-130.    , -129.6875, -129.375 , -129.0625, -128.75  , -128.4375,\n       -128.125 , -127.8125, -127.5   , -127.1875, -126.875 , -126.5625,\n       -126.25  , -125.9375, -125.625 , -125.3125, -125.    , -124.6875,\n       -124.375 , -124.0625, -123.75  , -123.4375, -123.125 , -122.8125,\n       -122.5   , -122.1875, -121.875 , -121.5625, -121.25  , -120.9375,\n       -120.625 , -120.3125, -120.    , -119.6875, -119.375 , -119.0625,\n       -118.75  , -118.4375, -118.125 , -117.8125, -117.5   , -117.1875,\n       -116.875 , -116.5625, -116.25  , -115.9375, -115.625 , -115.3125,\n       -115.    , -114.6875, -114.375 , -114.0625, -113.75  , -113.4375,\n       -113.125 , -112.8125, -112.5   , -112.1875, -111.875 , -111.5625,\n       -111.25  , -110.9375, -110.625 , -110.3125, -110.    ])</pre></li></ul></li><li>Indexes: (4)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2020-09-01 21:30:00', '2020-09-02 21:30:00',\n               '2020-09-03 21:30:00', '2020-09-04 21:30:00',\n               '2020-09-05 21:30:00', '2020-09-06 21:30:00',\n               '2020-09-07 21:30:00', '2020-09-08 21:30:00',\n               '2020-09-09 21:30:00', '2020-09-10 21:30:00',\n               '2020-09-11 21:30:00', '2020-09-12 21:30:00',\n               '2020-09-13 21:30:00', '2020-09-14 21:30:00',\n               '2020-09-15 21:30:00', '2020-09-16 21:30:00',\n               '2020-09-17 21:30:00', '2020-09-18 21:30:00',\n               '2020-09-19 21:30:00', '2020-09-20 21:30:00',\n               '2020-09-21 21:30:00', '2020-09-22 21:30:00',\n               '2020-09-23 21:30:00', '2020-09-24 21:30:00',\n               '2020-09-25 21:30:00', '2020-09-26 21:30:00',\n               '2020-09-27 21:30:00', '2020-09-28 21:30:00',\n               '2020-09-29 21:30:00', '2020-09-30 21:30:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li><li>levPandasIndex<pre>PandasIndex(Index([         0.99250002413,          0.97749990013,     0.9624997759999999,\n                  0.947499955,             0.93250006,             0.91749991,\n                   0.90249991,             0.88749996,             0.87249996,\n           0.8575000599999999,            0.842500125,             0.82750016,\n                    0.8100002,     0.7875000200000001,            0.762499965,\n           0.7375001050000001,              0.7125001,              0.6875001,\n                   0.65625015,     0.6187501999999999,             0.58125015,\n                    0.5437501,              0.5062501,    0.46875009999999995,\n          0.43125009999999997,              0.3937501,              0.3562501,\n          0.31279157999999996,             0.26647905,           0.2265135325,\n           0.1925410165877065,     0.1636615040877065,    0.13911500000000002,\n                      0.11825,             0.10051436,            0.085439015,\n                   0.06745011,   0.048281660000000004,   0.034271609999999994,\n         0.024079704999999996,   0.014542270999999999,   0.006684756500000001,\n                 0.0028636805,          0.00113378455, 0.00041406405000000003,\n               0.000138674505, 3.8000004999999996e-05],\n      dtype='float64', name='lev'))</pre></li><li>latPandasIndex<pre>PandasIndex(Index([ 27.0, 27.25,  27.5, 27.75,  28.0, 28.25,  28.5, 28.75,  29.0, 29.25,\n        29.5, 29.75,  30.0, 30.25,  30.5, 30.75,  31.0, 31.25,  31.5, 31.75,\n        32.0, 32.25,  32.5, 32.75,  33.0, 33.25,  33.5, 33.75,  34.0, 34.25,\n        34.5, 34.75,  35.0, 35.25,  35.5, 35.75,  36.0, 36.25,  36.5, 36.75,\n        37.0, 37.25,  37.5, 37.75,  38.0, 38.25,  38.5, 38.75,  39.0, 39.25,\n        39.5, 39.75,  40.0, 40.25,  40.5, 40.75,  41.0, 41.25,  41.5, 41.75,\n        42.0, 42.25,  42.5, 42.75,  43.0, 43.25,  43.5, 43.75,  44.0, 44.25,\n        44.5, 44.75,  45.0, 45.25,  45.5, 45.75,  46.0, 46.25,  46.5, 46.75,\n        47.0],\n      dtype='float64', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([   -130.0, -129.6875,  -129.375, -129.0625,   -128.75, -128.4375,\n        -128.125, -127.8125,    -127.5, -127.1875,  -126.875, -126.5625,\n         -126.25, -125.9375,  -125.625, -125.3125,    -125.0, -124.6875,\n        -124.375, -124.0625,   -123.75, -123.4375,  -123.125, -122.8125,\n          -122.5, -122.1875,  -121.875, -121.5625,   -121.25, -120.9375,\n        -120.625, -120.3125,    -120.0, -119.6875,  -119.375, -119.0625,\n         -118.75, -118.4375,  -118.125, -117.8125,    -117.5, -117.1875,\n        -116.875, -116.5625,   -116.25, -115.9375,  -115.625, -115.3125,\n          -115.0, -114.6875,  -114.375, -114.0625,   -113.75, -113.4375,\n        -113.125, -112.8125,    -112.5, -112.1875,  -111.875, -111.5625,\n         -111.25, -110.9375,  -110.625, -110.3125,    -110.0],\n      dtype='float64', name='lon'))</pre></li></ul></li><li>Attributes: (3)long_name :Dry mixing ratio of species NO2units :mol mol-1 dryaveraging_method :time-averaged</li></ul> In\u00a0[9]: Copied! <pre>computed = rolling_mean.compute()  \ncomputed  # has real numpy values\n</pre> computed = rolling_mean.compute()   computed  # has real numpy values Out[9]: <pre>&lt;xarray.DataArray 'SpeciesConc_NO2' (time: 30, lev: 47, lat: 81, lon: 65)&gt;\narray([[[[           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n         [           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n         [           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n         ...,\n         [           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n         [           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n         [           nan,            nan,            nan, ...,\n                     nan,            nan,            nan]],\n\n        [[           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n         [           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n         [           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n...\n          1.27299549e-09, 1.27348032e-09, 1.27298894e-09],\n         [1.27392474e-09, 1.27276323e-09, 1.27275179e-09, ...,\n          1.27361832e-09, 1.27358102e-09, 1.27298905e-09],\n         [1.27646427e-09, 1.27646427e-09, 1.27646427e-09, ...,\n          1.27646427e-09, 1.27646427e-09, 1.27646427e-09]],\n\n        [[1.25867883e-09, 1.25867883e-09, 1.25867883e-09, ...,\n          1.25867883e-09, 1.25867883e-09, 1.25867883e-09],\n         [1.25231525e-09, 1.25114186e-09, 1.25108790e-09, ...,\n          1.25833632e-09, 1.25830835e-09, 1.25872757e-09],\n         [1.25231525e-09, 1.25225497e-09, 1.25224831e-09, ...,\n          1.25869781e-09, 1.25869914e-09, 1.25872757e-09],\n         ...,\n         [1.28924105e-09, 1.28924049e-09, 1.28924071e-09, ...,\n          1.29703770e-09, 1.29703892e-09, 1.29704647e-09],\n         [1.28924105e-09, 1.28915789e-09, 1.28917210e-09, ...,\n          1.29697664e-09, 1.29696454e-09, 1.29704658e-09],\n         [1.29171085e-09, 1.29171085e-09, 1.29171085e-09, ...,\n          1.29171085e-09, 1.29171085e-09, 1.29171085e-09]]]],\n      dtype=float32)\nCoordinates:\n  * time     (time) datetime64[ns] 2020-09-01T21:30:00 ... 2020-09-30T21:30:00\n  * lev      (lev) float64 0.9925 0.9775 0.9625 ... 0.0004141 0.0001387 3.8e-05\n  * lat      (lat) float64 27.0 27.25 27.5 27.75 28.0 ... 46.25 46.5 46.75 47.0\n  * lon      (lon) float64 -130.0 -129.7 -129.4 -129.1 ... -110.6 -110.3 -110.0\nAttributes:\n    long_name:         Dry mixing ratio of species NO2\n    units:             mol mol-1 dry\n    averaging_method:  time-averaged</pre>xarray.DataArray'SpeciesConc_NO2'<ul><li>time: 30</li><li>lev: 47</li><li>lat: 81</li><li>lon: 65</li></ul><ul><li>nan nan nan nan nan ... 1.292e-09 1.292e-09 1.292e-09 1.292e-09<pre>array([[[[           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n         [           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n         [           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n         ...,\n         [           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n         [           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n         [           nan,            nan,            nan, ...,\n                     nan,            nan,            nan]],\n\n        [[           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n         [           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n         [           nan,            nan,            nan, ...,\n                     nan,            nan,            nan],\n...\n          1.27299549e-09, 1.27348032e-09, 1.27298894e-09],\n         [1.27392474e-09, 1.27276323e-09, 1.27275179e-09, ...,\n          1.27361832e-09, 1.27358102e-09, 1.27298905e-09],\n         [1.27646427e-09, 1.27646427e-09, 1.27646427e-09, ...,\n          1.27646427e-09, 1.27646427e-09, 1.27646427e-09]],\n\n        [[1.25867883e-09, 1.25867883e-09, 1.25867883e-09, ...,\n          1.25867883e-09, 1.25867883e-09, 1.25867883e-09],\n         [1.25231525e-09, 1.25114186e-09, 1.25108790e-09, ...,\n          1.25833632e-09, 1.25830835e-09, 1.25872757e-09],\n         [1.25231525e-09, 1.25225497e-09, 1.25224831e-09, ...,\n          1.25869781e-09, 1.25869914e-09, 1.25872757e-09],\n         ...,\n         [1.28924105e-09, 1.28924049e-09, 1.28924071e-09, ...,\n          1.29703770e-09, 1.29703892e-09, 1.29704647e-09],\n         [1.28924105e-09, 1.28915789e-09, 1.28917210e-09, ...,\n          1.29697664e-09, 1.29696454e-09, 1.29704658e-09],\n         [1.29171085e-09, 1.29171085e-09, 1.29171085e-09, ...,\n          1.29171085e-09, 1.29171085e-09, 1.29171085e-09]]]],\n      dtype=float32)</pre></li><li>Coordinates: (4)<ul><li>time(time)datetime64[ns]2020-09-01T21:30:00 ... 2020-09-...long_name :Timeaxis :T<pre>array(['2020-09-01T21:30:00.000000000', '2020-09-02T21:30:00.000000000',\n       '2020-09-03T21:30:00.000000000', '2020-09-04T21:30:00.000000000',\n       '2020-09-05T21:30:00.000000000', '2020-09-06T21:30:00.000000000',\n       '2020-09-07T21:30:00.000000000', '2020-09-08T21:30:00.000000000',\n       '2020-09-09T21:30:00.000000000', '2020-09-10T21:30:00.000000000',\n       '2020-09-11T21:30:00.000000000', '2020-09-12T21:30:00.000000000',\n       '2020-09-13T21:30:00.000000000', '2020-09-14T21:30:00.000000000',\n       '2020-09-15T21:30:00.000000000', '2020-09-16T21:30:00.000000000',\n       '2020-09-17T21:30:00.000000000', '2020-09-18T21:30:00.000000000',\n       '2020-09-19T21:30:00.000000000', '2020-09-20T21:30:00.000000000',\n       '2020-09-21T21:30:00.000000000', '2020-09-22T21:30:00.000000000',\n       '2020-09-23T21:30:00.000000000', '2020-09-24T21:30:00.000000000',\n       '2020-09-25T21:30:00.000000000', '2020-09-26T21:30:00.000000000',\n       '2020-09-27T21:30:00.000000000', '2020-09-28T21:30:00.000000000',\n       '2020-09-29T21:30:00.000000000', '2020-09-30T21:30:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>lev(lev)float640.9925 0.9775 ... 0.0001387 3.8e-05long_name :hybrid level at midpoints ((A/P0)+B)units :levelaxis :Zpositive :upstandard_name :atmosphere_hybrid_sigma_pressure_coordinateformula_terms :a: hyam b: hybm p0: P0 ps: PS<pre>array([9.925000e-01, 9.774999e-01, 9.624998e-01, 9.475000e-01, 9.325001e-01,\n       9.174999e-01, 9.024999e-01, 8.875000e-01, 8.725000e-01, 8.575001e-01,\n       8.425001e-01, 8.275002e-01, 8.100002e-01, 7.875000e-01, 7.625000e-01,\n       7.375001e-01, 7.125001e-01, 6.875001e-01, 6.562501e-01, 6.187502e-01,\n       5.812502e-01, 5.437501e-01, 5.062501e-01, 4.687501e-01, 4.312501e-01,\n       3.937501e-01, 3.562501e-01, 3.127916e-01, 2.664790e-01, 2.265135e-01,\n       1.925410e-01, 1.636615e-01, 1.391150e-01, 1.182500e-01, 1.005144e-01,\n       8.543901e-02, 6.745011e-02, 4.828166e-02, 3.427161e-02, 2.407970e-02,\n       1.454227e-02, 6.684757e-03, 2.863681e-03, 1.133785e-03, 4.140641e-04,\n       1.386745e-04, 3.800000e-05])</pre></li><li>lat(lat)float6427.0 27.25 27.5 ... 46.5 46.75 47.0long_name :Latitudeunits :degrees_northaxis :Y<pre>array([27.  , 27.25, 27.5 , 27.75, 28.  , 28.25, 28.5 , 28.75, 29.  , 29.25,\n       29.5 , 29.75, 30.  , 30.25, 30.5 , 30.75, 31.  , 31.25, 31.5 , 31.75,\n       32.  , 32.25, 32.5 , 32.75, 33.  , 33.25, 33.5 , 33.75, 34.  , 34.25,\n       34.5 , 34.75, 35.  , 35.25, 35.5 , 35.75, 36.  , 36.25, 36.5 , 36.75,\n       37.  , 37.25, 37.5 , 37.75, 38.  , 38.25, 38.5 , 38.75, 39.  , 39.25,\n       39.5 , 39.75, 40.  , 40.25, 40.5 , 40.75, 41.  , 41.25, 41.5 , 41.75,\n       42.  , 42.25, 42.5 , 42.75, 43.  , 43.25, 43.5 , 43.75, 44.  , 44.25,\n       44.5 , 44.75, 45.  , 45.25, 45.5 , 45.75, 46.  , 46.25, 46.5 , 46.75,\n       47.  ])</pre></li><li>lon(lon)float64-130.0 -129.7 ... -110.3 -110.0long_name :Longitudeunits :degrees_eastaxis :X<pre>array([-130.    , -129.6875, -129.375 , -129.0625, -128.75  , -128.4375,\n       -128.125 , -127.8125, -127.5   , -127.1875, -126.875 , -126.5625,\n       -126.25  , -125.9375, -125.625 , -125.3125, -125.    , -124.6875,\n       -124.375 , -124.0625, -123.75  , -123.4375, -123.125 , -122.8125,\n       -122.5   , -122.1875, -121.875 , -121.5625, -121.25  , -120.9375,\n       -120.625 , -120.3125, -120.    , -119.6875, -119.375 , -119.0625,\n       -118.75  , -118.4375, -118.125 , -117.8125, -117.5   , -117.1875,\n       -116.875 , -116.5625, -116.25  , -115.9375, -115.625 , -115.3125,\n       -115.    , -114.6875, -114.375 , -114.0625, -113.75  , -113.4375,\n       -113.125 , -112.8125, -112.5   , -112.1875, -111.875 , -111.5625,\n       -111.25  , -110.9375, -110.625 , -110.3125, -110.    ])</pre></li></ul></li><li>Indexes: (4)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2020-09-01 21:30:00', '2020-09-02 21:30:00',\n               '2020-09-03 21:30:00', '2020-09-04 21:30:00',\n               '2020-09-05 21:30:00', '2020-09-06 21:30:00',\n               '2020-09-07 21:30:00', '2020-09-08 21:30:00',\n               '2020-09-09 21:30:00', '2020-09-10 21:30:00',\n               '2020-09-11 21:30:00', '2020-09-12 21:30:00',\n               '2020-09-13 21:30:00', '2020-09-14 21:30:00',\n               '2020-09-15 21:30:00', '2020-09-16 21:30:00',\n               '2020-09-17 21:30:00', '2020-09-18 21:30:00',\n               '2020-09-19 21:30:00', '2020-09-20 21:30:00',\n               '2020-09-21 21:30:00', '2020-09-22 21:30:00',\n               '2020-09-23 21:30:00', '2020-09-24 21:30:00',\n               '2020-09-25 21:30:00', '2020-09-26 21:30:00',\n               '2020-09-27 21:30:00', '2020-09-28 21:30:00',\n               '2020-09-29 21:30:00', '2020-09-30 21:30:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li><li>levPandasIndex<pre>PandasIndex(Index([         0.99250002413,          0.97749990013,     0.9624997759999999,\n                  0.947499955,             0.93250006,             0.91749991,\n                   0.90249991,             0.88749996,             0.87249996,\n           0.8575000599999999,            0.842500125,             0.82750016,\n                    0.8100002,     0.7875000200000001,            0.762499965,\n           0.7375001050000001,              0.7125001,              0.6875001,\n                   0.65625015,     0.6187501999999999,             0.58125015,\n                    0.5437501,              0.5062501,    0.46875009999999995,\n          0.43125009999999997,              0.3937501,              0.3562501,\n          0.31279157999999996,             0.26647905,           0.2265135325,\n           0.1925410165877065,     0.1636615040877065,    0.13911500000000002,\n                      0.11825,             0.10051436,            0.085439015,\n                   0.06745011,   0.048281660000000004,   0.034271609999999994,\n         0.024079704999999996,   0.014542270999999999,   0.006684756500000001,\n                 0.0028636805,          0.00113378455, 0.00041406405000000003,\n               0.000138674505, 3.8000004999999996e-05],\n      dtype='float64', name='lev'))</pre></li><li>latPandasIndex<pre>PandasIndex(Index([ 27.0, 27.25,  27.5, 27.75,  28.0, 28.25,  28.5, 28.75,  29.0, 29.25,\n        29.5, 29.75,  30.0, 30.25,  30.5, 30.75,  31.0, 31.25,  31.5, 31.75,\n        32.0, 32.25,  32.5, 32.75,  33.0, 33.25,  33.5, 33.75,  34.0, 34.25,\n        34.5, 34.75,  35.0, 35.25,  35.5, 35.75,  36.0, 36.25,  36.5, 36.75,\n        37.0, 37.25,  37.5, 37.75,  38.0, 38.25,  38.5, 38.75,  39.0, 39.25,\n        39.5, 39.75,  40.0, 40.25,  40.5, 40.75,  41.0, 41.25,  41.5, 41.75,\n        42.0, 42.25,  42.5, 42.75,  43.0, 43.25,  43.5, 43.75,  44.0, 44.25,\n        44.5, 44.75,  45.0, 45.25,  45.5, 45.75,  46.0, 46.25,  46.5, 46.75,\n        47.0],\n      dtype='float64', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([   -130.0, -129.6875,  -129.375, -129.0625,   -128.75, -128.4375,\n        -128.125, -127.8125,    -127.5, -127.1875,  -126.875, -126.5625,\n         -126.25, -125.9375,  -125.625, -125.3125,    -125.0, -124.6875,\n        -124.375, -124.0625,   -123.75, -123.4375,  -123.125, -122.8125,\n          -122.5, -122.1875,  -121.875, -121.5625,   -121.25, -120.9375,\n        -120.625, -120.3125,    -120.0, -119.6875,  -119.375, -119.0625,\n         -118.75, -118.4375,  -118.125, -117.8125,    -117.5, -117.1875,\n        -116.875, -116.5625,   -116.25, -115.9375,  -115.625, -115.3125,\n          -115.0, -114.6875,  -114.375, -114.0625,   -113.75, -113.4375,\n        -113.125, -112.8125,    -112.5, -112.1875,  -111.875, -111.5625,\n         -111.25, -110.9375,  -110.625, -110.3125,    -110.0],\n      dtype='float64', name='lon'))</pre></li></ul></li><li>Attributes: (3)long_name :Dry mixing ratio of species NO2units :mol mol-1 dryaveraging_method :time-averaged</li></ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Lecture_9_Dask_Parallel_Computing/#lecture-9-dask-for-parellel-computing-and-big-data","title":"Lecture 9 Dask for Parellel Computing and Big Data\u00b6","text":""},{"location":"Lecture_9_Dask_Parallel_Computing/#what-is-dask","title":"What is Dask?\u00b6","text":"<p>Dask is a tool that helps us easily extend our familiar python data analysis tools to medium and big data, i.e. dataset that can't fit in our computer's RAM. In many cases, dask also allows us to speed up our analysis by using mutiple CPU cores. Dask can help us work more efficiently on our laptop, and it can also help us scale up our analysis on HPC and cloud platforms. Most importantly, dask is almost invisible to the user, meaning that you can focus on your science, rather than the details of parallel computing.</p> <p>Dask was created by the brilliant Matt Rocklin. You can learn more about it on</p> <ul> <li>The Dask Documentation</li> <li>The Dask Github Site</li> </ul> <p>Dask provides collections for big data and a scheduler for parallel computing.</p>"},{"location":"Lecture_9_Dask_Parallel_Computing/#dask-arrays","title":"Dask Arrays\u00b6","text":"<p>A dask array looks and feels a lot like a numpy array. However, a dask array doesn't directly hold any data. Instead, it symbolically represents the computations needed to generate the data. Nothing is actually computed until the actual numerical values are needed. This mode of operation is called \"lazy\"; it allows one to build up complex, large calculations symbolically before turning them over the scheduler for execution.</p> <p>If we want to create a numpy array of all ones, we do it like this:</p>"},{"location":"Lecture_9_Dask_Parallel_Computing/#a-bigger-calculation","title":"A Bigger Calculation\u00b6","text":"<p>The examples above were toy examples; the data (32 MB) is nowhere nearly big enough to warrant the use of dask.</p> <p>We can make it a lot bigger!</p>"},{"location":"Lecture_9_Dask_Parallel_Computing/#reduction","title":"Reduction\u00b6","text":"<p>All the usual numpy methods work on dask arrays. You can also apply numpy function directly to a dask array, and it will stay lazy.</p>"},{"location":"Lecture_9_Dask_Parallel_Computing/#dask-xarray","title":"Dask + Xarray\u00b6","text":""},{"location":"Lecture_9_Dask_Parallel_Computing/#extracting-underlying-data","title":"Extracting underlying data\u00b6","text":"<p>There are two ways to pull out the underlying array object in an xarray object.</p> <ol> <li><code>.to_numpy</code> or <code>.values</code> will always return a NumPy array. For dask-backed xarray objects, this means that compute will always be called</li> <li><code>.data</code> will return a Dask array</li> </ol> <pre><code>{tip}\nUse `to_numpy` or `as_numpy` instead of `.values` so that your code generalizes to other array types (like CuPy arrays, sparse arrays)\n</code></pre>"},{"location":"Lecture_9_Dask_Parallel_Computing/#lazy-computation","title":"Lazy computation\u00b6","text":"<p>Xarray seamlessly wraps dask so all computation is deferred until explicitly requested.</p>"},{"location":"Lecture_9_Dask_Parallel_Computing/#getting-concrete-values","title":"Getting concrete values\u00b6","text":"<p>At some point, you will want to actually get concrete values (usually a numpy array) from dask.</p> <p>There are two ways to compute values on dask arrays.</p> <ol> <li><code>.compute()</code> returns an xarray object just like a dask array</li> <li><code>.load()</code> replaces the dask array in the xarray object with a numpy array. This is equivalent to <code>ds = ds.compute()</code></li> </ol>"},{"location":"Lecture_9_Dask_Parallel_Computing/#reading-multi-file-dataset-with-dask","title":"Reading multi-file dataset with dask\u00b6","text":""},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/","title":"MetPy","text":"<p>Before installation, make sure:</p> MetPy currently supports Python &gt;= 3.8, by 11/20/2023  Need the following dependencies: - \"matplotlib&gt;=3.3.0\", - \"numpy&gt;=1.18.0\", - \"pandas&gt;=1.0.0\", - \"pint&gt;=0.15\", - \"pooch&gt;=1.2.0\", - \"pyproj&gt;=2.6.1\", - \"scipy&gt;=1.4.0\", - \"traitlets&gt;=5.0.5\", - \"xarray&gt;=0.18.0\"  mamba install metpyconda install metpy  conda install -c conda-forge metpy  In\u00a0[124]: Copied! <pre>import matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n\nimport numpy as np\nfrom metpy.plots import SkewT\nfrom metpy.units import units\nfrom metpy.calc import dewpoint_from_relative_humidity ## old_name: dewpoint_rh\n\nfrom metpy.calc import reduce_point_density\nfrom metpy.plots import add_metpy_logo, current_weather, sky_cover, StationPlot\n\n# Set up matplotlib to display plots in the notebook\n%matplotlib inline\n</pre> import matplotlib.pyplot as plt import cartopy.crs as ccrs import cartopy.feature as cfeature  import numpy as np from metpy.plots import SkewT from metpy.units import units from metpy.calc import dewpoint_from_relative_humidity ## old_name: dewpoint_rh  from metpy.calc import reduce_point_density from metpy.plots import add_metpy_logo, current_weather, sky_cover, StationPlot  # Set up matplotlib to display plots in the notebook %matplotlib inline In\u00a0[17]: Copied! <pre># First way to attach unit-multiply the units\n</pre> # First way to attach unit-multiply the units In\u00a0[18]: Copied! <pre>distance = np.arange(1, 5)\ndistance\n</pre> distance = np.arange(1, 5) distance Out[18]: <pre>array([1, 2, 3, 4])</pre> In\u00a0[19]: Copied! <pre>distance = np.arange(1, 5) * units.meters\ndistance\n</pre> distance = np.arange(1, 5) * units.meters distance Out[19]: Magnitude<pre>[1 2 3 4]</pre>Unitsmeter In\u00a0[20]: Copied! <pre># Another way to attach units is to create the array directly with the pint.Quantity() object:\n</pre> # Another way to attach units is to create the array directly with the pint.Quantity() object: In\u00a0[22]: Copied! <pre>time = units.Quantity(np.arange(2, 10, 2), 'sec')\ntime\n</pre> time = units.Quantity(np.arange(2, 10, 2), 'sec') time Out[22]: Magnitude<pre>[2 4 6 8]</pre>Unitssecond In\u00a0[25]: Copied! <pre>velocity = distance/time\nvelocity\n</pre> velocity = distance/time velocity Out[25]: Magnitude<pre>[0.5 0.5 0.5 0.5]</pre>Unitsmeter/second In\u00a0[109]: Copied! <pre># Pressure and Temperature data\npressure_levels = np.array([1000, 850, 700, 600, 500, 300, 200]) * units.hPa\ntemperature = np.array([30, 20, 10, 0, -10, -30, -40]) * units.degC\ndewpoint = dewpoint_from_relative_humidity(temperature, np.array([70, 100, 80, 60, 100, 50, 40]) * units.percent)\n</pre> # Pressure and Temperature data pressure_levels = np.array([1000, 850, 700, 600, 500, 300, 200]) * units.hPa temperature = np.array([30, 20, 10, 0, -10, -30, -40]) * units.degC dewpoint = dewpoint_from_relative_humidity(temperature, np.array([70, 100, 80, 60, 100, 50, 40]) * units.percent) In\u00a0[110]: Copied! <pre># Convert temperature and dewpoint to Kelvin for MetPy\ntemperature = temperature.to(units.K)\ndewpoint = dewpoint.to(units.K)\n</pre> # Convert temperature and dewpoint to Kelvin for MetPy temperature = temperature.to(units.K) dewpoint = dewpoint.to(units.K) In\u00a0[116]: Copied! <pre># Define Wind\nu = np.linspace(-10, 10, len(pressure_levels)) * units.knots\nv = np.array([-20, -20, -30, 5, 10, 20, 20]) * units.knots\n</pre> # Define Wind u = np.linspace(-10, 10, len(pressure_levels)) * units.knots v = np.array([-20, -20, -30, 5, 10, 20, 20]) * units.knots In\u00a0[119]: Copied! <pre># Create a skew-T log-P plot\nfig = plt.figure(figsize=(12, 9))\nskew = SkewT(fig, rotation=45)\n\n# Plot temperature and dewpoint profiles\nskew.plot(pressure_levels, temperature, label='Temperature', color='r', linewidth=2.5)\nskew.plot(pressure_levels, dewpoint, label='Dewpoint', color='g', linewidth=2.5) # linestyle='dashed'\n\n# Add wind barbs\nskew.plot_barbs(pressure_levels, u, v)\n\n# Add other plot features\nskew.ax.set_xlim(-40, 40)\nskew.ax.set_ylim(1000, 100)\nskew.ax.set_title('Skew-T Log-P Diagram')\nskew.ax.set_xlabel('Temperature (\u00b0C)')\nskew.ax.set_ylabel('Pressure (hPa)')\n\n# Add the relevant special lines\n# skew.plot_dry_adiabats()\n# skew.plot_moist_adiabats()\n# skew.plot_mixing_lines()\n\n# Set x, y limits\nskew.ax.set_xlim(-30, 40)\nskew.ax.set_ylim(1000, 100)\n\n# Add legend\nskew.ax.legend()\n\n# Show the plot\nplt.show()\n</pre> # Create a skew-T log-P plot fig = plt.figure(figsize=(12, 9)) skew = SkewT(fig, rotation=45)  # Plot temperature and dewpoint profiles skew.plot(pressure_levels, temperature, label='Temperature', color='r', linewidth=2.5) skew.plot(pressure_levels, dewpoint, label='Dewpoint', color='g', linewidth=2.5) # linestyle='dashed'  # Add wind barbs skew.plot_barbs(pressure_levels, u, v)  # Add other plot features skew.ax.set_xlim(-40, 40) skew.ax.set_ylim(1000, 100) skew.ax.set_title('Skew-T Log-P Diagram') skew.ax.set_xlabel('Temperature (\u00b0C)') skew.ax.set_ylabel('Pressure (hPa)')  # Add the relevant special lines # skew.plot_dry_adiabats() # skew.plot_moist_adiabats() # skew.plot_mixing_lines()  # Set x, y limits skew.ax.set_xlim(-30, 40) skew.ax.set_ylim(1000, 100)  # Add legend skew.ax.legend()  # Show the plot plt.show() In\u00a0[120]: Copied! <pre>import metpy.calc as mpcalc\n\nfig = plt.figure(figsize=(9, 9))\nskew = SkewT(fig)\n\n# Create arrays of pressure, temperature, dewpoint, and wind components\np = [902, 897, 893, 889, 883, 874, 866, 857, 849, 841, 833, 824, 812, 796, 776, 751,\n     727, 704, 680, 656, 629, 597, 565, 533, 501, 468, 435, 401, 366, 331, 295, 258,\n     220, 182, 144, 106] * units.hPa\nt = [-3, -3.7, -4.1, -4.5, -5.1, -5.8, -6.5, -7.2, -7.9, -8.6, -8.9, -7.6, -6, -5.1,\n     -5.2, -5.6, -5.4, -4.9, -5.2, -6.3, -8.4, -11.5, -14.9, -18.4, -21.9, -25.4,\n     -28, -32, -37, -43, -49, -54, -56, -57, -58, -60] * units.degC\ntd = [-22, -22.1, -22.2, -22.3, -22.4, -22.5, -22.6, -22.7, -22.8, -22.9, -22.4,\n      -21.6, -21.6, -21.9, -23.6, -27.1, -31, -38, -44, -46, -43, -37, -34, -36,\n      -42, -46, -49, -48, -47, -49, -55, -63, -72, -88, -93, -92] * units.degC\nu = np.linspace(-10, 10, len(p)) * units.knots\nv = np.linspace(-20, 20, len(p)) * units.knots\n\n# Calculate parcel profile\nprof = mpcalc.parcel_profile(p, t[0], td[0]).to('degC')\n\nskew.plot(p, t, 'r')\nskew.plot(p, td, 'g')\nskew.plot(p, prof, 'k')  # Plot parcel profile\nskew.plot_barbs(p[::5], u[::5], v[::5])\n\nskew.ax.set_xlim(-70, 15)\nskew.ax.set_ylim(1000, 100)\n\n# Add the relevant special lines\nskew.plot_dry_adiabats()\nskew.plot_moist_adiabats()\nskew.plot_mixing_lines()\n\nplt.show()\n</pre> import metpy.calc as mpcalc  fig = plt.figure(figsize=(9, 9)) skew = SkewT(fig)  # Create arrays of pressure, temperature, dewpoint, and wind components p = [902, 897, 893, 889, 883, 874, 866, 857, 849, 841, 833, 824, 812, 796, 776, 751,      727, 704, 680, 656, 629, 597, 565, 533, 501, 468, 435, 401, 366, 331, 295, 258,      220, 182, 144, 106] * units.hPa t = [-3, -3.7, -4.1, -4.5, -5.1, -5.8, -6.5, -7.2, -7.9, -8.6, -8.9, -7.6, -6, -5.1,      -5.2, -5.6, -5.4, -4.9, -5.2, -6.3, -8.4, -11.5, -14.9, -18.4, -21.9, -25.4,      -28, -32, -37, -43, -49, -54, -56, -57, -58, -60] * units.degC td = [-22, -22.1, -22.2, -22.3, -22.4, -22.5, -22.6, -22.7, -22.8, -22.9, -22.4,       -21.6, -21.6, -21.9, -23.6, -27.1, -31, -38, -44, -46, -43, -37, -34, -36,       -42, -46, -49, -48, -47, -49, -55, -63, -72, -88, -93, -92] * units.degC u = np.linspace(-10, 10, len(p)) * units.knots v = np.linspace(-20, 20, len(p)) * units.knots  # Calculate parcel profile prof = mpcalc.parcel_profile(p, t[0], td[0]).to('degC')  skew.plot(p, t, 'r') skew.plot(p, td, 'g') skew.plot(p, prof, 'k')  # Plot parcel profile skew.plot_barbs(p[::5], u[::5], v[::5])  skew.ax.set_xlim(-70, 15) skew.ax.set_ylim(1000, 100)  # Add the relevant special lines skew.plot_dry_adiabats() skew.plot_moist_adiabats() skew.plot_mixing_lines()  plt.show() In\u00a0[126]: Copied! <pre># Import tutorial data packages\nfrom metpy.cbook import get_test_data\nfrom metpy.io import metar\n</pre> # Import tutorial data packages from metpy.cbook import get_test_data from metpy.io import metar In\u00a0[125]: Copied! <pre>data = metar.parse_metar_file(get_test_data('metar_20190701_1200.txt', as_file_obj=False))\n\n# Drop rows with missing winds\ndata = data.dropna(how='any', subset=['wind_direction', 'wind_speed'])\n</pre> data = metar.parse_metar_file(get_test_data('metar_20190701_1200.txt', as_file_obj=False))  # Drop rows with missing winds data = data.dropna(how='any', subset=['wind_direction', 'wind_speed']) <pre>Downloading file 'metar_20190701_1200.txt' from 'https://github.com/Unidata/MetPy/raw/v1.5.1/staticdata/metar_20190701_1200.txt' to '/home/sw1225/.cache/metpy/v1.5.1'.\nDownloading file 'sfstns.tbl' from 'https://github.com/Unidata/MetPy/raw/v1.5.1/staticdata/sfstns.tbl' to '/home/sw1225/.cache/metpy/v1.5.1'.\nDownloading file 'master.txt' from 'https://github.com/Unidata/MetPy/raw/v1.5.1/staticdata/master.txt' to '/home/sw1225/.cache/metpy/v1.5.1'.\nDownloading file 'stations.txt' from 'https://github.com/Unidata/MetPy/raw/v1.5.1/staticdata/stations.txt' to '/home/sw1225/.cache/metpy/v1.5.1'.\nDownloading file 'airport-codes.csv' from 'https://github.com/Unidata/MetPy/raw/v1.5.1/staticdata/airport-codes.csv' to '/home/sw1225/.cache/metpy/v1.5.1'.\n</pre> <p>This sample data has way too many stations to plot all of them. The number of stations plotted will be reduced using <code>reduce_point_density</code>.</p> In\u00a0[127]: Copied! <pre># Set up the map projection\nproj = ccrs.LambertConformal(central_longitude=-95, central_latitude=35,\n                             standard_parallels=[35])\n\n# Use the Cartopy map projection to transform station locations to the map and\n# then refine the number of stations plotted by setting a 300km radius\npoint_locs = proj.transform_points(ccrs.PlateCarree(), data['longitude'].values,\n                                   data['latitude'].values)\ndata = data[reduce_point_density(point_locs, 300000.)]\n</pre> # Set up the map projection proj = ccrs.LambertConformal(central_longitude=-95, central_latitude=35,                              standard_parallels=[35])  # Use the Cartopy map projection to transform station locations to the map and # then refine the number of stations plotted by setting a 300km radius point_locs = proj.transform_points(ccrs.PlateCarree(), data['longitude'].values,                                    data['latitude'].values) data = data[reduce_point_density(point_locs, 300000.)] In\u00a0[129]: Copied! <pre># Change the DPI of the resulting figure. Higher DPI drastically improves the\n# look of the text rendering.\nplt.rcParams['savefig.dpi'] = 255\n\n# Create the figure and an axes set to the projection.\nfig = plt.figure(figsize=(20, 10))\nadd_metpy_logo(fig, 1100, 300, size='large')\nax = fig.add_subplot(1, 1, 1, projection=proj)\n\n# Add some various map elements to the plot to make it recognizable.\nax.add_feature(cfeature.LAND)\nax.add_feature(cfeature.OCEAN)\nax.add_feature(cfeature.LAKES)\nax.add_feature(cfeature.COASTLINE)\nax.add_feature(cfeature.STATES)\nax.add_feature(cfeature.BORDERS)\n\n# Set plot bounds\nax.set_extent((-118, -73, 23, 50))\n\n#\n# Here's the actual station plot\n#\n\n# Start the station plot by specifying the axes to draw on, as well as the\n# lon/lat of the stations (with transform). We also the fontsize to 12 pt.\nstationplot = StationPlot(ax, data['longitude'].values, data['latitude'].values,\n                          clip_on=True, transform=ccrs.PlateCarree(), fontsize=12)\n\n# Plot the temperature and dew point to the upper and lower left, respectively, of\n# the center point. Each one uses a different color.\nstationplot.plot_parameter('NW', data['air_temperature'].values, color='red')\nstationplot.plot_parameter('SW', data['dew_point_temperature'].values,\n                           color='darkgreen')\n\n# A more complex example uses a custom formatter to control how the sea-level pressure\n# values are plotted. This uses the standard trailing 3-digits of the pressure value\n# in tenths of millibars.\nstationplot.plot_parameter('NE', data['air_pressure_at_sea_level'].values,\n                           formatter=lambda v: format(10 * v, '.0f')[-3:])\n\n# Plot the cloud cover symbols in the center location. This uses the codes made above and\n# uses the `sky_cover` mapper to convert these values to font codes for the\n# weather symbol font.\nstationplot.plot_symbol('C', data['cloud_coverage'].values, sky_cover)\n\n# Same this time, but plot current weather to the left of center, using the\n# `current_weather` mapper to convert symbols to the right glyphs.\nstationplot.plot_symbol('W', data['current_wx1_symbol'].values, current_weather)\n\n# Add wind barbs\nstationplot.plot_barb(data['eastward_wind'].values, data['northward_wind'].values)\n\n# Also plot the actual text of the station id. Instead of cardinal directions,\n# plot further out by specifying a location of 2 increments in x and 0 in y.\nstationplot.plot_text((2, 0), data['station_id'].values)\n\nplt.show()\n</pre> # Change the DPI of the resulting figure. Higher DPI drastically improves the # look of the text rendering. plt.rcParams['savefig.dpi'] = 255  # Create the figure and an axes set to the projection. fig = plt.figure(figsize=(20, 10)) add_metpy_logo(fig, 1100, 300, size='large') ax = fig.add_subplot(1, 1, 1, projection=proj)  # Add some various map elements to the plot to make it recognizable. ax.add_feature(cfeature.LAND) ax.add_feature(cfeature.OCEAN) ax.add_feature(cfeature.LAKES) ax.add_feature(cfeature.COASTLINE) ax.add_feature(cfeature.STATES) ax.add_feature(cfeature.BORDERS)  # Set plot bounds ax.set_extent((-118, -73, 23, 50))  # # Here's the actual station plot #  # Start the station plot by specifying the axes to draw on, as well as the # lon/lat of the stations (with transform). We also the fontsize to 12 pt. stationplot = StationPlot(ax, data['longitude'].values, data['latitude'].values,                           clip_on=True, transform=ccrs.PlateCarree(), fontsize=12)  # Plot the temperature and dew point to the upper and lower left, respectively, of # the center point. Each one uses a different color. stationplot.plot_parameter('NW', data['air_temperature'].values, color='red') stationplot.plot_parameter('SW', data['dew_point_temperature'].values,                            color='darkgreen')  # A more complex example uses a custom formatter to control how the sea-level pressure # values are plotted. This uses the standard trailing 3-digits of the pressure value # in tenths of millibars. stationplot.plot_parameter('NE', data['air_pressure_at_sea_level'].values,                            formatter=lambda v: format(10 * v, '.0f')[-3:])  # Plot the cloud cover symbols in the center location. This uses the codes made above and # uses the `sky_cover` mapper to convert these values to font codes for the # weather symbol font. stationplot.plot_symbol('C', data['cloud_coverage'].values, sky_cover)  # Same this time, but plot current weather to the left of center, using the # `current_weather` mapper to convert symbols to the right glyphs. stationplot.plot_symbol('W', data['current_wx1_symbol'].values, current_weather)  # Add wind barbs stationplot.plot_barb(data['eastward_wind'].values, data['northward_wind'].values)  # Also plot the actual text of the station id. Instead of cardinal directions, # plot further out by specifying a location of 2 increments in x and 0 in y. stationplot.plot_text((2, 0), data['station_id'].values)  plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#metpy","title":"MetPy\u00b6","text":"<p>Tools that you either love or hate</p>"},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#by-siyi-wang-2023-nov-20th","title":"by Siyi Wang, 2023 Nov 20th\u00b6","text":""},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#1-introduction","title":"1. Introduction\u00b6","text":"<ul> <li><p>Meteorological Calculation: Built-in functions that calculate special physical quantities (q, lapse_rate) from basic physical quantities (T, RH, P, Wind) using well-documented equations. It allows to attach units before calculation.</p> </li> <li><p>Visualize meteorology variables: Pressure, Temperature, Dew Point, Wind Direction and Wind Speed etc..</p> </li> <li><p>Compatible with data in various formats: .nc, .csv, .hdf, .las from multiple data sources: Models, Observations, LIDAR, Satellite</p> </li> <li><p>Built-in functions to convert units painlessly: C-F-K, sigma-mb/hpa</p> </li> <li><p>Play around projections easily: Regional, Global, Polar, Half Hemisphere.</p> </li> </ul> <p>MetPy Official User Guide</p> <p>MetPy Official Tutorial-MetPy Monday</p>"},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#2-installing-packages","title":"2. Installing packages\u00b6","text":""},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#install-using-condamamba","title":"Install using conda/mamba\u00b6","text":""},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#intall-using-conda-forge","title":"Intall using conda-forge\u00b6","text":""},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#import-metpy-and-other-necessary-packages","title":"Import MetPy and other necessary packages\u00b6","text":""},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#using-metpy","title":"Using MetPy\u00b6","text":""},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#units-syntax","title":"Units Syntax\u00b6","text":""},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#unit-aware-calculations","title":"Unit-aware Calculations\u00b6","text":""},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#create-a-skew-t-log-p-plot","title":"Create a Skew-T log-P plot\u00b6","text":"<p>Visualizing the first type of special meteorological data, sounding data</p>"},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#lets-start-by-creating-some-sample-data-you-can-replace-this-with-your-own-data-later","title":"Let's start by creating some sample data. You can replace this with your own data later.\u00b6","text":""},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#plotting-using-skewt-function","title":"Plotting using SkewT() function\u00b6","text":""},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#for-more-complex-graphs-adding-atmospheric-profile-and-special-relevant-lines","title":"For more complex graphs - adding atmospheric profile and special relevant lines\u00b6","text":""},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#create-a-station-plot","title":"Create a Station Plot\u00b6","text":"<p>Visualizing the second type of special meteorological data, on-site observation</p> <p>Features: Scattered, not regularly distributed grid data</p>"},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#get-data","title":"Get Data\u00b6","text":""},{"location":"MetPy-Tutorial_2023Nov_SiyiWang/#plot-reduce-the-number-of-points","title":"Plot + reduce the number of points\u00b6","text":""},{"location":"MundyPyTesmo/","title":"ISMN (International Soil Moisture Network)","text":"In\u00a0[100]: Copied! <pre>pip install ismn\n</pre> pip install ismn  <pre>Requirement already satisfied: ismn in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (1.3.4)\nRequirement already satisfied: pygeogrids&gt;=0.3.2 in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (from ismn) (0.4.2)\nRequirement already satisfied: numpy in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (from ismn) (1.26.0)\nRequirement already satisfied: pandas in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (from ismn) (2.1.2)\nRequirement already satisfied: configparser in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (from ismn) (6.0.0)\nRequirement already satisfied: more-itertools in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (from ismn) (10.1.0)\nRequirement already satisfied: tqdm in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (from ismn) (4.66.1)\nRequirement already satisfied: netCDF4 in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (from pygeogrids&gt;=0.3.2-&gt;ismn) (1.6.5)\nRequirement already satisfied: pyproj in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (from pygeogrids&gt;=0.3.2-&gt;ismn) (3.6.1)\nRequirement already satisfied: pykdtree in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (from pygeogrids&gt;=0.3.2-&gt;ismn) (1.3.9)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (from pandas-&gt;ismn) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (from pandas-&gt;ismn) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (from pandas-&gt;ismn) (2023.3)\nRequirement already satisfied: six&gt;=1.5 in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;ismn) (1.16.0)\nRequirement already satisfied: cftime in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (from netCDF4-&gt;pygeogrids&gt;=0.3.2-&gt;ismn) (1.6.3)\nRequirement already satisfied: certifi in ./miniforge3/envs/rcaes_env_new/lib/python3.9/site-packages (from netCDF4-&gt;pygeogrids&gt;=0.3.2-&gt;ismn) (2023.11.17)\nNote: you may need to restart the kernel to use updated packages.\n</pre> <p>Impacts ecosystem functions and processes: agriculture, flooding, water resources, runoff, weather and climate, carbon cycling, etc.</p> <p>U.S. Climate Reference Network (USCRN)</p> In\u00a0[179]: Copied! <pre>from ismn.interface import ISMN_Interface\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport cartopy.crs as ccrs\n</pre> from ismn.interface import ISMN_Interface import numpy as np import matplotlib.pyplot as plt  import cartopy.crs as ccrs In\u00a0[104]: Copied! <pre>path = 'Data_separate_files_header_20131122_20231122_10739_4gp7_20231123.zip'\nismn_data = ISMN_Interface(path)\n</pre> path = 'Data_separate_files_header_20131122_20231122_10739_4gp7_20231123.zip' ismn_data = ISMN_Interface(path) <pre>Found existing ismn metadata in python_metadata/Data_separate_files_header_20131122_20231122_10739_4gp7_20231123.csv.\n</pre> In\u00a0[116]: Copied! <pre>ismn_data[10]\n</pre> ismn_data[10] Out[116]: <pre>Network 'USCRN' with Stations: ['Aberdeen-35-WNW', 'Arco-17-SW', 'Asheville-13-S', 'Asheville-8-SSW', 'Austin-33-NW', 'Avondale-2-N', 'Baker-5-W', 'Batesville-8-WNW', 'Bedford-5-WNW', 'Blackville-3-W', 'Bodega-6-WSW', 'Boulder-14-W', 'Bowling-Green-21-NNE', 'Brigham-City-28-WNW', 'Bronte-11-NNE', 'Brunswick-23-S', 'Buffalo-13-ESE', 'Cape-Charles-5-ENE', 'Champaign-9-SW', 'Charlottesville-2-SSE', 'Chatham-1-SE', 'Chillicothe-22-ENE', 'Coos-Bay-8-SW', 'Cortez-8-SE', 'Corvallis-10-SSW', 'Coshocton-8-NNE', 'Crossville-7-NW', 'Darrington-21-NNE', 'Denio-52-WSW', 'Des-Moines-17-E', 'Dillon-18-WSW', 'Dinosaur-2-E', 'Durham-11-W', 'Durham-2-N', 'Durham-2-SSW', 'Edinburg-17-NNE', 'Elgin-5-S', 'Elkins-21-ENE', 'Everglades-City-5-NE', 'Fairhope-3-NE', 'Fallbrook-5-NE', 'Gadsden-19-N', 'Gaylord-9-SSW', 'Goodridge-12-NNW', 'Goodwell-2-E', 'Goodwell-2-SE', 'Harrison-20-SSE', 'Holly-Springs-4-N', 'Ithaca-13-E', 'Jamestown-38-WSW', 'John-Day-35-WNW', 'Joplin-24-N', 'Kenai-29-ENE', 'Kingston-1-NW', 'Kingston-1-W', 'La-Junta-17-WSW', 'Lafayette-13-SE', 'Lander-11-SSE', 'Las-Cruces-20-N', 'Lewistown-42-WSW', 'Limestone-4-NNW', 'Lincoln-11-SW', 'Lincoln-8-ENE', 'Los-Alamos-13-W', 'Manhattan-6-SSW', 'McClellanville-7-NE', 'Medora-7-E', 'Merced-23-WSW', 'Mercury-3-SSW', 'Millbrook-3-W', 'Monahans-6-ENE', 'Monroe-26-N', 'Montrose-11-ENE', 'Moose-1-NNE', 'Muleshoe-19-S', 'Murphy-10-W', 'Necedah-5-WNW', 'Newton-11-SW', 'Newton-5-ENE', 'Newton-8-W', 'Northgate-5-ESE', 'Nunn-7-NNE', 'Oakley-19-SSW', 'Old-Town-2-W', 'Palestine-6-WNW', 'Panther-Junction-2-N', 'Pierre-24-S', 'Port-Aransas-32-NNE', 'Quinault-4-NE', 'Redding-12-WNW', 'Riley-10-WSW', 'Salem-10-W', 'Sandstone-6-W', 'Santa-Barbara-11-W', 'Sebring-23-SSE', 'Selma-13-WNW', 'Shabbona-5-NNE', 'Sioux-Falls-14-NNE', 'Socorro-20-N', 'Spokane-17-SSW', 'St.-Mary-1-SSW', 'Stillwater-2-W', 'Stillwater-5-WNW', 'Stovepipe-Wells-1-SW', 'Sundance-8-NNW', 'Titusville-7-E', 'Tucson-11-W', 'Versailles-3-NNW', 'Watkinsville-5-SSE', 'Whitman-5-ENE', 'Williams-35-NNW', 'Wolf-Point-29-ENE', 'Wolf-Point-34-NE', 'Yosemite-Village-12-W', 'Yuma-27-ENE']</pre> In\u00a0[27]: Copied! <pre>ismn_data['USCRN']['Millbrook-3-W']\n</pre> ismn_data['USCRN']['Millbrook-3-W'] Out[27]: <pre>Station 'Millbrook-3-W' with Sensors: ['Stevens-Hydraprobe-II-Sdi-12_soil_moisture_0.050000_0.050000', 'Stevens-Hydraprobe-II-Sdi-12_soil_moisture_0.100000_0.100000', 'Stevens-Hydraprobe-II-Sdi-12_soil_moisture_0.200000_0.200000', 'Stevens-Hydraprobe-II-Sdi-12_soil_moisture_0.500000_0.500000', 'Stevens-Hydraprobe-II-Sdi-12_soil_moisture_1.000000_1.000000']</pre> In\u00a0[28]: Copied! <pre>ismn_data['USCRN']['Millbrook-3-W'].metadata\n</pre> ismn_data['USCRN']['Millbrook-3-W'].metadata Out[28]: <pre>MetaData([\n  MetaVar([clay_fraction, 20.0, Depth([0.0, 0.3])]),\n  MetaVar([climate_KG, Dfb, None]),\n  MetaVar([climate_insitu, unknown, None]),\n  MetaVar([elevation, 126.0, None]),\n  MetaVar([instrument, Stevens-Hydraprobe-II-Sdi-12, Depth([0.05, 0.05])]),\n  MetaVar([latitude, 41.7857, None]),\n  MetaVar([lc_2000, 61, None]),\n  MetaVar([lc_2005, 61, None]),\n  MetaVar([lc_2010, 61, None]),\n  MetaVar([lc_insitu, unknown, None]),\n  MetaVar([longitude, -73.7422, None]),\n  MetaVar([network, USCRN, None]),\n  MetaVar([organic_carbon, 1.45, Depth([0.0, 0.3])]),\n  MetaVar([sand_fraction, 41.0, Depth([0.0, 0.3])]),\n  MetaVar([saturation, 0.46, Depth([0.0, 0.3])]),\n  MetaVar([silt_fraction, 39.0, Depth([0.0, 0.3])]),\n  MetaVar([station, Millbrook-3-W, None]),\n  MetaVar([timerange_from, 2013-11-22 00:00:00, None]),\n  MetaVar([timerange_to, 2023-11-21 22:00:00, None]),\n  MetaVar([variable, soil_moisture, Depth([0.05, 0.05])]),\n  MetaVar([instrument, Stevens-Hydraprobe-II-Sdi-12, Depth([0.1, 0.1])]),\n  MetaVar([variable, soil_moisture, Depth([0.1, 0.1])]),\n  MetaVar([instrument, Stevens-Hydraprobe-II-Sdi-12, Depth([0.2, 0.2])]),\n  MetaVar([variable, soil_moisture, Depth([0.2, 0.2])]),\n  MetaVar([clay_fraction, 20.0, Depth([0.3, 1.0])]),\n  MetaVar([instrument, Stevens-Hydraprobe-II-Sdi-12, Depth([0.5, 0.5])]),\n  MetaVar([organic_carbon, 0.5, Depth([0.3, 1.0])]),\n  MetaVar([sand_fraction, 45.0, Depth([0.3, 1.0])]),\n  MetaVar([saturation, 0.41, Depth([0.3, 1.0])]),\n  MetaVar([silt_fraction, 35.0, Depth([0.3, 1.0])]),\n  MetaVar([variable, soil_moisture, Depth([0.5, 0.5])]),\n  MetaVar([instrument, Stevens-Hydraprobe-II-Sdi-12, Depth([1.0, 1.0])]),\n  MetaVar([variable, soil_moisture, Depth([1.0, 1.0])])\n])</pre> In\u00a0[118]: Copied! <pre>ismn_data['USCRN']['Millbrook-3-W']['Stevens-Hydraprobe-II-Sdi-12_soil_moisture_0.050000_0.050000']\n</pre> ismn_data['USCRN']['Millbrook-3-W']['Stevens-Hydraprobe-II-Sdi-12_soil_moisture_0.050000_0.050000'] Out[118]: <pre>Stevens-Hydraprobe-II-Sdi-12_soil_moisture_0.050000_0.050000</pre> In\u00a0[120]: Copied! <pre>sensor = ismn_data['USCRN']['Millbrook-3-W']['Stevens-Hydraprobe-II-Sdi-12_soil_moisture_0.050000_0.050000']\nprint(sensor.metadata.to_pd())\nax = sensor.data.plot(figsize=(14,6))\nax.set_xlabel('Time [year]')\nax.set_ylabel(\"Soil Moisture\")\n</pre> sensor = ismn_data['USCRN']['Millbrook-3-W']['Stevens-Hydraprobe-II-Sdi-12_soil_moisture_0.050000_0.050000'] print(sensor.metadata.to_pd()) ax = sensor.data.plot(figsize=(14,6)) ax.set_xlabel('Time [year]') ax.set_ylabel(\"Soil Moisture\") <pre>variable        key       \nclay_fraction   val                                   20.0\n                depth_from                             0.0\n                depth_to                               0.3\nclimate_KG      val                                    Dfb\nclimate_insitu  val                                unknown\nelevation       val                                  126.0\ninstrument      val           Stevens-Hydraprobe-II-Sdi-12\n                depth_from                            0.05\n                depth_to                              0.05\nlatitude        val                                41.7857\nlc_2000         val                                     61\nlc_2005         val                                     61\nlc_2010         val                                     61\nlc_insitu       val                                unknown\nlongitude       val                               -73.7422\nnetwork         val                                  USCRN\norganic_carbon  val                                   1.45\n                depth_from                             0.0\n                depth_to                               0.3\nsand_fraction   val                                   41.0\n                depth_from                             0.0\n                depth_to                               0.3\nsaturation      val                                   0.46\n                depth_from                             0.0\n                depth_to                               0.3\nsilt_fraction   val                                   39.0\n                depth_from                             0.0\n                depth_to                               0.3\nstation         val                          Millbrook-3-W\ntimerange_from  val                    2013-11-22 00:00:00\ntimerange_to    val                    2023-11-21 22:00:00\nvariable        val                          soil_moisture\n                depth_from                            0.05\n                depth_to                              0.05\nName: data, dtype: object\n</pre> Out[120]: <pre>Text(0, 0.5, 'Soil Moisture')</pre> In\u00a0[128]: Copied! <pre>station, dist = ismn_data.collection.get_nearest_station(-74.4376005175904, 40.47736974602313)\nprint(f'Station {station.name} is {int(dist)} metres away from the ENR building:')\n</pre> station, dist = ismn_data.collection.get_nearest_station(-74.4376005175904, 40.47736974602313) print(f'Station {station.name} is {int(dist)} metres away from the ENR building:') <pre>Station NJMeadowlands is 44023 metres away from the ENR building:\n</pre> In\u00a0[135]: Copied! <pre>NJ_sensor = ismn_data['COSMOS']['NJMeadowlands']['Cosmic-ray-Probe_soil_moisture_0.000000_0.170000']\nprint(NJ_sensor.metadata.to_pd())\nax = NJ_sensor.data.plot(figsize=(14,6))\nax.set_xlabel('Time [year]')\nax.set_ylabel(\"Soil Moisture\")\n</pre> NJ_sensor = ismn_data['COSMOS']['NJMeadowlands']['Cosmic-ray-Probe_soil_moisture_0.000000_0.170000'] print(NJ_sensor.metadata.to_pd()) ax = NJ_sensor.data.plot(figsize=(14,6)) ax.set_xlabel('Time [year]') ax.set_ylabel(\"Soil Moisture\") <pre>variable        key       \nclay_fraction   val                          20.0\n                depth_from                    0.0\n                depth_to                      0.3\nclimate_KG      val                           Dfa\nclimate_insitu  val                       unknown\nelevation       val                           3.0\ninstrument      val              Cosmic-ray-Probe\n                depth_from                    0.0\n                depth_to                     0.17\nlatitude        val                       40.7691\nlc_2000         val                           180\nlc_2005         val                           180\nlc_2010         val                           180\nlc_insitu       val                       unknown\nlongitude       val                      -74.0853\nnetwork         val                        COSMOS\norganic_carbon  val                          1.45\n                depth_from                    0.0\n                depth_to                      0.3\nsand_fraction   val                          41.0\n                depth_from                    0.0\n                depth_to                      0.3\nsaturation      val                          0.46\n                depth_from                    0.0\n                depth_to                      0.3\nsilt_fraction   val                          39.0\n                depth_from                    0.0\n                depth_to                      0.3\nstation         val                 NJMeadowlands\ntimerange_from  val           2014-10-16 22:00:00\ntimerange_to    val           2015-08-04 13:00:00\nvariable        val                 soil_moisture\n                depth_from                    0.0\n                depth_to                     0.17\nName: data, dtype: object\n</pre> Out[135]: <pre>Text(0, 0.5, 'Soil Moisture')</pre> In\u00a0[149]: Copied! <pre>ismn_data.metadata\n</pre> ismn_data.metadata Out[149]: variable clay_fraction climate_KG climate_insitu elevation ... timerange_from timerange_to variable file_path file_type key depth_from depth_to val depth_from depth_to val depth_from depth_to val depth_from ... depth_to val depth_from depth_to val depth_from depth_to val val val 0 0.00 0.30 23.0 NaN NaN Cfa NaN NaN unknown NaN ... NaN 2013-11-22 00:00:00 NaN NaN 2021-09-14 16:00:00 0.025 0.025 soil_moisture ARM/Anthony/ARM_ARM_Anthony_sm_0.025000_0.0250... header_values 1 0.00 0.30 23.0 NaN NaN Cfa NaN NaN unknown NaN ... NaN 2013-11-22 00:00:00 NaN NaN 2021-09-14 16:00:00 0.025 0.025 soil_moisture ARM/Anthony/ARM_ARM_Anthony_sm_0.025000_0.0250... header_values 2 0.00 0.30 23.0 NaN NaN Cfa NaN NaN unknown NaN ... NaN 2013-11-22 00:00:00 NaN NaN 2021-09-14 16:00:00 0.025 0.025 soil_moisture ARM/Anthony/ARM_ARM_Anthony_sm_0.025000_0.0250... header_values 3 0.00 0.30 23.0 NaN NaN Cfa NaN NaN unknown NaN ... NaN 2016-03-07 18:00:00 NaN NaN 2021-09-21 12:00:00 0.050 0.050 soil_moisture ARM/Anthony/ARM_ARM_Anthony_sm_0.050000_0.0500... header_values 4 0.00 0.30 23.0 NaN NaN Cfa NaN NaN unknown NaN ... NaN 2016-03-07 18:00:00 NaN NaN 2021-09-21 12:00:00 0.100 0.100 soil_moisture ARM/Anthony/ARM_ARM_Anthony_sm_0.100000_0.1000... header_values ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 4748 0.05 0.05 30.0 NaN NaN Dfb NaN NaN unknown NaN ... NaN 2016-08-31 15:00:00 NaN NaN 2021-01-01 07:00:00 0.050 0.050 soil_moisture iRON/SpringValley/iRON_iRON_SpringValley_sm_0.... header_values 4749 0.20 0.20 44.0 NaN NaN Dfb NaN NaN unknown NaN ... NaN 2016-06-08 19:00:00 NaN NaN 2020-11-01 21:00:00 0.200 0.200 soil_moisture iRON/SpringValley/iRON_iRON_SpringValley_sm_0.... header_values 4750 0.20 0.20 44.0 NaN NaN Dfb NaN NaN unknown NaN ... NaN 2016-08-31 15:00:00 NaN NaN 2021-01-01 07:00:00 0.200 0.200 soil_moisture iRON/SpringValley/iRON_iRON_SpringValley_sm_0.... header_values 4751 0.51 0.51 48.0 NaN NaN Dfb NaN NaN unknown NaN ... NaN 2016-06-08 19:00:00 NaN NaN 2021-01-01 07:00:00 0.510 0.510 soil_moisture iRON/SpringValley/iRON_iRON_SpringValley_sm_0.... header_values 4752 0.51 0.51 48.0 NaN NaN Dfb NaN NaN unknown NaN ... NaN 2016-08-31 15:00:00 NaN NaN 2021-01-01 07:00:00 0.510 0.510 soil_moisture iRON/SpringValley/iRON_iRON_SpringValley_sm_0.... header_values <p>4753 rows \u00d7 62 columns</p> In\u00a0[180]: Copied! <pre>ismn_data.print_climate_dict()\n</pre> ismn_data.print_climate_dict() <pre>KOEPPEN GEIGER Climate Classification\n-------------------------------------\nAf  : Tropical Rainforest\nAm  : Tropical Monsoon\nAs  : Tropical Savanna Dry\nAw  : Tropical Savanna Wet\nBWk : Arid Desert Cold\nBWh : Arid Desert Hot\nBWn : Arid Desert With Frequent Fog\nBSk : Arid Steppe Cold\nBSh : Arid Steppe Hot\nBSn : Arid Steppe With Frequent Fog\nCsa : Temperate Dry Hot Summer\nCsb : Temperate Dry Warm Summer\nCsc : Temperate Dry Cold Summer\nCwa : Temperate Dry Winter, Hot Summer\nCwb : Temperate Dry Winter, Warm Summer\nCwc : Temperate Dry Winter, Cold Summer\nCfa : Temperate Without Dry Season, Hot Summer\nCfb : Temperate Without Dry Season, Warm Summer\nCfc : Temperate Without Dry Season, Cold Summer\nDsa : Cold Dry Summer, Hot Summer\nDsb : Cold Dry Summer, Warm Summer\nDsc : Cold Dry Summer, Cold Summer\nDsd : Cold Dry Summer, Very Cold Winter\nDwa : Cold Dry Winter, Hot Summer\nDwb : Cold Dry Winter, Warm Summer\nDwc : Cold Dry Winter, Cold Summer\nDwd : Cold Dry Winter, Very Cold Winter\nDfa : Cold Dry Without Dry Season, Hot Summer\nDfb : Cold Dry Without Dry Season, Warm Summer\nDfc : Cold Dry Without Dry Season, Cold Summer\nDfd : Cold Dry Without Dry Season, Very Cold Winter\nET  : Polar Tundra\nEF  : Polar Eternal Winter\nW   : Water\n</pre> In\u00a0[181]: Copied! <pre>ids = ismn_data.get_dataset_ids(variable='soil_moisture',\n                                max_depth=1,\n                                filter_meta_dict={'lc_2010': 130, 'climate_KG': 'Csb'}) ## K\u00f6ppen-Geiger (KG) climate classification ## land cover (lc)\n\nids\n</pre> ids = ismn_data.get_dataset_ids(variable='soil_moisture',                                 max_depth=1,                                 filter_meta_dict={'lc_2010': 130, 'climate_KG': 'Csb'}) ## K\u00f6ppen-Geiger (KG) climate classification ## land cover (lc)  ids Out[181]: <pre>[402,\n 434,\n 440,\n 459,\n 482,\n 488,\n 496,\n 503,\n 520,\n 528,\n 2952,\n 2953,\n 2954,\n 3047,\n 3048,\n 3049,\n 3050,\n 4294,\n 4295,\n 4296,\n 4297,\n 4298]</pre> In\u00a0[182]: Copied! <pre>ts, meta = ismn_data.read(ids, return_meta=True)\nax = ts.plot(figsize=(12,4), title=f'Time series for IDs', xlabel=\"Time [year]\", ylabel=\"Soil Moisture\")\n</pre> ts, meta = ismn_data.read(ids, return_meta=True) ax = ts.plot(figsize=(12,4), title=f'Time series for IDs', xlabel=\"Time [year]\", ylabel=\"Soil Moisture\") In\u00a0[245]: Copied! <pre>metadata = ismn_data.metadata[ismn_data.metadata[('variable', 'val')] == 'soil_moisture']\nnp.random.seed(123)\nids = np.random.choice(metadata.index, 15)\nsubset = ismn_data.subset_from_ids(ids)\nsubset\n</pre> metadata = ismn_data.metadata[ismn_data.metadata[('variable', 'val')] == 'soil_moisture'] np.random.seed(123) ids = np.random.choice(metadata.index, 15) subset = ismn_data.subset_from_ids(ids) subset <pre>Found existing ismn metadata in python_metadata/Data_separate_files_header_20131122_20231122_10739_4gp7_20231123.csv.\n</pre> Out[245]: <pre>ismn.base.IsmnRoot Zip at Data_separate_files_header_20131122_20231122_10739_4gp7_20231123.zip\nwith Networks[Stations]:\n------------------------\n  SNOTEL: ['VERNONCREEK', 'SilverCreekNv', 'TAYLORBUTTE', 'LEAVITTMEADOWS', 'SENTINELBUTTE'],\n  SCAN: ['IsbellFarms', 'MarbleCreek', 'BodieHills', 'JordanValleyCwma', 'Corozal'],\n  SOILSCAPE: ['node816', 'node914'],\n  ARM: ['Lamont-CF1', 'MapleCity'],\n  TxSON: ['CR200-21']</pre> In\u00a0[251]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(10,14), subplot_kw={'projection': ccrs.Robinson()})\nsubset.plot_station_locations(ax=ax, markersize=10,text_scalefactor=3)\nax.gridlines (linewidth = 1, color = 'red', alpha = 0.5, linestyle='-')\nax.set_extent([-127, -70, 24, 50.5])\nax.stock_img()\nplt.show()\n</pre> fig, ax = plt.subplots(1, 1, figsize=(10,14), subplot_kw={'projection': ccrs.Robinson()}) subset.plot_station_locations(ax=ax, markersize=10,text_scalefactor=3) ax.gridlines (linewidth = 1, color = 'red', alpha = 0.5, linestyle='-') ax.set_extent([-127, -70, 24, 50.5]) ax.stock_img() plt.show()"},{"location":"MundyPyTesmo/#ismn-international-soil-moisture-network","title":"ISMN (International Soil Moisture Network)\u00b6","text":""},{"location":"MundyPyTesmo/#by-kerry-mundy","title":"By Kerry Mundy\u00b6","text":""},{"location":"MundyPyTesmo/#in-situ-soil-moisture-database-which-has-the-means-for-validating-and-improving-global-satellite-products-land-surface-climate-models-etc-httpsismnbafgdeendataviewer","title":"In situ soil moisture database which has the means for validating and improving global satellite products, land surface, climate models, etc. https://ismn.bafg.de/en/dataviewer/\u00b6","text":""},{"location":"MundyPyTesmo/#reader-tool-that-is-used-to-access-in-situ-soil-moisture-data","title":"Reader tool that is used to access in-situ soil moisture data\u00b6","text":""},{"location":"MundyPyTesmo/#provides-functions-to-access-single-networks-and-stations-around-the-globe","title":"Provides functions to access single networks and stations around the globe\u00b6","text":""},{"location":"MundyPyTesmo/#install-package","title":"Install Package\u00b6","text":""},{"location":"MundyPyTesmo/#why-soil-moisture","title":"Why soil moisture?\u00b6","text":""},{"location":"MundyPyTesmo/#ismn-interface","title":"ISMN Interface\u00b6","text":"<p>Collects metadata for all sensors - iterate through the files to collect information</p>"},{"location":"MundyPyTesmo/#understanding-the-data","title":"Understanding the Data\u00b6","text":""},{"location":"MundyPyTesmo/#networks-and-stations","title":"Networks and Stations\u00b6","text":""},{"location":"MundyPyTesmo/#soil-moisture-probes","title":"Soil Moisture Probes\u00b6","text":""},{"location":"MundyPyTesmo/#sensors-contain-a-time-series-sensordata-and-metadata-sensormetadata-we-can-convert-metadata-to-a-dataframe","title":"Sensors contain a time series (sensor.data) and metadata (sensor.metadata) - we can convert metadata to a dataframe\u00b6","text":""},{"location":"MundyPyTesmo/#what-station-is-closest-to-us","title":"What station is closest to us?\u00b6","text":""},{"location":"MundyPyTesmo/#reading-specific-sensors","title":"Reading Specific Sensors\u00b6","text":""},{"location":"MundyPyTesmo/#plot-station-locations-with-subset-of-data","title":"Plot Station Locations With Subset of Data\u00b6","text":""},{"location":"NagellaPyFlo/","title":"PyFlo","text":"In\u00a0[1]: Copied! <pre># conda install -c conda-forge pyflo\n# mamba install -c conda-forge pyflo\n# pip install pyflo\n</pre> # conda install -c conda-forge pyflo # mamba install -c conda-forge pyflo # pip install pyflo In\u00a0[2]: Copied! <pre>from pyflo import system\nfrom pyflo.nrcs import hydrology\n</pre> from pyflo import system from pyflo.nrcs import hydrology In\u00a0[3]: Copied! <pre>import xarray as xr\nfrom matplotlib import pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy\nimport cartopy.feature as cfeature\nimport pandas as pd\nimport numpy as np\n</pre> import xarray as xr from matplotlib import pyplot as plt import cartopy.crs as ccrs import cartopy import cartopy.feature as cfeature import pandas as pd import numpy as np In\u00a0[3]: Copied! <pre># Define the runoff distribution\n\nrunoff_distribution = system.array_from_csv('/home/an691/rcaes/runoff_d1.csv')\nrunoff_distribution\n</pre> # Define the runoff distribution  runoff_distribution = system.array_from_csv('/home/an691/rcaes/runoff_d1.csv') runoff_distribution Out[3]: <pre>array([[0.   , 0.   ],\n       [0.1  , 0.03 ],\n       [0.2  , 0.1  ],\n       [0.3  , 0.19 ],\n       [0.4  , 0.31 ],\n       [0.5  , 0.47 ],\n       [0.6  , 0.66 ],\n       [0.7  , 0.82 ],\n       [0.8  , 0.93 ],\n       [0.9  , 0.99 ],\n       [1.   , 1.   ],\n       [1.1  , 0.99 ],\n       [1.2  , 0.93 ],\n       [1.3  , 0.86 ],\n       [1.4  , 0.78 ],\n       [1.5  , 0.68 ],\n       [1.6  , 0.56 ],\n       [1.7  , 0.46 ],\n       [1.8  , 0.39 ],\n       [1.9  , 0.33 ],\n       [2.   , 0.28 ],\n       [2.2  , 0.207],\n       [2.4  , 0.147],\n       [2.6  , 0.107],\n       [2.8  , 0.077],\n       [3.   , 0.055],\n       [3.2  , 0.04 ],\n       [3.4  , 0.029],\n       [3.6  , 0.021],\n       [3.8  , 0.015],\n       [4.   , 0.011],\n       [4.5  , 0.005],\n       [5.   , 0.   ]])</pre> In\u00a0[6]: Copied! <pre># Define the Pre-development Basin - SCS\npre_scs_basin = hydrology.Basin(\n    area=132,\n    cn=69.0,\n    tc=1.0,\n    runoff_dist=runoff_distribution,\n    peak_factor=1.0)\npre_scs_unit_hydrograph = pre_scs_basin.unit_hydrograph(interval=0.3)\n</pre> # Define the Pre-development Basin - SCS pre_scs_basin = hydrology.Basin(     area=132,     cn=69.0,     tc=1.0,     runoff_dist=runoff_distribution,     peak_factor=1.0) pre_scs_unit_hydrograph = pre_scs_basin.unit_hydrograph(interval=0.3) In\u00a0[7]: Copied! <pre># Define the Post-development Basin - SCS\n\npost_scs_basin = hydrology.Basin(\n    area=132,\n    cn=90.0,\n    tc=1.0,\n    runoff_dist=runoff_distribution,\n    peak_factor=2.0)\npost_scs_unit_hydrograph = post_scs_basin.unit_hydrograph(interval=0.3)\n</pre> # Define the Post-development Basin - SCS  post_scs_basin = hydrology.Basin(     area=132,     cn=90.0,     tc=1.0,     runoff_dist=runoff_distribution,     peak_factor=2.0) post_scs_unit_hydrograph = post_scs_basin.unit_hydrograph(interval=0.3) In\u00a0[8]: Copied! <pre>fig, axes = plt.subplots(ncols=2, figsize=(10,4))\nax0, ax1 = axes\n\n#Pre-development Hydrograph\nx_pre = pre_scs_unit_hydrograph[:, 0]\ny_pre = pre_scs_unit_hydrograph[:, 1]\nax0.plot(x_pre, y_pre, 'k')\nax0.plot(x_pre, y_pre, 'bo')\nax0.set_title(r'Pre-development Unit Hydrograph (SCS)')\nax0.set_xlabel(r'Time ($hr$)')\nax0.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')\n\n#Pre-development Hydrograph\nx_post = post_scs_unit_hydrograph[:, 0]\ny_post = post_scs_unit_hydrograph[:, 1]\nax1.plot(x_post, y_post, 'k')\nax1.plot(x_post, y_post, 'bo')\nax1.set_title(r'Post-development Unit Hydrograph (SCS)')\nax1.set_xlabel(r'Time ($hr$)')\nax1.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')\n\nplt.tight_layout()\n</pre> fig, axes = plt.subplots(ncols=2, figsize=(10,4)) ax0, ax1 = axes  #Pre-development Hydrograph x_pre = pre_scs_unit_hydrograph[:, 0] y_pre = pre_scs_unit_hydrograph[:, 1] ax0.plot(x_pre, y_pre, 'k') ax0.plot(x_pre, y_pre, 'bo') ax0.set_title(r'Pre-development Unit Hydrograph (SCS)') ax0.set_xlabel(r'Time ($hr$)') ax0.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')  #Pre-development Hydrograph x_post = post_scs_unit_hydrograph[:, 0] y_post = post_scs_unit_hydrograph[:, 1] ax1.plot(x_post, y_post, 'k') ax1.plot(x_post, y_post, 'bo') ax1.set_title(r'Post-development Unit Hydrograph (SCS)') ax1.set_xlabel(r'Time ($hr$)') ax1.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')  plt.tight_layout() In\u00a0[9]: Copied! <pre>#Define Rainfall Distribution intensity over a 1 hour duration\nrainfall_dist = np.array([\n    (0.00, 0.000),\n    (0.05, 0.074),\n    (0.10, 0.174),\n    (0.15, 0.280),\n    (0.20, 0.378),\n    (0.25, 0.448),\n    (0.30, 0.496),\n    (0.35, 0.526),\n    (0.40, 0.540),\n    (0.45, 0.540),\n    (0.50, 0.540),\n    (0.55, 0.542),\n    (0.60, 0.554),\n    (0.65, 0.582),\n    (0.70, 0.640),\n    (0.75, 0.724),\n    (0.80, 0.816),\n    (0.85, 0.886),\n    (0.90, 0.940),\n    (0.95, 0.980),\n    (1.00, 1.000)\n])\nrainfall_depths1 = rainfall_dist * [6.0, 5.0]  # Scale array to 5 inches over 6 hours.\nrainfall_depths2 = rainfall_dist * [6.0, 12.0]  # Scale array to 12 inches over 6 hours.\n</pre> #Define Rainfall Distribution intensity over a 1 hour duration rainfall_dist = np.array([     (0.00, 0.000),     (0.05, 0.074),     (0.10, 0.174),     (0.15, 0.280),     (0.20, 0.378),     (0.25, 0.448),     (0.30, 0.496),     (0.35, 0.526),     (0.40, 0.540),     (0.45, 0.540),     (0.50, 0.540),     (0.55, 0.542),     (0.60, 0.554),     (0.65, 0.582),     (0.70, 0.640),     (0.75, 0.724),     (0.80, 0.816),     (0.85, 0.886),     (0.90, 0.940),     (0.95, 0.980),     (1.00, 1.000) ]) rainfall_depths1 = rainfall_dist * [6.0, 5.0]  # Scale array to 5 inches over 6 hours. rainfall_depths2 = rainfall_dist * [6.0, 12.0]  # Scale array to 12 inches over 6 hours. In\u00a0[10]: Copied! <pre>pre_flood_hydrograph = pre_scs_basin.flood_hydrograph(rainfall_depths1, interval=0.3)\npost_flood_hydrograph = post_scs_basin.flood_hydrograph(rainfall_depths1, interval=0.3)\npre_flood_hydrograph2 = pre_scs_basin.flood_hydrograph(rainfall_depths2, interval=0.3)\npost_flood_hydrograph2 = post_scs_basin.flood_hydrograph(rainfall_depths2, interval=0.3)\n</pre> pre_flood_hydrograph = pre_scs_basin.flood_hydrograph(rainfall_depths1, interval=0.3) post_flood_hydrograph = post_scs_basin.flood_hydrograph(rainfall_depths1, interval=0.3) pre_flood_hydrograph2 = pre_scs_basin.flood_hydrograph(rainfall_depths2, interval=0.3) post_flood_hydrograph2 = post_scs_basin.flood_hydrograph(rainfall_depths2, interval=0.3) In\u00a0[12]: Copied! <pre>fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,6))\nax0, ax1, ax2, ax3 = axes.flatten()\n\n#Pre-development Hydrograph\nx_pre_flood = pre_flood_hydrograph[:, 0]\ny_pre_flood = pre_flood_hydrograph[:, 1]\nax0.plot(x_pre_flood, y_pre_flood, 'k')\nax0.plot(x_pre_flood, y_pre_flood, 'bo')\nax0.set_title(r'Pre-development, 5 in')\nax0.set_xlabel(r'Time ($hr$)')\nax0.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')\n\n#Pre-development Hydrograph\nx_post_flood = post_flood_hydrograph[:, 0]\ny_post_flood = post_flood_hydrograph[:, 1]\nax1.plot(x_post_flood, y_post_flood, 'k')\nax1.plot(x_post_flood, y_post_flood, 'bo')\nax1.set_title(r'Post-development, 5 in')\nax1.set_xlabel(r'Time ($hr$)')\nax1.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')\n\n#Pre-development Hydrograph\nx_pre_flood = pre_flood_hydrograph2[:, 0]\ny_pre_flood = pre_flood_hydrograph2[:, 1]\nax2.plot(x_pre_flood, y_pre_flood, 'k')\nax2.plot(x_pre_flood, y_pre_flood, 'bo')\nax2.set_title(r'Pre-development, 12 in')\nax2.set_xlabel(r'Time ($hr$)')\nax2.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')\n\n#Pre-development Hydrograph\nx_post_flood = post_flood_hydrograph2[:, 0]\ny_post_flood = post_flood_hydrograph2[:, 1]\nax3.plot(x_post_flood, y_post_flood, 'k')\nax3.plot(x_post_flood, y_post_flood, 'bo')\nax3.set_title(r'Post-development, 12 in')\nax3.set_xlabel(r'Time ($hr$)')\nax3.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')\n\n\n#Adding a title to all subplots\nplt.suptitle(\"NRCS (SCS) Method Flood Hydrographs\")\n\n\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,6)) ax0, ax1, ax2, ax3 = axes.flatten()  #Pre-development Hydrograph x_pre_flood = pre_flood_hydrograph[:, 0] y_pre_flood = pre_flood_hydrograph[:, 1] ax0.plot(x_pre_flood, y_pre_flood, 'k') ax0.plot(x_pre_flood, y_pre_flood, 'bo') ax0.set_title(r'Pre-development, 5 in') ax0.set_xlabel(r'Time ($hr$)') ax0.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')  #Pre-development Hydrograph x_post_flood = post_flood_hydrograph[:, 0] y_post_flood = post_flood_hydrograph[:, 1] ax1.plot(x_post_flood, y_post_flood, 'k') ax1.plot(x_post_flood, y_post_flood, 'bo') ax1.set_title(r'Post-development, 5 in') ax1.set_xlabel(r'Time ($hr$)') ax1.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')  #Pre-development Hydrograph x_pre_flood = pre_flood_hydrograph2[:, 0] y_pre_flood = pre_flood_hydrograph2[:, 1] ax2.plot(x_pre_flood, y_pre_flood, 'k') ax2.plot(x_pre_flood, y_pre_flood, 'bo') ax2.set_title(r'Pre-development, 12 in') ax2.set_xlabel(r'Time ($hr$)') ax2.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')  #Pre-development Hydrograph x_post_flood = post_flood_hydrograph2[:, 0] y_post_flood = post_flood_hydrograph2[:, 1] ax3.plot(x_post_flood, y_post_flood, 'k') ax3.plot(x_post_flood, y_post_flood, 'bo') ax3.set_title(r'Post-development, 12 in') ax3.set_xlabel(r'Time ($hr$)') ax3.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')   #Adding a title to all subplots plt.suptitle(\"NRCS (SCS) Method Flood Hydrographs\")   plt.tight_layout() plt.show() In\u00a0[13]: Copied! <pre>from pyflo.rational import hydrology\n</pre> from pyflo.rational import hydrology In\u00a0[14]: Copied! <pre># Define the Pre-development Basin - Rational\npre_rat_basin = hydrology.Basin(\n    area=132,\n    c=0.15,\n    tc=1.0)\n\n# Define the Post-development Basin - Rational\n\npost_rat_basin = hydrology.Basin(\n    area=132,\n    c=0.70,\n    tc=1.0)\n</pre> # Define the Pre-development Basin - Rational pre_rat_basin = hydrology.Basin(     area=132,     c=0.15,     tc=1.0)  # Define the Post-development Basin - Rational  post_rat_basin = hydrology.Basin(     area=132,     c=0.70,     tc=1.0) In\u00a0[15]: Copied! <pre># Define Rainfall Distribution\nrainfall_dist = np.array([\n    (0.01, 0.0001),\n    (0.05, 0.1),\n    (0.10, 0.2),\n    (0.15, 0.3),\n    (0.20, 0.4),\n    (0.25, 0.5),\n    (0.30, 0.6),\n    (0.35, 0.7),\n    (0.40, 0.8),\n    (0.45, 0.9),\n    (0.50, 1.0),\n    (0.55, 0.9),\n    (0.60, 0.8),\n    (0.65, 0.7),\n    (0.70, 0.6),\n    (0.75, 0.5),\n    (0.80, 0.4),\n    (0.85, 0.3),\n    (0.90, 0.2),\n    (0.95, 0.1),\n    (1.00, 0.0001)\n])\nrainfall_depths1 = rainfall_dist * [6.0, 5.0]  # Scale array to 5 inches over 6 hours.\nrainfall_depths2 = rainfall_dist * [6.0, 12.0]  # Scale array to 12 inches over 6 hours.\n</pre> # Define Rainfall Distribution rainfall_dist = np.array([     (0.01, 0.0001),     (0.05, 0.1),     (0.10, 0.2),     (0.15, 0.3),     (0.20, 0.4),     (0.25, 0.5),     (0.30, 0.6),     (0.35, 0.7),     (0.40, 0.8),     (0.45, 0.9),     (0.50, 1.0),     (0.55, 0.9),     (0.60, 0.8),     (0.65, 0.7),     (0.70, 0.6),     (0.75, 0.5),     (0.80, 0.4),     (0.85, 0.3),     (0.90, 0.2),     (0.95, 0.1),     (1.00, 0.0001) ]) rainfall_depths1 = rainfall_dist * [6.0, 5.0]  # Scale array to 5 inches over 6 hours. rainfall_depths2 = rainfall_dist * [6.0, 12.0]  # Scale array to 12 inches over 6 hours. In\u00a0[16]: Copied! <pre>pre_rat_flood_hydrograph = pre_rat_basin.flood_hydrograph(rainfall_depths1, interval=0.1)\npost_rat_flood_hydrograph = post_rat_basin.flood_hydrograph(rainfall_depths1, interval=0.3)\npre_rat_flood_hydrograph2 = pre_rat_basin.flood_hydrograph(rainfall_depths2, interval=0.3)\npost_rat_flood_hydrograph2 = post_rat_basin.flood_hydrograph(rainfall_depths2, interval=0.3)\n</pre> pre_rat_flood_hydrograph = pre_rat_basin.flood_hydrograph(rainfall_depths1, interval=0.1) post_rat_flood_hydrograph = post_rat_basin.flood_hydrograph(rainfall_depths1, interval=0.3) pre_rat_flood_hydrograph2 = pre_rat_basin.flood_hydrograph(rainfall_depths2, interval=0.3) post_rat_flood_hydrograph2 = post_rat_basin.flood_hydrograph(rainfall_depths2, interval=0.3) In\u00a0[17]: Copied! <pre>fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,6))\nax0, ax1, ax2, ax3 = axes.flatten()\n\n#Pre-development Hydrograph, 5 in\nx_pre_flood = pre_rat_flood_hydrograph[:, 0]\ny_pre_flood = pre_rat_flood_hydrograph[:, 1]\nax0.plot(x_pre_flood, y_pre_flood, 'k')\nax0.plot(x_pre_flood, y_pre_flood, 'bo')\nax0.set_title(r'Pre-development (5 in)')\nax0.set_xlabel(r'Time ($hr$)')\nax0.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')\n\n#Pre-development Hydrograph, 5 in\nx_post_flood = post_rat_flood_hydrograph[:, 0]\ny_post_flood = post_rat_flood_hydrograph[:, 1]\nax1.plot(x_post_flood, y_post_flood, 'k')\nax1.plot(x_post_flood, y_post_flood, 'bo')\nax1.set_title(r'Post-development (5 in)')\nax1.set_xlabel(r'Time ($hr$)')\nax1.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')\n\n#Pre-development Hydrograph, 12 in\nx_pre_flood = pre_rat_flood_hydrograph2[:, 0]\ny_pre_flood = pre_rat_flood_hydrograph2[:, 1]\nax2.plot(x_pre_flood, y_pre_flood, 'k')\nax2.plot(x_pre_flood, y_pre_flood, 'bo')\nax2.set_title(r'Pre-development (12 in)')\nax2.set_xlabel(r'Time ($hr$)')\nax2.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')\n\n#Pre-development Hydrograph, 12 in\nx_post_flood = post_rat_flood_hydrograph2[:, 0]\ny_post_flood = post_rat_flood_hydrograph2[:, 1]\nax3.plot(x_post_flood, y_post_flood, 'k')\nax3.plot(x_post_flood, y_post_flood, 'bo')\nax3.set_title(r'Post-development (12 in)')\nax3.set_xlabel(r'Time ($hr$)')\nax3.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')\n\n#Adding a title to all subplots\nplt.suptitle(\"Rational Method Flood Hydrographs\")\n\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,6)) ax0, ax1, ax2, ax3 = axes.flatten()  #Pre-development Hydrograph, 5 in x_pre_flood = pre_rat_flood_hydrograph[:, 0] y_pre_flood = pre_rat_flood_hydrograph[:, 1] ax0.plot(x_pre_flood, y_pre_flood, 'k') ax0.plot(x_pre_flood, y_pre_flood, 'bo') ax0.set_title(r'Pre-development (5 in)') ax0.set_xlabel(r'Time ($hr$)') ax0.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')  #Pre-development Hydrograph, 5 in x_post_flood = post_rat_flood_hydrograph[:, 0] y_post_flood = post_rat_flood_hydrograph[:, 1] ax1.plot(x_post_flood, y_post_flood, 'k') ax1.plot(x_post_flood, y_post_flood, 'bo') ax1.set_title(r'Post-development (5 in)') ax1.set_xlabel(r'Time ($hr$)') ax1.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')  #Pre-development Hydrograph, 12 in x_pre_flood = pre_rat_flood_hydrograph2[:, 0] y_pre_flood = pre_rat_flood_hydrograph2[:, 1] ax2.plot(x_pre_flood, y_pre_flood, 'k') ax2.plot(x_pre_flood, y_pre_flood, 'bo') ax2.set_title(r'Pre-development (12 in)') ax2.set_xlabel(r'Time ($hr$)') ax2.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')  #Pre-development Hydrograph, 12 in x_post_flood = post_rat_flood_hydrograph2[:, 0] y_post_flood = post_rat_flood_hydrograph2[:, 1] ax3.plot(x_post_flood, y_post_flood, 'k') ax3.plot(x_post_flood, y_post_flood, 'bo') ax3.set_title(r'Post-development (12 in)') ax3.set_xlabel(r'Time ($hr$)') ax3.set_ylabel(r'Discharge ($\\frac{ft^{3}}{s}$)')  #Adding a title to all subplots plt.suptitle(\"Rational Method Flood Hydrographs\")  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"NagellaPyFlo/#pyflo","title":"PyFlo\u00b6","text":""},{"location":"NagellaPyFlo/#by-amulya-nagella","title":"By: Amulya Nagella\u00b6","text":"<p>This is a python package that can be used for estimating runoff rates, rainfall/runoff storage over time, and flood routing analyses. The tutorial below will take you through the process of generating a hydrograph using 2 different methods: Soil Conservation Service (SCS) Method and the Rational Method.</p> <p>The hydrograph will represent the time-flow relationship per unit of runoff or rainfall depth.</p> <p>Rainfall: amount of precipitation that falls into a specifc area (usually in the form of rain)</p> <p>Runoff: precipitation that flows over watersheds (does not infiltrate into soil or vegetation) and flows off into water bodies (streams, lakes, or rivers).</p>"},{"location":"NagellaPyFlo/#package-installation","title":"Package Installation\u00b6","text":""},{"location":"NagellaPyFlo/#import-packages","title":"Import Packages\u00b6","text":""},{"location":"NagellaPyFlo/#method-1-scs-or-nrcs-equation","title":"Method 1: SCS (or NRCS) Equation\u00b6","text":"<p>This method is suitable for a wide range of watershed sizes and land uses (rural and urban areas). Generally preferred for longer-duration storms.</p> <p></p>"},{"location":"NagellaPyFlo/#unit-hydrograph","title":"Unit Hydrograph\u00b6","text":"<p>Represents the direct runoff from a unit depth (usually 1 inch) of rainfall over a watershed during a short-duration rainfall event.</p>"},{"location":"NagellaPyFlo/#required-parameters","title":"Required Parameters:\u00b6","text":"<p>area = the region that the water is flowing into, size of the basin.</p> <p>cn = runoff coefficient, indicating the intensity of the runoff amount (rainfall volume).</p> <p>tc = time of concentration, amount of time it takes the runoff volume to travel from the most remote location within the watershed into the outlet of the watershed.</p> <p>runoff distribution = unscaled unit hydrograph runoff distribution.</p> <p>peak_factor = scaling of the peak factor (depending on the conditions of the area).</p>"},{"location":"NagellaPyFlo/#flood-hydrograph","title":"Flood Hydrograph\u00b6","text":"<p>Represents the discharge of a waer bodie over time in response to a specific rainfall event oer a longer duration.</p>"},{"location":"NagellaPyFlo/#method-2-rational-equation","title":"Method 2: Rational Equation\u00b6","text":"<p>This method is mainly used for small urban or suburban watersheds and is best suited for short-duration storms. It assumes a uniform rainfall intensity across the watershed during the storm.</p> <p></p> <p></p>"},{"location":"NagellaPyFlo/#flood-hydrograph","title":"Flood Hydrograph\u00b6","text":"<p>Represents the discharge of a water bodies over time in response to a specific rainfall event over a longer duration.</p>"},{"location":"NagellaPyFlo/#required-parameters","title":"Required Parameters:\u00b6","text":"<p>area = the region that the water is flowing into, size of the basin.</p> <p>c = runoff coefficient, ratio value between 0-1.</p> <p>tc = time of concentration, amount of time it takes the runoff volume to travel from the most remote location within the watershed into the outlet of the watershed.</p>"},{"location":"Schedule/","title":"Schedule","text":""},{"location":"Schedule/#schedule","title":"Schedule","text":"date topic Assignment Sept 11 Course introduction Sept 18 Intro to Unix Shell, Python Environment, Core Python Language Amarel &amp; GitHub Account Sept 25 Intro to Git, Functions and Classes Assignment 1 due Oct 2 Basic Python: Numpy and Matplotlib Assignment 2 due Oct 9 Guest lecture: Introduction to Amarel Oct 16 Python for Tabular Data: Basic Pandas Assignment 3 Due Oct 23 Python for Tabular Data: Advanced Pandas Final Project Topic Due Oct 30 Python for Multidimensional Data: Xarray Assignment 4 Due Nov 6 Dask for Big Data Nov 13 Making Maps: Cartopy Assignment 5  Due Nov 20 Environmental Sciences Packages: GeoPandas and others Assignment 6  Due Nov 27 Reproducible Research: Python Packaging and Organization Dec 4 Cloud Computing Final Project Part I &amp; Assignment 7 Due Dec 11 No Class Final Project Part II Due"},{"location":"Syllabus/","title":"Syllabus","text":""},{"location":"Syllabus/#part-1-course-information","title":"Part 1: Course Information","text":"<p>Class Time: Monday, 2 to 5 PM  Location: ENR 323</p>"},{"location":"Syllabus/#instructor","title":"Instructor:","text":"<p>Xiaomeng Jin Department of Environmental Sciences Office: ENR 230 Email: xiaomeng.jin@rutgers.edu Office Hour: Friday, 1 \u2013 2 PM</p>"},{"location":"Syllabus/#part-2-overview","title":"Part 2: Overview","text":"<p>This course will introduce modern computing software, programming tools and best practices for open-source research that are transparent, accessible, reproducible and inclusive. The course consists of three components:   (1) Introduction to programming in the open-source Python language and in-depth exploration of the numerical analysis and visualization packages that comprise the modern scientific Python ecosystem;  (2) Introduction to the concept of open science and best practices for conducting open-source research;   (3) Introduction to cloud and parallel computing for big data analysis. The course is designed to be accessible for graduate students in atmospheric science, environmental sciences or other disciplines in earth sciences.  Student learning will be facilitated through a combination of lectures, in-class exercises, homework assignments and class projects.</p>"},{"location":"Syllabus/#part-3-course-structure","title":"Part 3: Course Structure","text":"<p>Format: The instructor will present new materials in the first half of the lecture. The second half of the class will be flipped: students will work first in small groups and then individually on assignments.  Textbook: There is no required textbook. All materials will come from free online resources and the course website itself.  Computers: Students can either bring their laptops or use the computers in ENR 323. Students will use Amarel, the university\u2019s high performance computing cluster, to work on their assignments and final project.  </p>"},{"location":"Syllabus/#part-4-grading-policy","title":"Part 4: Grading Policy","text":""},{"location":"Syllabus/#weekly-assignments-70","title":"Weekly Assignments (70%)","text":"<p>\u2022   Total: 100 \u2022   All questions complete: 50 \u2022   All questions correct: 30  \u2022   Clean, elegant, efficient code: rate between 0 and 10  \u2022   Clear comments and explanations: rate between 0 and 10  \u2022   Late penalty: -20 per day (24 hrs) \u2022   Lowest grade on an assignment will be dropped. </p>"},{"location":"Syllabus/#final-project-30","title":"Final Project (30%)","text":"<p> Part I: Individual Project (20%)  The goal of the final project is to assess your ability to combine and apply the skills you have learned in class in the context of a real-world research problem. Our class has mostly focused on tools for data analysis and visualization, so this must be the focus of your final project. Specifically, we seek to assess your ability to do the following tasks:  \u2022   Discover and download real datasets in standard formats (e.g. CSV, netCDF)  \u2022   Load the data into pandas or xarray, performing any necessary data cleanup (dealing with missing values, proper time encoding, etc.) along the way.  \u2022   Perform realistic scientific calculation involving, for example tasks such as grouping, aggregating, and applying mathematical formulas.  \u2022   Visualize your results in well-formatted plots. </p> <p> Part II: Reproducing Another Student\u2019s Project (10%)   The goal of the second part is to assess the reproducibility of the student\u2019s project, and whether the students can reproduce and collaborate with others on code development. Our class focuses on conducting open-source research that are transparent, accessible, reproducible and inclusive, so your final project should demonstrate your understanding and ability to perform open-source research. We seek to assess your ability to:  \u2022   Clearly document your analysis to make it reproducible.  \u2022   Reproduce the other student\u2019s final project.  \u2022   Bonus points will be given if the students submit pull requests and issues for code development. </p>"},{"location":"Syllabus/#part-5-reference-materials","title":"Part 5: Reference Materials:","text":"<p>\u2022 An Introduction to Earth and Environmental Data Science</p>"},{"location":"Untitled/","title":"Untitled","text":"In\u00a0[4]: Copied! <pre>(180*360*48*24*365*4)/1e6/1000\n</pre> (180*360*48*24*365*4)/1e6/1000 Out[4]: <pre>108.988416</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"VizeppiStatsModelTutorial/","title":"Statsmodel","text":"In\u00a0[288]: Copied! <pre>#to activate the environment: conda activate rcaes_env_new\n\n#to install  use: \n#mamba install -c conda-forge statsmodels\n# or use\n#conda install -c conda-forge statsmodels\n</pre> #to activate the environment: conda activate rcaes_env_new  #to install  use:  #mamba install -c conda-forge statsmodels # or use #conda install -c conda-forge statsmodels In\u00a0[289]: Copied! <pre>#import the dependencies of the package\nimport numpy as np\nimport pandas as pd\nimport patsy as pa\nimport scipy as sci\n#optional dependecies\nimport matplotlib.pyplot as plt\n</pre> #import the dependencies of the package import numpy as np import pandas as pd import patsy as pa import scipy as sci #optional dependecies import matplotlib.pyplot as plt In\u00a0[290]: Copied! <pre>#import modules\nimport statsmodels.api as sm\nfrom patsy import dmatrices\nimport statsmodels.formula.api as smf\n</pre> #import modules import statsmodels.api as sm from patsy import dmatrices import statsmodels.formula.api as smf In\u00a0[291]: Copied! <pre>#load sample data using pandas dataframe\ndf = pd.read_csv('/home/vgi3/rcaes/StatsModelTutorial/LIS_2021_DB.csv', na_values = 'ND')\n</pre> #load sample data using pandas dataframe df = pd.read_csv('/home/vgi3/rcaes/StatsModelTutorial/LIS_2021_DB.csv', na_values = 'ND')  In\u00a0[292]: Copied! <pre>df\n</pre> df Out[292]: ID RECORD_ID DATE_STATION_LINK DATE YEAR MONTH TIME_24H STATION_ID DEPTH_M CLASS ... FIELD_NOTES WEATHER PREV_24_H_RAIN PREV_48_H_RAIN TIDE_TIME_NR TIDE_RANGE_FT_NR TIDE_TYPE_NR TIDE_TIME_KP TIDE_RANGE_FT_KP TIDE_TYPE_KP 0 23807 9-413_1/5/2021_6:43_2.3 1/5/2021_9-413 1/5/2021 2021 January 6:43 9-413 2.3 Bottom ... pH probe on YSI EXO1 sonde malfunctioned Clear, cold, dry 0.00 0.12 9:19 0.16 Low 9:37 0.17 Low 1 23808 9-413_1/5/2021_6:44_1 1/5/2021_9-413 1/5/2021 2021 January 6:44 9-413 1.0 Middle ... pH probe on YSI EXO1 sonde malfunctioned Clear, cold, dry 0.00 0.12 9:19 0.16 Low 9:37 0.17 Low 2 23809 9-413_1/5/2021_6:45_0.5 1/5/2021_9-413 1/5/2021 2021 January 6:45 9-413 0.5 Surface ... pH probe on YSI EXO1 sonde malfunctioned Clear, cold, dry 0.00 0.12 9:19 0.16 Low 9:37 0.17 Low 3 23810 9-412_1/5/2021_6:51_4.2 1/5/2021_9-412 1/5/2021 2021 January 6:51 9-412 4.2 Bottom ... pH probe on YSI EXO1 sonde malfunctioned Clear, cold, dry 0.00 0.12 9:19 0.16 Low 9:37 0.17 Low 4 23811 9-412_1/5/2021_6:52_2.1 1/5/2021_9-412 1/5/2021 2021 January 6:52 9-412 2.1 Middle ... pH probe on YSI EXO1 sonde malfunctioned Clear, cold, dry 0.00 0.12 9:19 0.16 Low 9:37 0.17 Low ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1303 25110 Dup E-12_11/30/2021_7:48_2.2 11/30/2021_Dup E-12 11/30/2021 2021 November 7:48 Dup E-12 2.2 Middle ... NaN Cloudy, calm waters 0.01 0.01 6:47 7.91 High 7:03 7.83 High 1304 25111 Dup E-12_11/30/2021_7:48_0.5 11/30/2021_Dup E-12 11/30/2021 2021 November 7:48 Dup E-12 0.5 Surface ... NaN Cloudy, calm waters 0.01 0.01 6:47 7.91 High 7:03 7.83 High 1305 25112 Dup DI2_11/30/2021_8:06_9.1 11/30/2021_Dup DI2 11/30/2021 2021 November 8:06 Dup DI2 9.1 Bottom ... NaN Cloudy, calm waters 0.01 0.01 6:47 7.91 High 7:03 7.83 High 1306 25113 Dup DI2_11/30/2021_8:07_4.8 11/30/2021_Dup DI2 11/30/2021 2021 November 8:07 Dup DI2 4.8 Middle ... NaN Cloudy, calm waters 0.01 0.01 6:47 7.91 High 7:03 7.83 High 1307 25114 Dup DI2_11/30/2021_8:07_0.5 11/30/2021_Dup DI2 11/30/2021 2021 November 8:07 Dup DI2 0.5 Surface ... NaN Cloudy, calm waters 0.01 0.01 6:47 7.91 High 7:03 7.83 High <p>1308 rows \u00d7 46 columns</p> In\u00a0[293]: Copied! <pre>#isolate the columns used for analysis\ndf = df[['DISSOLVED_OXYGEN_MG_L', 'TEMPERATURE_C', 'PH']]\n</pre> #isolate the columns used for analysis df = df[['DISSOLVED_OXYGEN_MG_L', 'TEMPERATURE_C', 'PH']] In\u00a0[294]: Copied! <pre>#drop NA values, reset the index to match the new # of columns\ndf = df.dropna()\ndf = df.reset_index(drop=True)\n</pre> #drop NA values, reset the index to match the new # of columns df = df.dropna() df = df.reset_index(drop=True) In\u00a0[295]: Copied! <pre>df.info()\n</pre> df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1239 entries, 0 to 1238\nData columns (total 3 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   DISSOLVED_OXYGEN_MG_L  1239 non-null   float64\n 1   TEMPERATURE_C          1239 non-null   float64\n 2   PH                     1239 non-null   float64\ndtypes: float64(3)\nmemory usage: 29.2 KB\n</pre> In\u00a0[296]: Copied! <pre>pd.to_numeric(df['PH'])\n</pre> pd.to_numeric(df['PH']) Out[296]: <pre>0       8.36\n1       8.38\n2       8.39\n3       8.45\n4       8.45\n        ... \n1234    8.25\n1235    8.25\n1236    8.19\n1237    8.19\n1238    8.20\nName: PH, Length: 1239, dtype: float64</pre> In\u00a0[205]: Copied! <pre>#explore dataset with pandasframes corr function look for potential relationships\n</pre> #explore dataset with pandasframes corr function look for potential relationships In\u00a0[206]: Copied! <pre>df.corr()\n#looks like there is a pretty good correlation between DO and TEMP, and DO and PH\n</pre> df.corr() #looks like there is a pretty good correlation between DO and TEMP, and DO and PH Out[206]: DISSOLVED_OXYGEN_MG_L TEMPERATURE_C PH DISSOLVED_OXYGEN_MG_L 1.000000 -0.836311 0.936684 TEMPERATURE_C -0.836311 1.000000 -0.782899 PH 0.936684 -0.782899 1.000000 In\u00a0[207]: Copied! <pre>#visualize potential relationships with scatter plots\nfig, ax = plt.subplots()\nax0 = plt.scatter(df['TEMPERATURE_C'], df['DISSOLVED_OXYGEN_MG_L'])\nax.set_xlabel('TEMPERATURE_C')\nax.set_ylabel('Dissolved_Oxygen_mg/L')\nax.set_title('DO vs TEMP')\n</pre> #visualize potential relationships with scatter plots fig, ax = plt.subplots() ax0 = plt.scatter(df['TEMPERATURE_C'], df['DISSOLVED_OXYGEN_MG_L']) ax.set_xlabel('TEMPERATURE_C') ax.set_ylabel('Dissolved_Oxygen_mg/L') ax.set_title('DO vs TEMP') Out[207]: <pre>Text(0.5, 1.0, 'DO vs TEMP')</pre> In\u00a0[208]: Copied! <pre>fig, ax = plt.subplots()\nax1 = plt.scatter(x=df['PH'], y=df['DISSOLVED_OXYGEN_MG_L'])\nax.set_xlabel('PH')\nax.set_ylabel('Dissolved_Oxygen_mg/L')\nax.set_title('DO vs PH')\n</pre> fig, ax = plt.subplots() ax1 = plt.scatter(x=df['PH'], y=df['DISSOLVED_OXYGEN_MG_L']) ax.set_xlabel('PH') ax.set_ylabel('Dissolved_Oxygen_mg/L') ax.set_title('DO vs PH') Out[208]: <pre>Text(0.5, 1.0, 'DO vs PH')</pre> In\u00a0[298]: Copied! <pre>#Statsmodel.api or Statsmodelformula.api?\n</pre> #Statsmodel.api or Statsmodelformula.api? In\u00a0[299]: Copied! <pre>#Design Matrices (endo, exo)\n    # matrix of endogenous variables (dependent, y)\n    # matrix of exogenous variables (independent, x)\n\n#ex. use patsy dmatrices\nY,X = dmatrices ('DISSOLVED_OXYGEN_MG_L~ PH + TEMPERATURE_C', data=df, return_type='dataframe')\n</pre> #Design Matrices (endo, exo)     # matrix of endogenous variables (dependent, y)     # matrix of exogenous variables (independent, x)  #ex. use patsy dmatrices Y,X = dmatrices ('DISSOLVED_OXYGEN_MG_L~ PH + TEMPERATURE_C', data=df, return_type='dataframe') In\u00a0[300]: Copied! <pre>#see below for an example of entering variables directly into model formula\n</pre> #see below for an example of entering variables directly into model formula In\u00a0[301]: Copied! <pre>#statsmodel documentation https://www.statsmodels.org/stable/user-guide.html\n</pre> #statsmodel documentation https://www.statsmodels.org/stable/user-guide.html In\u00a0[302]: Copied! <pre># unique formulas can be made, but this tutorial will use a model that already comes in the module\n</pre> # unique formulas can be made, but this tutorial will use a model that already comes in the module In\u00a0[305]: Copied! <pre>#view options in statsmodel.api \ndir(sm)\n</pre> #view options in statsmodel.api  dir(sm) Out[305]: <pre>['BayesGaussMI',\n 'BinomialBayesMixedGLM',\n 'ConditionalLogit',\n 'ConditionalMNLogit',\n 'ConditionalPoisson',\n 'Factor',\n 'GEE',\n 'GLM',\n 'GLMGam',\n 'GLS',\n 'GLSAR',\n 'GeneralizedPoisson',\n 'HurdleCountModel',\n 'Logit',\n 'MANOVA',\n 'MI',\n 'MICE',\n 'MICEData',\n 'MNLogit',\n 'MixedLM',\n 'NegativeBinomial',\n 'NegativeBinomialP',\n 'NominalGEE',\n 'OLS',\n 'OrdinalGEE',\n 'PCA',\n 'PHReg',\n 'Poisson',\n 'PoissonBayesMixedGLM',\n 'ProbPlot',\n 'Probit',\n 'QuantReg',\n 'RLM',\n 'RecursiveLS',\n 'SurvfuncRight',\n 'TruncatedLFNegativeBinomialP',\n 'TruncatedLFPoisson',\n 'WLS',\n 'ZeroInflatedGeneralizedPoisson',\n 'ZeroInflatedNegativeBinomialP',\n 'ZeroInflatedPoisson',\n '__all__',\n '__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n '__version__',\n '__version_info__',\n 'add_constant',\n 'categorical',\n 'cov_struct',\n 'datasets',\n 'distributions',\n 'duration',\n 'emplike',\n 'families',\n 'formula',\n 'gam',\n 'genmod',\n 'graphics',\n 'iolib',\n 'load',\n 'load_pickle',\n 'multivariate',\n 'nonparametric',\n 'qqline',\n 'qqplot',\n 'qqplot_2samples',\n 'regression',\n 'robust',\n 'show_versions',\n 'stats',\n 'test',\n 'tools',\n 'tsa',\n 'webdoc']</pre> In\u00a0[306]: Copied! <pre>#view options in statsmodelformula.api\ndir(smf)\n</pre> #view options in statsmodelformula.api dir(smf) Out[306]: <pre>['__all__',\n '__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n 'conditional_logit',\n 'conditional_mnlogit',\n 'conditional_poisson',\n 'gee',\n 'glm',\n 'glmgam',\n 'gls',\n 'glsar',\n 'logit',\n 'mixedlm',\n 'mnlogit',\n 'negativebinomial',\n 'nominal_gee',\n 'ols',\n 'ordinal_gee',\n 'phreg',\n 'poisson',\n 'probit',\n 'quantreg',\n 'rlm',\n 'wls']</pre> In\u00a0[217]: Copied! <pre>#following example will use ordinary least squares multiple linear regression\n#model will predict dissolved oxygen based on PH and Temperature\n # y = a + B1X1 + B2X2 + ...\n</pre> #following example will use ordinary least squares multiple linear regression #model will predict dissolved oxygen based on PH and Temperature  # y = a + B1X1 + B2X2 + ... In\u00a0[307]: Copied! <pre>#using statsmodels.api\n#inputs to the model are design matrices\nmod = sm.OLS(Y, X)\n\n#update matrix so it has constant added to it\nX= sm.add_constant(X)\n</pre> #using statsmodels.api #inputs to the model are design matrices mod = sm.OLS(Y, X)  #update matrix so it has constant added to it X= sm.add_constant(X)  In\u00a0[309]: Copied! <pre># check that column for intercept is added, yes\nX.head()\n</pre> # check that column for intercept is added, yes X.head() Out[309]: Intercept PH TEMPERATURE_C 0 1.0 8.36 3.596 1 1.0 8.38 3.345 2 1.0 8.39 2.748 3 1.0 8.45 2.721 4 1.0 8.45 2.687 In\u00a0[310]: Copied! <pre>#using statsmodelsformula api to do the same thing\n#model accepts patsy formula\n#enter variables directly into patsy formula in model\n#left side of tilde is \"Y\", right side of equation is \"X's\"\n\nest = smf.ols(formula='DISSOLVED_OXYGEN_MG_L~PH+TEMPERATURE_C', data=df)\n</pre> #using statsmodelsformula api to do the same thing #model accepts patsy formula #enter variables directly into patsy formula in model #left side of tilde is \"Y\", right side of equation is \"X's\"  est = smf.ols(formula='DISSOLVED_OXYGEN_MG_L~PH+TEMPERATURE_C', data=df) In\u00a0[311]: Copied! <pre>#statsmodel.api way\nres = mod.fit()\nres.summary()\n</pre> #statsmodel.api way res = mod.fit() res.summary() Out[311]: OLS Regression Results Dep. Variable: DISSOLVED_OXYGEN_MG_L   R-squared:             0.905 Model: OLS   Adj. R-squared:        0.905 Method: Least Squares   F-statistic:           5872. Date: Mon, 04 Dec 2023   Prob (F-statistic):   0.00 Time: 11:38:35   Log-Likelihood:      -1601.8 No. Observations:   1239   AIC:                   3210. Df Residuals:   1236   BIC:                   3225. Df Model:      2 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] Intercept   -43.9343     1.098   -40.028  0.000   -46.088   -41.781 PH     6.8033     0.132    51.629  0.000     6.545     7.062 TEMPERATURE_C    -0.1079     0.006   -18.858  0.000    -0.119    -0.097 Omnibus: 47.685   Durbin-Watson:         0.711 Prob(Omnibus):  0.000   Jarque-Bera (JB):     52.409 Skew: -0.502   Prob(JB):           4.16e-12 Kurtosis:  3.092   Cond. No.               907. Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  In\u00a0[312]: Copied! <pre>#statsmodelformula.api way does same thing\nest = est.fit() \nest.summary()\n</pre> #statsmodelformula.api way does same thing est = est.fit()  est.summary() Out[312]: OLS Regression Results Dep. Variable: DISSOLVED_OXYGEN_MG_L   R-squared:             0.905 Model: OLS   Adj. R-squared:        0.905 Method: Least Squares   F-statistic:           5872. Date: Mon, 04 Dec 2023   Prob (F-statistic):   0.00 Time: 11:38:37   Log-Likelihood:      -1601.8 No. Observations:   1239   AIC:                   3210. Df Residuals:   1236   BIC:                   3225. Df Model:      2 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] Intercept   -43.9343     1.098   -40.028  0.000   -46.088   -41.781 PH     6.8033     0.132    51.629  0.000     6.545     7.062 TEMPERATURE_C    -0.1079     0.006   -18.858  0.000    -0.119    -0.097 Omnibus: 47.685   Durbin-Watson:         0.711 Prob(Omnibus):  0.000   Jarque-Bera (JB):     52.409 Skew: -0.502   Prob(JB):           4.16e-12 Kurtosis:  3.092   Cond. No.               907. Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  In\u00a0[313]: Copied! <pre>#summary table looks a little different\nprint(res.summary())\n</pre> #summary table looks a little different print(res.summary())  <pre>                              OLS Regression Results                             \n=================================================================================\nDep. Variable:     DISSOLVED_OXYGEN_MG_L   R-squared:                       0.905\nModel:                               OLS   Adj. R-squared:                  0.905\nMethod:                    Least Squares   F-statistic:                     5872.\nDate:                   Mon, 04 Dec 2023   Prob (F-statistic):               0.00\nTime:                           11:38:41   Log-Likelihood:                -1601.8\nNo. Observations:                   1239   AIC:                             3210.\nDf Residuals:                       1236   BIC:                             3225.\nDf Model:                              2                                         \nCovariance Type:               nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept       -43.9343      1.098    -40.028      0.000     -46.088     -41.781\nPH                6.8033      0.132     51.629      0.000       6.545       7.062\nTEMPERATURE_C    -0.1079      0.006    -18.858      0.000      -0.119      -0.097\n==============================================================================\nOmnibus:                       47.685   Durbin-Watson:                   0.711\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               52.409\nSkew:                          -0.502   Prob(JB):                     4.16e-12\nKurtosis:                       3.092   Cond. No.                         907.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[315]: Copied! <pre>#get more model specifics isolated\nprint(\"Parameters: \", res.params)\nprint(\"Standard errors: \", res.bse)\nprint(\"Predicted values: \", res.predict())\n</pre> #get more model specifics isolated print(\"Parameters: \", res.params) print(\"Standard errors: \", res.bse) print(\"Predicted values: \", res.predict()) <pre>Parameters:  Intercept       -43.934307\nPH                6.803343\nTEMPERATURE_C    -0.107912\ndtype: float64\nStandard errors:  Intercept        1.097581\nPH               0.131773\nTEMPERATURE_C    0.005722\ndtype: float64\nPredicted values:  [12.55359218 12.71674491 12.8492017  ... 10.72322241 10.73627974\n 10.80517646]\n</pre> In\u00a0[316]: Copied! <pre>#isolate individual constants\nalpha = res.params['Intercept']\nprint(alpha)\n</pre> #isolate individual constants alpha = res.params['Intercept'] print(alpha) <pre>-43.93430733048154\n</pre> In\u00a0[317]: Copied! <pre>#isolate coefficients\nb1 = res.params['PH']\nprint(b1)\n</pre> #isolate coefficients b1 = res.params['PH'] print(b1) <pre>6.803343350479119\n</pre> In\u00a0[318]: Copied! <pre>#isolate coefficients\nb2= res.params['TEMPERATURE_C']\nprint(b2)\n</pre> #isolate coefficients b2= res.params['TEMPERATURE_C'] print(b2) <pre>-0.1079118195999333\n</pre> In\u00a0[319]: Copied! <pre>#get predictions\npred = res.predict()\n</pre> #get predictions pred = res.predict() In\u00a0[320]: Copied! <pre>pred\n</pre> pred Out[320]: <pre>array([12.55359218, 12.71674491, 12.8492017 , ..., 10.72322241,\n       10.73627974, 10.80517646])</pre> In\u00a0[323]: Copied! <pre>#add column of predictions to original dataset for easy comparison\ndf['pred_DO'] = pred\n</pre> #add column of predictions to original dataset for easy comparison df['pred_DO'] = pred In\u00a0[324]: Copied! <pre>df.head()\n</pre> df.head() Out[324]: DISSOLVED_OXYGEN_MG_L TEMPERATURE_C PH pred_DO 0 12.61 3.596 8.36 12.553592 1 12.53 3.345 8.38 12.716745 2 12.64 2.748 8.39 12.849202 3 13.01 2.721 8.45 13.260316 4 13.19 2.687 8.45 13.263985 In\u00a0[332]: Copied! <pre>#first make copy of X variable matrix\nX2= X.copy()\n</pre> #first make copy of X variable matrix X2= X.copy() In\u00a0[333]: Copied! <pre>#set one variable equal to the average\nPH_mean= df['PH'].mean()\n#add mean PH to X\nX2['PH'] = PH_mean\nX2.head()\n</pre> #set one variable equal to the average PH_mean= df['PH'].mean() #add mean PH to X X2['PH'] = PH_mean X2.head() Out[333]: Intercept PH TEMPERATURE_C 0 1.0 7.703269 3.596 1 1.0 7.703269 3.345 2 1.0 7.703269 2.748 3 1.0 7.703269 2.721 4 1.0 7.703269 2.687 In\u00a0[334]: Copied! <pre>#predicted DO with mean pH added to original dataframe\ndf['pred_DO_w/mean_pH'] = res.predict(X2)\n</pre> #predicted DO with mean pH added to original dataframe df['pred_DO_w/mean_pH'] = res.predict(X2) In\u00a0[335]: Copied! <pre>df.head()\n</pre> df.head() Out[335]: DISSOLVED_OXYGEN_MG_L TEMPERATURE_C PH pred_DO pred_DO_w/mean_pH pred_DO_w/mean_temp 0 12.61 3.596 8.36 12.553592 8.085624 11.006152 1 12.53 3.345 8.38 12.716745 8.112710 11.142219 2 12.64 2.748 8.39 12.849202 8.177133 11.210252 3 13.01 2.721 8.45 13.260316 8.180047 11.618453 4 13.19 2.687 8.45 13.263985 8.183716 11.618453 In\u00a0[336]: Copied! <pre>fig, ax = plt.subplots()\n\nax.scatter(df['TEMPERATURE_C'], df['DISSOLVED_OXYGEN_MG_L'], label = 'Actual_DO')\nax.plot(df['TEMPERATURE_C'], df['pred_DO_w/mean_pH'], color = 'BLACK', label='Regression_Line')\nax.set_xlabel('TEMPERATURE_C')\nax.set_ylabel('Dissolved Oxygen mg/L')\nax.legend()\nax.set_title('Predicted DO regression at a mean PH of 7.7')\n</pre> fig, ax = plt.subplots()  ax.scatter(df['TEMPERATURE_C'], df['DISSOLVED_OXYGEN_MG_L'], label = 'Actual_DO') ax.plot(df['TEMPERATURE_C'], df['pred_DO_w/mean_pH'], color = 'BLACK', label='Regression_Line') ax.set_xlabel('TEMPERATURE_C') ax.set_ylabel('Dissolved Oxygen mg/L') ax.legend() ax.set_title('Predicted DO regression at a mean PH of 7.7') Out[336]: <pre>Text(0.5, 1.0, 'Predicted DO regression at a mean PH of 7.7')</pre> In\u00a0[337]: Copied! <pre>#follow same steps as before\nX3=X.copy()\nTEMP_mean= df['TEMPERATURE_C'].mean()\nX3['TEMPERATURE_C'] = TEMP_mean\ndf['pred_DO_w/mean_temp'] = res.predict(X3)\ndf.head()\n</pre> #follow same steps as before X3=X.copy() TEMP_mean= df['TEMPERATURE_C'].mean() X3['TEMPERATURE_C'] = TEMP_mean df['pred_DO_w/mean_temp'] = res.predict(X3) df.head() Out[337]: DISSOLVED_OXYGEN_MG_L TEMPERATURE_C PH pred_DO pred_DO_w/mean_pH pred_DO_w/mean_temp 0 12.61 3.596 8.36 12.553592 8.085624 11.006152 1 12.53 3.345 8.38 12.716745 8.112710 11.142219 2 12.64 2.748 8.39 12.849202 8.177133 11.210252 3 13.01 2.721 8.45 13.260316 8.180047 11.618453 4 13.19 2.687 8.45 13.263985 8.183716 11.618453 In\u00a0[338]: Copied! <pre>fig, ax = plt.subplots()\n\nax.scatter(df['PH'], df['DISSOLVED_OXYGEN_MG_L'], label = 'Actual_DO')\nax.plot(df['PH'], df['pred_DO_w/mean_temp'], color = 'BLACK', label='Regression_Line')\nax.set_xlabel('PH')\nax.set_ylabel('DIssolved Oxygen mg/L')\nax.legend()\nax.set_title('Predicted DO regression at a mean temp of 17.9 celsius')\n</pre> fig, ax = plt.subplots()  ax.scatter(df['PH'], df['DISSOLVED_OXYGEN_MG_L'], label = 'Actual_DO') ax.plot(df['PH'], df['pred_DO_w/mean_temp'], color = 'BLACK', label='Regression_Line') ax.set_xlabel('PH') ax.set_ylabel('DIssolved Oxygen mg/L') ax.legend() ax.set_title('Predicted DO regression at a mean temp of 17.9 celsius') Out[338]: <pre>Text(0.5, 1.0, 'Predicted DO regression at a mean temp of 17.9 celsius')</pre> In\u00a0[339]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(Y,Y, color = 'BLACK', label = 'Actual DO plotted on Actual DO')\nax.scatter(Y, pred, label = 'Predicted DO plotted on Acutal DO')\nax.set_xlabel('Actual DO')\nax.set_ylabel('Predicted DO')\nax.set_title('Predicted DO vs Actual DO')\nax.legend()\n</pre> fig, ax = plt.subplots() ax.plot(Y,Y, color = 'BLACK', label = 'Actual DO plotted on Actual DO') ax.scatter(Y, pred, label = 'Predicted DO plotted on Acutal DO') ax.set_xlabel('Actual DO') ax.set_ylabel('Predicted DO') ax.set_title('Predicted DO vs Actual DO') ax.legend() Out[339]: <pre>&lt;matplotlib.legend.Legend at 0x7f18cc7ce070&gt;</pre>"},{"location":"VizeppiStatsModelTutorial/#statsmodel","title":"Statsmodel\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#statsmodel-is-a-python-module-that-is-useful-for-statistical-model-and-tests","title":"Statsmodel is a python module that is useful for statistical model and tests.\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#documentation-available-from-httpswwwstatsmodelsorgstableindexhtml","title":"documentation available from https://www.statsmodels.org/stable/index.html\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#by-valeria-izeppi","title":"by Valeria Izeppi\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#december-4-2023","title":"December 4, 2023\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#install-statsmodel","title":"Install StatsModel\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#setting-up","title":"Setting up\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#load-data","title":"Load Data\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#picking-x-and-y-variables","title":"Picking X and Y variables\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#formatting-x-and-y","title":"Formatting X and Y\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#you-can-use-statmodelapi-or-statsmodelformulaapi-for-running-your-statistical-test-and-model","title":"You can use statmodel.api or statsmodelformula.api for running your statistical test and model.\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#the-differences-are-listed-below","title":"The differences are listed below.\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#statsmodelapi","title":"statsmodel.api:\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#requires-design-matrices-see-below","title":"requires design matrices (see below)\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#variables-xy-can-be-assigned-to-an-object","title":"variables (x,y) can be assigned to an object\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#this-is-usfeul-when-you-have-lots-of-variables-because-only-need-to-call-you-design-matrices","title":"This is usfeul when you have lots of variables, because only need to call you design matrices.\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#user-must-manually-add-constants-to-formulas","title":"User must manually add constants to formulas.\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#statsmodelformulaapi","title":"statsmodelformula.api\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#can-enter-variables-into-model-formula-directly","title":"can enter variables into model formula directly\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#automatically-adds-constants-to-formulas","title":"automatically adds constants to formulas\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#accepts-formula-and-data-frame-arguments","title":"accepts formula and data frame arguments\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#pick-the-statistical-test-or-model","title":"Pick the statistical test or model\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#create-the-model","title":"Create the model\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#fit-the-model-and-get-model-summary","title":"Fit the Model and Get Model Summary\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#more-ways-to-get-information-from-a-model","title":"More ways to get information from a model\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#test-out-model-and-make-predictions","title":"Test out model and make predictions\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#visualize-results","title":"Visualize Results\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#to-plot-regression-between-y-variable-and-one-x-variable-at-a-time-do-vs-temp-and-do-vs-ph","title":"To plot regression between Y variable and one X variable at a time (DO vs TEMP, and DO vs PH)\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#1-plot-do-against-temperature","title":"1. Plot DO against Temperature\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#2-plot-do-against-ph","title":"2. Plot DO against PH\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#to-plot-predicted-against-actual-data-predicted-do-vs-actual-do","title":"To plot Predicted against Actual Data (Predicted DO vs Actual DO)\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#thank-you-for-learing-how-to-build-your-model-in-statsmodel-following-this-tutorial-other-steps-that-can-be-taken-after-building-your-model-would-be-to-test-multiple-models-to-find-the-best-fit-for-your-model-and-validate-your-model-by-testing-it-with-new-data-by-using-the-tools-shown-in-this-tutorial-you-should-be-able-to-make-a-very-model-very-easily","title":"Thank you for learing how to build your model in StatsModel following this tutorial. Other steps that can be taken after building your model would be to test multiple models to find the best fit for your model and validate your model by testing it with new data. By using the tools shown in this tutorial, you should be able to make a very model very easily!\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#sources","title":"Sources\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#statsmodelorg-accessed-from-httpswwwstatsmodelsorgstableindexhtml","title":"statsmodel.org accessed from https://www.statsmodels.org/stable/index.html\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#introduction-to-regression-analysis-and-predictions-accessed-from-httpsmichael-fuchs-pythonnetlifyapp20190628introduction-to-regression-analysis-and-predictions","title":"Introduction to regression analysis and predictions, accessed from https://michael-fuchs-python.netlify.app/2019/06/28/introduction-to-regression-analysis-and-predictions/'\u00b6","text":""},{"location":"VizeppiStatsModelTutorial/#python-multiple-regression-with-statsmodels-accessed-from-httpswwwyoutubecomwatchvi4ygoybgfqi","title":"python - multiple regression with statsmodels, accessed from https://www.youtube.com/watch?v=I4yGOYBGfqI'\u00b6","text":""},{"location":"assignment_1/","title":"Assignment 1 Introduction to Python","text":"<p>Now we will begin the process of reading the file with python</p> In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <p>It should be a familiar type we learned about in class.</p> In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <p>By now you have figured out what is in this data. Let's now transform it into a more useful format.</p> In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied!"},{"location":"assignment_1/#assignment-1-introduction-to-python","title":"Assignment 1 Introduction to Python\u00b6","text":""},{"location":"assignment_1/#preparation","title":"Preparation:\u00b6","text":"<ol> <li>Open the assignment link on Canvas.</li> <li>Accept the assignment and link it with your name.</li> <li>A repository will be created under your name.</li> </ol> <p>Use Amarel to launch a Shell terminal and use the terminal to do the following tasks:</p> <ol> <li>Create a new directory called <code>rcaes</code> within your home directory</li> <li>Go to 'rcaes' directory.</li> <li>GitHub Authentication: gh auth login</li> </ol> <ul> <li>What account do you want to log into? GitHub.com</li> <li>What is your preferred protocol for Git operations? SSH</li> <li>Generate a new SSH key to add to your GitHub account? Yes</li> <li>Enter a passphrase for your new SSH key (Optional) *********</li> <li>Title for your SSH key: GitHub CLI</li> <li>How would you like to authenticate GitHub CLI? Login with a web browser</li> <li>First copy your one-time code: XXXXXX</li> <li>Press Enter to open github.com in your browser... Enter</li> </ul> <ol> <li>In the browser, enter the one-time code.</li> <li>Go back to your GitHub repository.</li> <li>Clone your GitHub repository: 'Code' -&gt; 'GitHub CLI' -&gt; 'gh repo clone rcaes2023/assignment_1-YOURGHNAME'</li> </ol>"},{"location":"assignment_1/#python-lists-and-loops","title":"Python Lists and Loops\u00b6","text":"<p>In this problem, we will explore the basic data structures and flow controls of python by manually parsing a CSV file.</p> <p>Note that this is a futile exercise. In the \"real world\" you should never manually parse a CSV file. There are utilities out there that will do it for you much more quickly and efficiently. However, it is a useful exercise for learning python.</p> <p>Before starting the python part, use the JupyterLab file browser to browse to this file. Click to open it. What do you see?</p>"},{"location":"assignment_1/#open-the-file-using-the-open-function","title":"Open the file using the <code>open</code> function\u00b6","text":"<p>Specifically, run the command</p> <pre><code>file = open('rcaes_roster.csv')</code></pre>"},{"location":"assignment_1/#use-the-help-function-to-get-the-documentation-for-your-new-variable-file","title":"Use the <code>help</code> function to get the documentation for your new variable <code>file</code>\u00b6","text":"<p>This will produce a long list of methods you can use with <code>file</code>.</p>"},{"location":"assignment_1/#read-the-lines-of-the-file-into-a-variable-called-lines","title":"Read the lines of the file into a variable called <code>lines</code>\u00b6","text":"<p>Hint: use the documentation above to find the method that sounds most likely to do what you want.</p>"},{"location":"assignment_1/#display-lines-at-the-end-of-a-cell-in-order-to-see-its-contents","title":"Display <code>lines</code> at the end of a cell in order to see its contents\u00b6","text":""},{"location":"assignment_1/#display-the-number-of-students-in-class","title":"Display the number of students in class\u00b6","text":""},{"location":"assignment_1/#use-slicing-to-display-the-first-three-items-of-the-list-and-the-last-3","title":"Use slicing to display the first three items of the list. And the last 3\u00b6","text":""},{"location":"assignment_1/#now-iterate-through-lines-and-print-the-item-if-it-contains-your-netid","title":"Now iterate through <code>lines</code> and <code>print</code> the item if it contains your NetID\u00b6","text":""},{"location":"assignment_1/#write-code-to-transform-the-data-into-a-dictionary-whose-keys-are-netids-and-whose-values-are-full-names","title":"Write code to transform the data into a dictionary whose keys are NetIDs and whose values are full names.\u00b6","text":"<p>(Hint: You might need to review Python's string methods. You can start with creating an empty dictionary and adding keys/values in a loop.)</p>"},{"location":"assignment_1/#use-this-dictionary-to-look-up-your-own-name-using-your-netid","title":"Use this dictionary to look up your own name using your NetID\u00b6","text":""},{"location":"assignment_1/#figure-out-who-has-the-longest-last-name-in-the-class-bonus-10","title":"Figure out who has the longest last name in the class (Bonus: 10)\u00b6","text":"<p>(Hint: First create a list of last names.)</p>"},{"location":"assignment_1/#assignment-submission","title":"Assignment submission:\u00b6","text":"<p>When you're done with your assignment, submit your assignment using git:</p> <pre><code>git add *\ngit commit -m 'first commit' (or any other message)\ngit push</code></pre>"},{"location":"assignment_2/","title":"Assignment 2: Python Functions and Classes","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied!"},{"location":"assignment_2/#assignment-2-python-functions-and-classes","title":"Assignment 2: Python Functions and Classes\u00b6","text":""},{"location":"assignment_2/#part-i-writing-functions","title":"Part I: Writing Functions\u00b6","text":""},{"location":"assignment_2/#1-write-a-function-to-convert-temperature-from-kelvin-to-celsius-and-another-function-to-covert-from-celsius-to-kelvin","title":"1. Write a function to convert temperature from kelvin to celsius, and another function to covert from celsius to kelvin\u00b6","text":""},{"location":"assignment_2/#2-write-a-function-to-convert-temperature-to-fahrenheit","title":"2. Write a function to convert temperature to fahrenheit\u00b6","text":"<p>Include an optional keyword argument to specify whether the input is in  celcius or kelvin. Call your previously defined functions if necessary.</p>"},{"location":"assignment_2/#3-check-that-the-outputs-are-sensible","title":"3. Check that the outputs are sensible\u00b6","text":"<p>by trying a few examples</p>"},{"location":"assignment_2/#4-now-write-a-function-that-converts-from-farenheit","title":"4. Now write a function that converts from farenheit\u00b6","text":"<p>and uses a keyword argument to specify whether you want the output in celcius or kelvin</p>"},{"location":"assignment_2/#5-write-a-function-that-takes-two-arguments-feet-and-inches-and-returns-height-in-meters","title":"5. Write a function that takes two arguments (feet and inches) and returns height in meters\u00b6","text":"<p>Verify it gives sensible answers</p>"},{"location":"assignment_2/#6-write-a-function-takes-one-argument-height-in-meters-and-returns-two-arguments-feet-and-inches","title":"6. Write a function takes one argument (height in meters) and returns two arguments (feet and inches)\u00b6","text":"<p>(Consult the tutorial on numbers if you are stuck on how to implement this.)</p>"},{"location":"assignment_2/#7-verify-that-the-round-trip-conversion-from-and-back-to-meters-is-consistent","title":"7. Verify that the \"round trip\" conversion from and back to meters is consistent\u00b6","text":"<p>Check for 3 different values of height in meters</p>"},{"location":"assignment_2/#part-iv-classes","title":"Part IV: Classes\u00b6","text":"<p>Write a class that represents a Location. The constructor class should accept the arguments <code>name</code>, <code>latitude</code>, and <code>longitude</code>.</p> <p>Check if the latitude is between -90 to 90, and the longitude is between -180 and 180. Raise ValueError if not.</p> <p>You should implement a method to calculate its relative location to our ENR building:</p> <ul> <li><code>relative_loc_from_ENR</code>: The lat/lon of ENR buidling is: 40.476\u02daN, 74.43\u02daW. Print out if the location is 1) to the north or south or ENR; (2)to the east or west of ENR.</li> </ul>"},{"location":"intro_to_python/","title":"Core Python Language","text":"In\u00a0[1]: Copied! <pre># comments are anything that comes after the \"#\" symbol\na = 1       # assign 1 to variable a\nb = \"hello\" # assign \"hello\" to variable b\n</pre> # comments are anything that comes after the \"#\" symbol a = 1       # assign 1 to variable a b = \"hello\" # assign \"hello\" to variable b <p>The following identifiers are used as reserved words, or keywords of the language, and cannot be used as ordinary identifiers. They must be spelled exactly as written here:</p> <pre><code>False      class      finally    is         return\nNone       continue   for        lambda     try\nTrue       def        from       nonlocal   while\nand        del        global     not        with\nas         elif       if         or         yield\nassert     else       import     pass\nbreak      except     in         raise</code></pre> <p>Additionally, the following a built in functions which are always available in your namespace once you open a python interpreter</p> <pre><code>abs() dict() help() min() setattr() all() dir() hex() next() slice() any()\ndivmod() id() object() sorted() ascii() enumerate() input() oct() staticmethod()\nbin() eval() int() open() str() bool() exec() isinstance() ord() sum() bytearray()\nfilter() issubclass() pow() super() bytes() float() iter() print() tuple()\ncallable() format() len() property() type() chr() frozenset() list() range()\nvars() classmethod() getattr() locals() repr() zip() compile() globals() map()\nreversed() __import__() complex() hasattr() max() round() delattr() hash()\nmemoryview() set()</code></pre> In\u00a0[2]: Copied! <pre># how to we see our variables?\nprint(a)\nprint(b)\nprint(a,b)\n</pre> # how to we see our variables? print(a) print(b) print(a,b) <pre>1\nhello\n1 hello\n</pre> <p>All variables are objects. Every object has a type (class). To find out what type your variables are</p> In\u00a0[3]: Copied! <pre>print(type(a))\nprint(type(b))\n</pre> print(type(a)) print(type(b)) <pre>&lt;class 'int'&gt;\n&lt;class 'str'&gt;\n</pre> In\u00a0[4]: Copied! <pre># as a shortcut, iPython notebooks will automatically print whatever is on the last line\ntype(b)\n</pre> # as a shortcut, iPython notebooks will automatically print whatever is on the last line type(b) Out[4]: <pre>str</pre> In\u00a0[5]: Copied! <pre># we can check for the type of an object\nprint(type(a) is int)\nprint(type(a) is str)\n</pre> # we can check for the type of an object print(type(a) is int) print(type(a) is str) <pre>True\nFalse\n</pre> <p>Different objects attributes and methods, which can be accessed via the syntax <code>variable.method</code></p> <p>IPython will autocomplete if you press <code>&lt;tab&gt;</code> to show you the methods available.</p> In\u00a0[6]: Copied! <pre># this returns the method itself\nb.capitalize\n</pre> # this returns the method itself b.capitalize Out[6]: <pre>&lt;function str.capitalize()&gt;</pre> In\u00a0[7]: Copied! <pre># this calls the method\nb.capitalize()\n# there are lots of other methods\n</pre> # this calls the method b.capitalize() # there are lots of other methods Out[7]: <pre>'Hello'</pre> In\u00a0[8]: Copied! <pre># binary operations act differently on different types of objects\nc = 'World'\nprint(b + c)\nprint(a + 2)\nprint(a + b)\n</pre> # binary operations act differently on different types of objects c = 'World' print(b + c) print(a + 2) print(a + b) <pre>helloWorld\n3\n</pre> <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[8], line 5\n      3 print(b + c)\n      4 print(a + 2)\n----&gt; 5 print(a + b)\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'</pre> In\u00a0[9]: Copied! <pre># addition / subtraction\n1+1-5\n</pre> # addition / subtraction 1+1-5 Out[9]: <pre>-3</pre> In\u00a0[10]: Copied! <pre># multiplication\n5 * 10\n</pre> # multiplication 5 * 10 Out[10]: <pre>50</pre> In\u00a0[11]: Copied! <pre># division\n1/2\n</pre> # division 1/2 Out[11]: <pre>0.5</pre> In\u00a0[12]: Copied! <pre># that was automatically converted to a float\ntype(1/2)\n</pre> # that was automatically converted to a float type(1/2) Out[12]: <pre>float</pre> In\u00a0[13]: Copied! <pre># exponentiation\n2**4\n</pre> # exponentiation 2**4 Out[13]: <pre>16</pre> In\u00a0[14]: Copied! <pre># rounding\nround(9/10)\n</pre> # rounding round(9/10) Out[14]: <pre>1</pre> In\u00a0[15]: Copied! <pre># built in complex number support\n(1+2j) / (3-4j)\n</pre> # built in complex number support (1+2j) / (3-4j) Out[15]: <pre>(-0.2+0.4j)</pre> In\u00a0[16]: Copied! <pre># logic\nTrue and True\n</pre> # logic True and True Out[16]: <pre>True</pre> In\u00a0[17]: Copied! <pre>True and False\n</pre> True and False Out[17]: <pre>False</pre> In\u00a0[18]: Copied! <pre>True or True\n</pre> True or True Out[18]: <pre>True</pre> In\u00a0[19]: Copied! <pre>(not True) or (not False)\n</pre> (not True) or (not False) Out[19]: <pre>True</pre> In\u00a0[20]: Copied! <pre>x = 100\nif x &gt; 0:\n    print('Positive Number')\nelif x &lt; 0:\n    print('Negative Number')\nelse:\n    print ('Zero!')\n</pre> x = 100 if x &gt; 0:     print('Positive Number') elif x &lt; 0:     print('Negative Number') else:     print ('Zero!') <pre>Positive Number\n</pre> In\u00a0[21]: Copied! <pre># indentation is MANDATORY\n# blocks are closed by indentation level\nif x &gt; 0:\n    print('Positive Number')\n    if x &gt;= 100:\n        print('Huge number!')\n</pre> # indentation is MANDATORY # blocks are closed by indentation level if x &gt; 0:     print('Positive Number')     if x &gt;= 100:         print('Huge number!') <pre>Positive Number\nHuge number!\n</pre> In\u00a0[22]: Copied! <pre># make a loop \ncount = 0\nwhile count &lt; 10:\n    # bad way\n    # count = count + 1\n    # better way\n    count += 1\nprint(count)\n</pre> # make a loop  count = 0 while count &lt; 10:     # bad way     # count = count + 1     # better way     count += 1 print(count) <pre>10\n</pre> In\u00a0[23]: Copied! <pre># use range\nfor i in range(5):\n    print(i)\n</pre> # use range for i in range(5):     print(i) <pre>0\n1\n2\n3\n4\n</pre> <p>Important point: in python, we always count from 0!</p> In\u00a0[24]: Copied! <pre># what is range?\ntype(range)\n</pre> # what is range? type(range) Out[24]: <pre>type</pre> In\u00a0[25]: Copied! <pre>range?\n</pre> range? <pre>Init signature: range(self, /, *args, **kwargs)\nDocstring:     \nrange(stop) -&gt; range object\nrange(start, stop[, step]) -&gt; range object\n\nReturn an object that produces a sequence of integers from start (inclusive)\nto stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\nstart defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\nThese are exactly the valid indices for a list of 4 elements.\nWhen step is given, it specifies the increment (or decrement).\nType:           type\nSubclasses:     </pre> In\u00a0[26]: Copied! <pre># iterate over a list we make up\nfor pet in ['dog', 'cat', 'fish']:\n    print(pet, len(pet))\n</pre> # iterate over a list we make up for pet in ['dog', 'cat', 'fish']:     print(pet, len(pet)) <pre>dog 3\ncat 3\nfish 4\n</pre> <p>What is the thing in brackets? A list! Lists are one of the core python data structures.</p> In\u00a0[27]: Copied! <pre>l = ['dog', 'cat', 'fish']\ntype(l)\n</pre> l = ['dog', 'cat', 'fish'] type(l) Out[27]: <pre>list</pre> In\u00a0[28]: Copied! <pre># list have lots of methods\nl.sort()\nl\n</pre> # list have lots of methods l.sort() l Out[28]: <pre>['cat', 'dog', 'fish']</pre> In\u00a0[29]: Copied! <pre># we can convert a range to a list\nr = list(range(5))\nr\n</pre> # we can convert a range to a list r = list(range(5)) r Out[29]: <pre>[0, 1, 2, 3, 4]</pre> In\u00a0[30]: Copied! <pre>while r:\n    p = r.pop()\n    print('p:', p)\n    print('r:', r)\n</pre> while r:     p = r.pop()     print('p:', p)     print('r:', r) <pre>p: 4\nr: [0, 1, 2, 3]\np: 3\nr: [0, 1, 2]\np: 2\nr: [0, 1]\np: 1\nr: [0]\np: 0\nr: []\n</pre> <p>There are many different ways to interact with lists. Exploring them is part of the fun of python.</p> <p>list.append(x) Add an item to the end of the list. Equivalent to a[len(a):] = [x].</p> <p>list.extend(L) Extend the list by appending all the items in the given list. Equivalent to a[len(a):] = L.</p> <p>list.insert(i, x) Insert an item at a given position. The first argument is the index of the element before which to insert, so a.insert(0, x) inserts at the front of the list, and a.insert(len(a), x) is equivalent to a.append(x).</p> <p>list.remove(x) Remove the first item from the list whose value is x. It is an error if there is no such item.</p> <p>list.pop([i]) Remove the item at the given position in the list, and return it. If no index is specified, a.pop() removes and returns the last item in the list. (The square brackets around the i in the method signature denote that the parameter is optional, not that you should type square brackets at that position. You will see this notation frequently in the Python Library Reference.)</p> <p>list.clear() Remove all items from the list. Equivalent to del a[:].</p> <p>list.index(x) Return the index in the list of the first item whose value is x. It is an error if there is no such item.</p> <p>list.count(x) Return the number of times x appears in the list.</p> <p>list.sort() Sort the items of the list in place.</p> <p>list.reverse() Reverse the elements of the list in place.</p> <p>list.copy() Return a shallow copy of the list. Equivalent to a[:].</p> <p>Don't assume you know how list operations work!</p> In\u00a0[31]: Copied! <pre># \"add\" two lists\nx = list(range(5))\ny = list(range(10,15))\nz = x + y\nz\n</pre> # \"add\" two lists x = list(range(5)) y = list(range(10,15)) z = x + y z Out[31]: <pre>[0, 1, 2, 3, 4, 10, 11, 12, 13, 14]</pre> In\u00a0[32]: Copied! <pre># access items from a list\nprint('first', z[0])\nprint('last', z[-1])\nprint('first 3', z[:3])\nprint('last 3', z[-3:])\nprint('middle, skipping every other item', z[5:10:2])\n</pre> # access items from a list print('first', z[0]) print('last', z[-1]) print('first 3', z[:3]) print('last 3', z[-3:]) print('middle, skipping every other item', z[5:10:2]) <pre>first 0\nlast 14\nfirst 3 [0, 1, 2]\nlast 3 [12, 13, 14]\nmiddle, skipping every other item [10, 12, 14]\n</pre> <p>MEMORIZE THIS SYNTAX! It is central to so much of python and often proves confusing for users coming from other languages.</p> <p>In terms of set notation, python indexing is left inclusive, right exclusive. If you remember this, you will never go wrong.</p> In\u00a0[33]: Copied! <pre># that means we get an error from the following\nN = len(z)\nz[N]\n</pre> # that means we get an error from the following N = len(z) z[N] <pre>\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[33], line 3\n      1 # that means we get an error from the following\n      2 N = len(z)\n----&gt; 3 z[N]\n\nIndexError: list index out of range</pre> In\u00a0[37]: Copied! <pre># this index notation also applies to strings\nname = 'Xiaomeng Jin'\nprint(name[:4])\n</pre> # this index notation also applies to strings name = 'Xiaomeng Jin' print(name[:4]) <pre>Xiao\n</pre> In\u00a0[38]: Copied! <pre>print(name[:-4])\n</pre> print(name[:-4]) <pre>Xiaomeng\n</pre> In\u00a0[39]: Copied! <pre>print(name[-4:])\n</pre> print(name[-4:]) <pre> Jin\n</pre> In\u00a0[40]: Copied! <pre># you can also test for the presence of items in a list\n5 in z\n</pre> # you can also test for the presence of items in a list 5 in z Out[40]: <pre>False</pre> <p>Lists are not meant for math! They don't have a datatype.</p> In\u00a0[41]: Copied! <pre>z[4] = 'fish'\nz\n</pre> z[4] = 'fish' z Out[41]: <pre>[0, 1, 2, 3, 'fish', 10, 11, 12, 13, 14]</pre> <p>Python is full of tricks for iterating and working with lists</p> In\u00a0[42]: Copied! <pre># a cool python trick: list comprehension\nsquares = [n**2 for n in range(5)]\nsquares\n</pre> # a cool python trick: list comprehension squares = [n**2 for n in range(5)] squares Out[42]: <pre>[0, 1, 4, 9, 16]</pre> In\u00a0[43]: Copied! <pre># iterate over two lists together uzing zip\nfor item1, item2 in zip(x,y):\n    print('first:', item1, 'second:', item2)\n</pre> # iterate over two lists together uzing zip for item1, item2 in zip(x,y):     print('first:', item1, 'second:', item2) <pre>first: 0 second: 10\nfirst: 1 second: 11\nfirst: 2 second: 12\nfirst: 3 second: 13\nfirst: 4 second: 14\n</pre> In\u00a0[48]: Copied! <pre># tuples are created with parentheses, or just commas\na = ('Jin', 32, True)\nb = 'Wang', 25, False\ntype(b)\n</pre> # tuples are created with parentheses, or just commas a = ('Jin', 32, True) b = 'Wang', 25, False type(b) Out[48]: <pre>tuple</pre> In\u00a0[50]: Copied! <pre># can be indexed like arrays\nprint(a[1]) # not the first element!\n</pre> # can be indexed like arrays print(a[1]) # not the first element! <pre>32\n</pre> In\u00a0[51]: Copied! <pre># and they can be unpacked\nname, age, status = a\n</pre> # and they can be unpacked name, age, status = a In\u00a0[52]: Copied! <pre># different ways to create dictionaries\nd = {'name': 'Jin', 'age': 32}\ne = dict(name='Wang', age=25)\ne\n</pre> # different ways to create dictionaries d = {'name': 'Jin', 'age': 32} e = dict(name='Wang', age=25) e Out[52]: <pre>{'name': 'Wang', 'age': 25}</pre> In\u00a0[53]: Copied! <pre># access a value\nd['name']\n</pre> # access a value d['name'] Out[53]: <pre>'Jin'</pre> <p>Square brackets <code>[...]</code> are python for \"get item\" in many different contexts.</p> In\u00a0[54]: Copied! <pre># test for the presence of a key\nprint('age' in d)\nprint('height' in e)\n</pre> # test for the presence of a key print('age' in d) print('height' in e) <pre>True\nFalse\n</pre> In\u00a0[55]: Copied! <pre># try to access a non-existant key\nd['height']\n</pre> # try to access a non-existant key d['height'] <pre>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[55], line 2\n      1 # try to access a non-existant key\n----&gt; 2 d['height']\n\nKeyError: 'height'</pre> In\u00a0[56]: Copied! <pre># add a new key\nd['height'] = (5,3) # a tuple\nd\n</pre> # add a new key d['height'] = (5,3) # a tuple d Out[56]: <pre>{'name': 'Jin', 'age': 32, 'height': (5, 3)}</pre> In\u00a0[60]: Copied! <pre># keys don't have to be strings\nd[99] = 'nighty nine'\nd\n</pre> # keys don't have to be strings d[99] = 'nighty nine' d Out[60]: <pre>{'name': 'Jin', 'age': 32, 'height': (5, 3), 99: 'nighty nine'}</pre> In\u00a0[61]: Copied! <pre># iterate over keys\nfor k in d:\n    print(k, d[k])\n</pre> # iterate over keys for k in d:     print(k, d[k]) <pre>name Jin\nage 32\nheight (5, 3)\n99 nighty nine\n</pre> In\u00a0[62]: Copied! <pre># better way\n### python 2\n### for key, val in d.iteritems()\nfor key, val in d.items():\n    print(key, val)\n</pre> # better way ### python 2 ### for key, val in d.iteritems() for key, val in d.items():     print(key, val) <pre>name Jin\nage 32\nheight (5, 3)\n99 nighty nine\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"intro_to_python/#core-python-language","title":"Core Python Language\u00b6","text":"<p>Mostly copied from the official python tutorial</p>"},{"location":"intro_to_python/#invoking-python","title":"Invoking Python\u00b6","text":"<p>There are three main ways to use python.</p> <ol> <li>By running a python file, e.g. <code>python myscript.py</code></li> <li>Through an interactive console (python interpreter or ipython shell)</li> <li>In an interactive iPython notebook</li> </ol> <p>We will be using the iPython notebook.</p>"},{"location":"intro_to_python/#python-versions","title":"Python Versions\u00b6","text":"<p>There are two versions of the python language out there: python 2 and python 3. Python 2 is more common in the wild but is depracated. The community is moving to python 3. As new python learners, you should learn python 3. But it is important to be aware that python 2 exists. It is possible that a package you want to use is only supported in python 2. In general, it is pretty easy to switch between then.</p> <p>Some of the main changes in python 3 are:</p> <ul> <li><code>print</code> is a function</li> <li>Integer division returns a float</li> <li>Iterators behave differently</li> <li>Unicode is used for encoding code</li> </ul>"},{"location":"intro_to_python/#basic-variables-numbers-and-string","title":"Basic Variables: Numbers and String\u00b6","text":""},{"location":"intro_to_python/#math","title":"Math\u00b6","text":"<p>Basic arithmetic and boolean logic is part of the core python library.</p>"},{"location":"intro_to_python/#conditionals","title":"Conditionals\u00b6","text":"<p>The first step to programming. Plus an intro to python syntax.</p>"},{"location":"intro_to_python/#more-flow-control","title":"More Flow Control\u00b6","text":""},{"location":"intro_to_python/#lists","title":"Lists\u00b6","text":""},{"location":"intro_to_python/#other-data-structures","title":"Other Data Structures\u00b6","text":"<p>We are almost there. We have the building blocks we need to do basic programming. But python has some other data structures we need to learn about.</p>"},{"location":"intro_to_python/#tuples","title":"Tuples\u00b6","text":"<p>Tuples are similar to lists, but they are immutable\u2014they can't be extended or modified. What is the point of this? Generally speaking: to pack together inhomogeneous data. Tuples can then be unpacked and distributed by other parts of your code.</p> <p>Tuples may seem confusing at first, but with time you will come to appreciate them.</p>"},{"location":"intro_to_python/#dictionaries","title":"Dictionaries\u00b6","text":"<p>This is an extremely useful data structure. It maps keys to values.</p> <p>Dictionaries are unordered!</p>"},{"location":"lecture_1_intro/","title":"Lecture 1 Introduction","text":""},{"location":"lecture_1_intro/#presentation-slides-are-posted-on-canvas","title":"Presentation slides are posted on Canvas","text":""},{"location":"lecture_1_intro/#key-points","title":"Key Points","text":"<ol> <li>What is computational research?</li> <li>Data analysis pipeline for computational research. </li> <li>Challenges in research computing: Complexity, Reproducibility, Data Size. </li> <li>Topics we will cover in class.  </li> <li>Class logistics. </li> </ol>"},{"location":"lecture_1_intro/#topics-we-will-cover-in-class","title":"Topics we will cover in class","text":"<ol> <li>Python Programming</li> <li>Open-source Computing</li> <li>Big Data</li> </ol>"},{"location":"lecture_1_intro/#learning-goals","title":"Learning Goals","text":"<ol> <li>Be able to construct complete, well-structured programs in Python.</li> <li>Read and write most common atmospheric and environmental sciences data formats.</li> <li>Perform basic exploratory data analysis.</li> <li>Use visualization to enhance interpretation of environmental science data, including making maps and interactive visualizations.</li> <li>Practice open-source research through version control, packaging etc.</li> <li>Practice big-data analysis with parallel computing.</li> <li>Understand the concepts of cloud computing. </li> </ol>"}]}